{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "900a3b1b-84a0-4af6-b03b-dc3cf74f7572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Medallion Architecture Data Cleaning Pipeline \n",
    "\n",
    "Delta Live Tables offer a fault-tolerant, optimized approach for building reliable data pipelines, making them ideal for this use case.\n",
    "\n",
    "In the real world, roles & responsibilities of E2E data projects are as shown: \n",
    "- **Data Engineers**: Focus on building pipelines that handle common data issues such as duplicates, formatting of columns, schema definition, and invalid values.\n",
    "\n",
    "- **Data Scientists**: Work on EDA, imputing missing values, handling outliers, and preparing data for modeling (feature engineering / selection / dimensionality reduction etc).\n",
    "\n",
    "In this notebook, I will be implementing a simplified **Medallion Architecture** using **Delta Live Tables (DLT) \n",
    " in Azure Databricks** to simulate real-world data engineering practices. \n",
    "\n",
    "I will be using the following visualisation as a guide to build the data pipeline. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://media.datacamp.com/cms/ad_4nxe4oejrhu9gexxri3ea6vmsu1fgxcxbvlwmbaj4ji5s2u31dg3hbyyg4sxmd7ma8-9zamnbxadzz_h4kllvjylicug3v4-iinvx65erdijn4htymmqvc3mjqblskqzdu5ttmodyua.png\">\n",
    "\n",
    "\n",
    "\n",
    "By the end of this notebook, I should be able to: \n",
    "- Output a **thoroughly cleansed target dataset** ready for data scientists' to conduct EDA, dataset preprocessing and other model building practices. \n",
    "\n",
    "- Define **feature and target variables** from the target table clearly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6befe41e-23c0-430c-8ee8-fa478acdc63a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16d7e7af-a7c5-4d6e-a563-c17376d47c4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, when, count, desc, isnan, isnull, lit, length, trim, lower, upper, to_date, concat_ws,  regexp_extract\n",
    ")\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, IntegerType, DateType, NumericType\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89517c7b-26cf-4b0f-913d-191bb16b4ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bronze Delta Table\n",
    "\n",
    "This serves as a 'landing place' for raw data for single-source of truth purposes. In case data processing in subsequent stages go faulty, data specialists can use the **Bronze Delta Table** for reference, ensuring data integrity. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a9d406-f5a0-4549-86a1-7a0dcfa14804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import dlt\n",
    "\n",
    "# This will only be allowed if I can create a DLT pipeline (not allowed due to Azure for Students)\n",
    "\n",
    "# @dlt.table(name=\"bronze_raw_lendingclub_data\", comment=\"Ingest raw loan data from Lending Club csv\")\n",
    "# def bronze_raw_loans():\n",
    "#     return spark.read.csv(\"/FileStore/tables/accepted_2007_to_2018Q4.csv\", \n",
    "#                           header=True, \n",
    "#                           inferSchema=True)\n",
    "    \n",
    "# I will need to ensure inferSchema = True, so that all columns dtypes are auto-detected to lessen my workload later \n",
    "\n",
    "# ✅ The below allows DLT pipeline not to be created \n",
    "bronze_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"/FileStore/tables/accepted_2007_to_2018Q4.csv\")\n",
    ")\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze\")\n",
    "\n",
    "# ✅ 2. Save as a Delta table in the `bronze` schema\n",
    "bronze_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze.lendingclub_raw\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d26b8cd-5002-4eae-adf1-7387365eb7fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver Delta Table\n",
    "\n",
    "Next, the pipeline to produce a Silver Delta Table will mainly perform key data cleaning steps.\n",
    "  - Deal with Duplicates\n",
    "  - Remove String Column Spaces\n",
    "  - Handle String Formatting / Spelling Issues \n",
    "  - Ensure UTF-8 for String Columns \n",
    "  - Schema Definition \n",
    "  - Invalid Value Handling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ead7f5e-4328-4e0e-833c-2d1e8236f966",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### String Columns Cleaning\n",
    "\n",
    "These cleaning steps will take reference from **sandbox/string_issues** for specific cleaning steps to maintain data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "631f46e7-fd83-46d8-84dc-188bc310b3b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def drop_duplicates(df):\n",
    "    duplicate_rows = df.count() - df.dropDuplicates().count()\n",
    "\n",
    "    return df.dropDuplicates()\n",
    "\n",
    "def handle_string_cols_spaces(df): \n",
    "    string_cols = [\n",
    "        field.name for field in df.schema.fields\n",
    "        if isinstance(field.dataType, StringType)]\n",
    "    \n",
    "    # Replaces each existing column with new <string> values which are trimmed \n",
    "    for col_name in string_cols:\n",
    "        df = df.withColumn(col_name, trim(col(col_name)))\n",
    "    \n",
    "    return df \n",
    "\n",
    "def handle_string_cols_formatting(df):  \n",
    "    \"\"\"\n",
    "    Uses library of RapidFuzz to provide lightweight similarity calculations, optimised for performance\n",
    "\n",
    "    Takes reference from String issues are in ../sandbox/string_issues.ipynb\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Original Number of Rows: {df.count()}. \")\n",
    "    \n",
    "    # # 1. Drops unusable String columns \n",
    "    # unusable_cols = [\"emp_title\",\n",
    "    # \"hardship_type\",\n",
    "    # \"verification_status_joint\",\n",
    "    # \"hardship_status\",\n",
    "    # \"deferral_term\",\n",
    "    # \"hardship_length\",\n",
    "    # \"hardship_loan_status\",\n",
    "    # \"settlement_status\", \"annual_inc_joint\", 'dti_joint']\n",
    "    # df = df.drop(*unusable_cols) # allows dropping of multiple columns\n",
    "\n",
    "    # 2. Fix addr_state (check len() > 2)\n",
    "    df = df.filter(length( col('addr_state') ) == 2)\n",
    "\n",
    "    # 3. Fix invalid string column values \n",
    "\n",
    "    # invalid_entries_list = [ \"application_type\",\n",
    "    #     \"policy_code\",\n",
    "    #     \"home_ownership\",\n",
    "    #     \"verification_status\",\n",
    "    #     \"loan_status\",\n",
    "    #     \"pymnt_plan\",\n",
    "    #     \"initial_list_status\",\n",
    "    #     \"hardship_flag\"]\n",
    "    \n",
    "    df = df.filter(\n",
    "        col(\"application_type\").isin(\"Individual\", \"Joint App\") &\n",
    "        col(\"policy_code\").isin(\"1.0\", \"0.0\") &  # stored as string initially\n",
    "        col(\"home_ownership\").isin(\"MORTGAGE\", \"RENT\", \"OWN\", \"ANY\", \"OTHER\") &\n",
    "        col(\"verification_status\").isin(\"Source Verified\", \"Not Verified\", \"Verified\") &\n",
    "        col(\"loan_status\").isin(\n",
    "            \"Fully Paid\", \"Current\", \"Charged Off\",\n",
    "            \"Late (31-120 days)\", \"In Grace Period\",\n",
    "            \"Late (16-30 days)\", \"Default\"\n",
    "        ) &\n",
    "        col(\"pymnt_plan\").isin(\"n\", \"y\") &\n",
    "        col(\"initial_list_status\").isin(\"f\", \"w\") &\n",
    "        col(\"hardship_flag\").isin(\"N\", \"Y\")\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\"policy_code\", when(col(\"policy_code\") == \"1.0\", 1).when(col(\"policy_code\") == \"0.0\", 0))\n",
    "\n",
    "    # 4. Drop meaningless string columns \n",
    "    meaningless_columns = [\n",
    "    \"url\", \"desc\", \"title\", \"zip_code\", \"purpose\"]\n",
    "    df = df.drop(*meaningless_columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "def cast_string_to_numeric_cols(df): \n",
    "    numeric_columns = [\n",
    "    \"id\", \"member_id\", \"annual_inc\", \"annual_inc_joint\",\n",
    "    \"dti\", \"dti_joint\", \"delinq_2yrs\", \"fico_range_low\", \"fico_range_high\",\n",
    "    \"inq_last_6mths\", \"mths_since_last_delinq\", \"mths_since_last_record\",\n",
    "    \"open_acc\", \"pub_rec\", \"revol_bal\", \"revol_util\", \"total_acc\", \n",
    "    \"out_prncp\", \"out_prncp_inv\", \"total_pymnt\", \"total_pymnt_inv\", \n",
    "    \"total_rec_prncp\", \"total_rec_int\", \"total_rec_late_fee\", \"recoveries\", \n",
    "    \"collection_recovery_fee\", \"last_fico_range_high\", \"last_fico_range_low\", \n",
    "    \"collections_12_mths_ex_med\", \"mths_since_last_major_derog\",\n",
    "    \"acc_now_delinq\", \"tot_coll_amt\", \"loan_amnt\", \"funded_amnt\", \n",
    "    \"funded_amnt_inv\", \"installment\", \"tot_cur_bal\", \"total_rev_hi_lim\", \n",
    "    \"inq_fi\", \"hardship_amount\", \"hardship_dpd\", \"orig_projected_additional_accrued_interest\",\n",
    "    \"hardship_payoff_balance_amount\", \"hardship_last_payment_amount\", \n",
    "    \"settlement_amount\", \"settlement_percentage\", \"settlement_term\", \n",
    "    \"avg_cur_bal\", \"total_bal_il\", \"bc_util\", \"il_util\", \"total_cu_tl\",\n",
    "    \"max_bal_bc\", \"percent_bc_gt_75\", \"total_bal_ex_mort\", \"all_util\", \n",
    "    \"open_acc_6m\", \"open_act_il\", \"open_il_12m\", \"last_pymnt_amnt\", \"open_il_24m\", \"mths_since_rcnt_il\", \n",
    "    \"open_rv_12m\", \"open_rv_24m\", 'emp_length', 'term']\n",
    "\n",
    "    # Deal with Type Casting to Numeric Data \n",
    "    int_cols = ['id', 'member_id']\n",
    "\n",
    "    for column in numeric_columns: \n",
    "        if column in int_cols: \n",
    "            df = df.withColumn(column, col(column).cast(IntegerType()))\n",
    "        elif column == 'emp_length':\n",
    "            # Convert emp_length to integer values\n",
    "            df = df.withColumn(\n",
    "                \"emp_length\",\n",
    "                when(col(\"emp_length\").rlike(\"10\\\\+\"), 10)\n",
    "                .when(col(\"emp_length\").rlike(\"< 1\"), 0)\n",
    "                .otherwise(\n",
    "                    regexp_extract(col(\"emp_length\"), r\"(\\d+)\", 1).cast(\"int\") # extracts 1st regex group digit from string \n",
    "                )\n",
    "            )\n",
    "        elif column == 'term': \n",
    "            df = df.withColumn(\"term\",regexp_extract(col(\"term\"), r\"(\\d+)\", 1).cast(\"int\"))\n",
    "\n",
    "        else: \n",
    "            df = df.withColumn(column, col(column).cast(DoubleType()))\n",
    "\n",
    "    return df \n",
    "\n",
    "def cast_string_to_date_cols(df):\n",
    "    date_columns = [\n",
    "    \"issue_d\", \"earliest_cr_line\", \"last_pymnt_d\", \"last_credit_pull_d\", \"next_pymnt_d\",\n",
    "    \"sec_app_earliest_cr_line\", \"hardship_start_date\", \"hardship_end_date\", \n",
    "    \"payment_plan_start_date\", \"debt_settlement_flag_date\", \"settlement_date\"]\n",
    "\n",
    "    # Clean and cast each column\n",
    "    for date_col in date_columns:\n",
    "        # Format is 'MMM-yyyy' → Add dummy day '01' → Convert to 'yyyy-MM-dd'\n",
    "        df = df.withColumn(\n",
    "            date_col,\n",
    "            to_date(concat_ws(\"-\", col(date_col), lit(\"01\")), \"MMM-yyyy-dd\")\n",
    "        )\n",
    "\n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ee7024d4-5f90-441d-8a5d-0811e5abf156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 0: Read from Bronze Table \n",
    "bronze_df_copy = spark.read.table(\"bronze.lendingclub_raw\")\n",
    "\n",
    "# Step 1: Drop duplicate rows\n",
    "print(f\"Original number of rows: {bronze_df_copy.count()}\\n\")\n",
    "\n",
    "df_cleaned = drop_duplicates(bronze_df_copy)\n",
    "print('✅ Duplicates removed...')\n",
    "\n",
    "\n",
    "# Step 2: Trim spaces in all string columns\n",
    "df_cleaned = handle_string_cols_spaces(df_cleaned)\n",
    "print('✅ Trailing / Leading Spaces removed...')\n",
    "\n",
    "# Step 3: Filter & fix invalid string formatting\n",
    "df_cleaned = handle_string_cols_formatting(df_cleaned)\n",
    "print('✅ Invalid String Column Formatting Settled & Meaningless Columns Dropped ...')\n",
    "\n",
    "# Step 4: Type Casting\n",
    "df_cleaned = cast_string_to_numeric_cols(df_cleaned)\n",
    "df_cleaned = cast_string_to_date_cols(df_cleaned)\n",
    "print('✅ String Columns Correctly Type Casted...\\n')\n",
    "\n",
    "\n",
    "print(f\"New number of rows: {df_cleaned.count()}\")\n",
    "\n",
    "# Step 5: Save as Silver Delta Table 1 (Cleaned Strings Version)\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS silver\")\n",
    "df_cleaned.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver.lendingclub_cleaned_string\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf8fc3af-0837-4bea-8562-45059b9de0e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Numeric Columns Cleaning\n",
    "These cleaning steps will take reference from **sandbox/numeric_issues** for specific cleaning steps to maintain data integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "759f4246-8312-4491-b388-87d546532a1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clear_invalid_numerical_entries(df):\n",
    "    df = df.filter( ~(col('dti') < 0  ))\n",
    "    df = df.filter( ~ (col('total_rec_late_fee') < 0  ))\n",
    "\n",
    "    return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a27dc02-485f-40da-b988-01bb0c84afeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_table1 = spark.read.table(\"silver.lendingclub_cleaned_string\")\n",
    "print(f\"Original number of rows: {silver_table1.count()}\\n\")\n",
    "\n",
    "silver_table2 = clear_invalid_numerical_entries(silver_table1)\n",
    "print('✅ Invalid Numerical Values Settled...')\n",
    "\n",
    "print(f\"Final number of rows: {silver_table2.count()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5acd300-046c-47d3-a98b-ee19f2a0e31f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(silver_table2.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76b27bc0-aa39-4405-9c48-bb5aa6255e22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: Save as Silver Delta Table 2 (Cleaned Strings Version)\n",
    "silver_table2.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver.lendingclub_cleaned_numeric\")\n",
    "\n",
    "silver_table2.limit(10).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "442cf3f5-d36f-4f87-b6a6-e66623340857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Gold Delta Table \n",
    "Finally, to produce a Gold Delta Table, I will need to sort the dataset in chronological order. \n",
    "\n",
    "For credit risk modeling, banks use past data loan data to predict future defaults / metrics. As such, we want our dataset to be sorted in **chronological order**, so that built models are trained on older data, and tested on newer data **(out-of-time split)**. \n",
    "\n",
    "There should not be random splitting of data **(out-of-sample split)**, e.g. `train-test-split` from `sklearn` since credit-risk modeling is a **time-series problem**.\n",
    "\n",
    "Hence, I will be sorting the dataset right from the start. \n",
    "\n",
    "By producing the Gold Delta Table, the subsequent jobs would require data scientists to impute missing values, conduct feature engineering and dimensionality reduction for accurate credit risk modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53463362-4d20-4f47-be24-5fc7f18503c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_table2 = spark.read.table(\"silver.lendingclub_cleaned_numeric\")\n",
    "\n",
    "gold_df = silver_table2.orderBy([\"issue_d\"], ascending=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18d12cbe-53a8-4dc3-bc40-7a1c3dd21ae2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: Save as Silver Delta Table 2 (Cleaned Strings Version)\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold\")\n",
    "\n",
    "gold_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold.medallion_cleaned_lc_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6b65bf7-5f22-4f55-b3aa-2e65addc06a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if Gold Delta is accessible to data scientists\n",
    "gold_table = spark.read.table('gold.medallion_cleaned_lc_data')\n",
    "gold_table.limit(10).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f37a9ad8-3563-4562-b9e1-5d38f16f2307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As seen, the `issue_d` column wasn't sorted in order even after I have conducted the sorting before saving it as a Gold Delta Table. After researching, I realised that the distributed computing environment in PySpark prevents this from happening. To counter this, data scientists will have to take note to sort by the data before any machine learning model building can happen. "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Medallion Architecture Data Cleaning Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
