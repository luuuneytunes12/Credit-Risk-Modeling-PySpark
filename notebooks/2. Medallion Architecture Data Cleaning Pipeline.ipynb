{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "900a3b1b-84a0-4af6-b03b-dc3cf74f7572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Medallion Architecture Data Cleaning Pipeline \n",
    "\n",
    "Delta Live Tables offer a fault-tolerant, optimized approach for building reliable data pipelines, making them ideal for this use case.\n",
    "\n",
    "In the real world, roles & responsibilities of E2E data projects are as shown: \n",
    "- **Data Engineers**: Focus on building pipelines that handle common data issues such as duplicates, formatting of columns, schema definition, and invalid values.\n",
    "\n",
    "- **Data Scientists**: Work on EDA, imputing missing values, handling outliers, and preparing data for modeling (feature engineering / selection / dimensionality reduction etc).\n",
    "\n",
    "In this notebook, I will be implementing a simplified **Medallion Architecture** using **Delta Live Tables (DLT) \n",
    " in Azure Databricks** to simulate real-world data engineering practices. \n",
    "\n",
    "I will be using the following visualisation as a guide to build the data pipeline. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://media.datacamp.com/cms/ad_4nxe4oejrhu9gexxri3ea6vmsu1fgxcxbvlwmbaj4ji5s2u31dg3hbyyg4sxmd7ma8-9zamnbxadzz_h4kllvjylicug3v4-iinvx65erdijn4htymmqvc3mjqblskqzdu5ttmodyua.png\">\n",
    "\n",
    "\n",
    "\n",
    "By the end of this notebook, I should be able to: \n",
    "- Output a **thoroughly cleansed target dataset** ready for data scientists' to conduct EDA, dataset preprocessing and other model building practices. \n",
    "\n",
    "- Define **feature and target variables** from the target table clearly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89517c7b-26cf-4b0f-913d-191bb16b4ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bronze Delta Table\n",
    "\n",
    "This serves as a 'landing place' for raw data for single-source of truth purposes. In case data processing in subsequent stages go faulty, data specialists can use the **Bronze Delta Table** for reference, ensuring data integrity. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a9d406-f5a0-4549-86a1-7a0dcfa14804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# + --------------------------- +\n",
    "# | Bronze Delta Table Pipeline |\n",
    "# + --------------------------- +\n",
    "# ===============================\n",
    "\n",
    "import dlt\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types\n",
    "\n",
    "\n",
    "\n",
    "# This will only be allowed if I can create a DLT pipeline (not allowed due to Azure for Students)\n",
    "\n",
    "# @dlt.table(name=\"bronze_raw_lendingclub_data\", comment=\"Ingest raw loan data from Lending Club csv\")\n",
    "# def bronze_raw_loans():\n",
    "#     return spark.read.csv(\"/FileStore/tables/accepted_2007_to_2018Q4.csv\", \n",
    "#                           header=True, \n",
    "#                           inferSchema=True)\n",
    "    \n",
    "# I will need to ensure inferSchema = True, so that all columns dtypes are auto-detected to lessen my workload later \n",
    "\n",
    "# ✅ The below allows DLT pipeline not to be created \n",
    "bronze_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"/FileStore/tables/accepted_2007_to_2018Q4.csv\")\n",
    ")\n",
    "\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze\")\n",
    "\n",
    "# ✅ 2. Save as a Delta table in the `bronze` schema\n",
    "bronze_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze.lendingclub_raw\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d26b8cd-5002-4eae-adf1-7387365eb7fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver Delta Table\n",
    "\n",
    "Next, the pipeline to produce a Silver Delta Table will mainly perform key data cleaning steps.\n",
    "  - Deal with Duplicates\n",
    "  - Remove String Column Spaces\n",
    "  - Handle String Formatting / Spelling Issues \n",
    "  - Ensure UTF-8 for String Columns \n",
    "  - Schema Definition \n",
    "  - Invalid Value Handling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "631f46e7-fd83-46d8-84dc-188bc310b3b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Functions needed for Silver Delta Pipeline \n",
    "\n",
    "def drop_duplicates(df):\n",
    "    duplicate_rows = df.count() - df.dropDuplicates().count()\n",
    "    print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "\n",
    "    return df.dropDuplicates()\n",
    "\n",
    "\n",
    "################################\n",
    "# +---------------------------+#\n",
    "# |Handle String Column Issues|#\n",
    "# +---------------------------+# \n",
    "################################\n",
    "\n",
    "def handle_string_cols_spaces(df): \n",
    "    string_cols = [\n",
    "        field.name for field in df.schema.fields\n",
    "        if isinstance(field.dataType, pyspark.sql.types.StringType)]\n",
    "    \n",
    "    # Replaces each existing column with new <string> values which are trimmed \n",
    "    for col_name in string_cols:\n",
    "        df = df.withColumn(col_name, F.trim(pyspark.sql.functions.col(col_name)))\n",
    "    \n",
    "    return df \n",
    "\n",
    "def handle_string_cols_formatting(df):  \n",
    "    \"\"\"\n",
    "    Uses library of RapidFuzz to provide lightweight similarity calculations, optimised for performance\n",
    "\n",
    "    Takes reference from String issues are in ../sandbox/string_issues.ipynb\n",
    "    \"\"\"\n",
    "    # Drops unusable String columns \n",
    "\n",
    "\n",
    "    # Fix addr_state (check len() > 2)\n",
    "\n",
    "    # Fix invalid string column values (mafan)\n",
    "\n",
    "    # Drop meaningless string columns \n",
    "\n",
    "    return \n",
    "\n",
    "\n",
    "def utf8_string_cols(df):  \n",
    "    return \n",
    "\n",
    "\n",
    "#################################\n",
    "# +----------------------------+#\n",
    "# |Handle Numeric Column Issues|#\n",
    "# +----------------------------+# \n",
    "#################################\n",
    "\n",
    "- Type Casting...\n",
    "\n",
    "# def define_new_schema(df: DataFrame) -> DataFrame: \n",
    "#     # ensure nullable = true \n",
    "#     return \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "571f0371-a068-41d1-8db5-8dd439b1496a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# 1. Define Schema  \n",
    "def define_schema(df: pd.Dataframe) -> pd.DataFrame: \n",
    "\n",
    "\n",
    "\n",
    "# # 4. ... \n",
    "\n",
    "# from pyspark.sql import DataFrame\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# def define_schema(df: DataFrame) -> DataFrame:\n",
    "#     \"\"\"Define explicit schema, instead of schema inference, which is prone to error\"\"\"\n",
    "#     # Another syntax is StructField thingy ... \n",
    "#     my_ddl_schema = '''\n",
    "#                     Item_Identifier STRING,\n",
    "#                     Item_Weight STRING, \n",
    "#                     Item_Fat_Content STRING,\n",
    "#                     Item_Visibility DOUBLE,\n",
    "#                     Item_Type STRING,\n",
    "#                     Item_MRP DOUBLE,\n",
    "#                     Outlet_Identifier STRING,\n",
    "#                     Outlet_Establishment_Year INT,\n",
    "#                     Outlet_Size STRING,\n",
    "#                     Outlet_Location_Type STRING,\n",
    "#                     Outlet_Type STRING,\n",
    "#                     Item_Outlet_Sales DOUBLE\n",
    "\n",
    "#                     ''' \n",
    "\n",
    "#     df = dlt.read(\"bronze_raw_lendingclub_data\")  # reads the bronze delta table as a DataFrame\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3461cff-7132-4ab6-85ac-cabac95f857b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# @dlt.table(name=\"silver_cleaned_lendingclub_data\", comment=\"Full data cleaning pipeline to create Silver Delta Table\")\n",
    "# def silver_cleaned_loans(bronze_df.DataFrame) -> DataFrame: \n",
    "#     # Outliers shall not be removed here, since they are normally dealt with by data scientists with knowledge of statistics\n",
    "\n",
    "#     bronze_df = drop_duplicates(bronze_df)\n",
    "#     print('✅ Duplicates removed...')\n",
    "\n",
    "\n",
    "#     bronze_df = handle_string_cols_spaces(bronze_df)\n",
    "#     print('✅ Trailing / Leading Spaces removed...')\n",
    "\n",
    "#     bronze_df = handle_string_cols_formatting(bronze_df)\n",
    "#     bronze_df = utf8_string_cols(bronze_df)\n",
    "#     bronze_df = \n",
    "\n",
    "\n",
    "#     bronze_df = define_new_schema(bronze_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "442cf3f5-d36f-4f87-b6a6-e66623340857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Gold Delta Table \n",
    "Finally, to product a Gold Delta Table, the pipeline built should perform the following steps. \n",
    "  - Attain Derived Columns, e.g. KPI ratios \n",
    "  - Create ML target variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53463362-4d20-4f47-be24-5fc7f18503c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# @dlt.table(name=\"gold_lendingclub_data\", comment=\"Ready for data scientists\")\n",
    "# def gold_processed_loans(gold_df.DataFrame) -> DataFrame: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b46a685f-70c1-45f4-b85d-cf1e15581386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## 3. Renaming Columns \n",
    "Some of the column names are too short-formed for understanding. As such, I will be renaming them for ease of interpretation. "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Medallion Architecture Data Cleaning Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
