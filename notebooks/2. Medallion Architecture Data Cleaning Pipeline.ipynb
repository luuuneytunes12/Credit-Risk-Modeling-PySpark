{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "900a3b1b-84a0-4af6-b03b-dc3cf74f7572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Medallion Architecture Data Cleaning Pipeline \n",
    "\n",
    "Delta Live Tables offer a fault-tolerant, optimized approach for building reliable data pipelines, making them ideal for this use case.\n",
    "\n",
    "In the real world, roles & responsibilities of E2E data projects are as shown: \n",
    "- **Data Engineers**: Focus on building pipelines that handle common data issues such as duplicates, formatting of columns, schema definition, and invalid values.\n",
    "\n",
    "- **Data Scientists**: Work on EDA, imputing missing values, handling outliers, and preparing data for modeling (feature engineering / selection / dimensionality reduction etc).\n",
    "\n",
    "In this notebook, I will be implementing a simplified **Medallion Architecture** using **Delta Live Tables (DLT) \n",
    " in Azure Databricks** to simulate real-world data engineering practices. \n",
    "\n",
    "I will be using the following visualisation as a guide to build the data pipeline. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://media.datacamp.com/cms/ad_4nxe4oejrhu9gexxri3ea6vmsu1fgxcxbvlwmbaj4ji5s2u31dg3hbyyg4sxmd7ma8-9zamnbxadzz_h4kllvjylicug3v4-iinvx65erdijn4htymmqvc3mjqblskqzdu5ttmodyua.png\">\n",
    "\n",
    "\n",
    "\n",
    "By the end of this notebook, I should be able to: \n",
    "- Output a **thoroughly cleansed target dataset** ready for data scientists' to conduct EDA, dataset preprocessing and other model building practices. \n",
    "\n",
    "- Define **feature and target variables** from the target table clearly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89517c7b-26cf-4b0f-913d-191bb16b4ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bronze Delta Table\n",
    "\n",
    "This serves as a 'landing place' for raw data for single-source of truth purposes. In case data processing in subsequent stages go faulty, data specialists can use the **Bronze Delta Table** for reference, ensuring data integrity. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5a9d406-f5a0-4549-86a1-7a0dcfa14804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# + --------------------------- +\n",
    "# | Bronze Delta Table Pipeline |\n",
    "# + --------------------------- +\n",
    "# ===============================\n",
    "\n",
    "import dlt\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types\n",
    "\n",
    "\n",
    "@dlt.table(name=\"bronze_raw_lendingclub_data\", comment=\"Ingest raw loan data from Lending Club csv\")\n",
    "def bronze_raw_loans():\n",
    "    return spark.read.csv(\"/FileStore/tables/accepted_2007_to_2018Q4.csv\", \n",
    "                          header=True, \n",
    "                          inferSchema=True)\n",
    "    \n",
    "# I will need to ensure inferSchema = True, so that all columns dtypes are auto-detected to lessen my workload later "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d26b8cd-5002-4eae-adf1-7387365eb7fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver Delta Table\n",
    "\n",
    "Next, the pipeline to produce a Silver Delta Table will mainly perform key data cleaning steps.\n",
    "  - Deal with Duplicates\n",
    "  - Remove String Column Spaces\n",
    "  - Handle String Formatting / Spelling Issues \n",
    "  - Ensure UTF-8 for String Columns \n",
    "  - Schema Definition \n",
    "  - Invalid Value Handling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "631f46e7-fd83-46d8-84dc-188bc310b3b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Functions needed for Silver Delta Pipeline \n",
    "\n",
    "def drop_duplicates(df: DataFrame) -> DataFrame:\n",
    "    duplicate_rows = df.count() - df.dropDuplicates().count()\n",
    "    print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "\n",
    "    return df.dropDuplicates()\n",
    "\n",
    "def handle_string_cols_spaces(df: DataFrame) -> DataFrame: \n",
    "\n",
    "    # Collect all <string> columns \n",
    "    string_cols = [\n",
    "        field.name for field in df.schema.fields\n",
    "        if isinstance(field.dataType, pyspark.sql.types.StringType)\n",
    "    ]\n",
    "\n",
    "    # Replaces each existing column with new <string> values which are trimmed \n",
    "    for col_name in string_cols:\n",
    "        df = df.withColumn(col_name, pyspark.sql.functions.trim(pyspark.sql.functions.col(col_name)))\n",
    "\n",
    "    return df \n",
    "\n",
    "def handle_string_cols_formatting(df: DataFrame) -> DataFrame:  \n",
    "    \"\"\"\n",
    "    Uses library of RapidFuzz to provide lightweight similarity calculations, optimised for performance\n",
    "    String issues are in ../sandbox/string_issues.ipynb\n",
    "    \"\"\"\n",
    "    return \n",
    "\n",
    "\n",
    "def utf8_string_cols(df: DataFrame) -> DataFrame: \n",
    "    return \n",
    "\n",
    "def handle_invalid_values(df: DataFrame) -> DataFrame: \n",
    "    return \n",
    "\n",
    "\n",
    "def define_new_schema(df: DataFrame) -> DataFrame: \n",
    "    # ensure nullable = true \n",
    "    return \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "571f0371-a068-41d1-8db5-8dd439b1496a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# 1. Define Schema  \n",
    "def define_schema(df: pd.Dataframe) -> pd.DataFrame: \n",
    "\n",
    "\n",
    "\n",
    "# 4. ... \n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def define_schema(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Define explicit schema, instead of schema inference, which is prone to error\"\"\"\n",
    "    # Another syntax is StructField thingy ... \n",
    "    my_ddl_schema = '''\n",
    "                    Item_Identifier STRING,\n",
    "                    Item_Weight STRING, \n",
    "                    Item_Fat_Content STRING,\n",
    "                    Item_Visibility DOUBLE,\n",
    "                    Item_Type STRING,\n",
    "                    Item_MRP DOUBLE,\n",
    "                    Outlet_Identifier STRING,\n",
    "                    Outlet_Establishment_Year INT,\n",
    "                    Outlet_Size STRING,\n",
    "                    Outlet_Location_Type STRING,\n",
    "                    Outlet_Type STRING,\n",
    "                    Item_Outlet_Sales DOUBLE\n",
    "\n",
    "                    ''' \n",
    "\n",
    "    df = dlt.read(\"bronze_raw_lendingclub_data\")  # reads the bronze delta table as a DataFrame\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3461cff-7132-4ab6-85ac-cabac95f857b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(name=\"silver_cleaned_lendingclub_data\", comment=\"Full data cleaning pipeline to create Silver Delta Table\")\n",
    "def silver_cleaned_loans(bronze_df.DataFrame) -> DataFrame: \n",
    "    # Outliers shall not be removed here, since they are normally dealt with by data scientists with knowledge of statistics\n",
    "\n",
    "    bronze_df = drop_duplicates(bronze_df)\n",
    "    print('✅ Duplicates removed...')\n",
    "\n",
    "\n",
    "    bronze_df = handle_string_cols_spaces(bronze_df)\n",
    "    print('✅ Trailing / Leading Spaces removed...')\n",
    "\n",
    "    bronze_df = handle_string_cols_formatting(bronze_df)\n",
    "    bronze_df = utf8_string_cols(bronze_df)\n",
    "    bronze_df = \n",
    "\n",
    "\n",
    "    bronze_df = define_new_schema(bronze_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "442cf3f5-d36f-4f87-b6a6-e66623340857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Gold Delta Table \n",
    "Finally, to product a Gold Delta Table, the pipeline built should perform the following steps. \n",
    "  - Attain Derived Columns, e.g. KPI ratios \n",
    "  - Create ML target variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53463362-4d20-4f47-be24-5fc7f18503c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(name=\"gold_lendingclub_data\", comment=\"Ready for data scientists\")\n",
    "def gold_processed_loans(gold_df.DataFrame) -> DataFrame: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64e640e4-bde3-496a-a2d0-637052886cfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample Code for Medallion Architecture \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session (if not already done)\n",
    "spark = SparkSession.builder.appName(\"ModularMedallionPipeline\").getOrCreate()\n",
    "\n",
    "# ----------------------------- #\n",
    "# Bronze Layer: Data Ingestion  #\n",
    "# ----------------------------- #\n",
    "def ingest_raw_data(input_path, output_path):\n",
    "    df = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
    "    return output_path\n",
    "\n",
    "# ----------------------------- #\n",
    "# Silver Layer: Cleaning        #\n",
    "# ----------------------------- #\n",
    "def remove_duplicates(df):\n",
    "    return df.dropDuplicates()\n",
    "\n",
    "def standardize_column_names(df):\n",
    "    new_cols = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    return df.toDF(*new_cols)\n",
    "\n",
    "def clean_data(input_path, output_path):\n",
    "    df = spark.read.format(\"delta\").load(input_path)\n",
    "    df = remove_duplicates(df)\n",
    "    df = standardize_column_names(df)\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
    "    return output_path\n",
    "\n",
    "# ----------------------------- #\n",
    "# Gold Layer: Aggregation/FE    #\n",
    "# ----------------------------- #\n",
    "def aggregate_and_engineer_features(input_path, output_path):\n",
    "    df = spark.read.format(\"delta\").load(input_path)\n",
    "    # Example: Add a dummy feature (customize as needed)\n",
    "    from pyspark.sql.functions import col\n",
    "    if \"loan_amount\" in df.columns and \"income\" in df.columns:\n",
    "        df = df.withColumn(\"dti\", col(\"loan_amount\") / col(\"income\"))\n",
    "    # Add more aggregations/feature engineering as needed\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
    "    return output_path\n",
    "\n",
    "# ----------------------------- #\n",
    "# Main Pipeline Execution       #\n",
    "# ----------------------------- #\n",
    "if __name__ == \"__main__\":\n",
    "    bronze_path = \"/mnt/bronze-zone/raw_delta\"\n",
    "    silver_path = \"/mnt/silver-zone/cleaned_delta\"\n",
    "    gold_path = \"/mnt/gold-zone/final_delta\"\n",
    "    raw_csv_path = \"dbfs:/mnt/bronze-zone/raw_data.csv\"\n",
    "\n",
    "    # Bronze: Ingest raw data\n",
    "    ingest_raw_data(raw_csv_path, bronze_path)\n",
    "\n",
    "    # Silver: Clean and standardize\n",
    "    clean_data(bronze_path, silver_path)\n",
    "\n",
    "    # Gold: Aggregate and feature engineer\n",
    "    aggregate_and_engineer_features(silver_path, gold_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b46a685f-70c1-45f4-b85d-cf1e15581386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## 3. Renaming Columns \n",
    "Some of the column names are too short-formed for understanding. As such, I will be renaming them for ease of interpretation. "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Medallion Architecture Data Cleaning Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
