{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4aba98c-df7e-4b63-a56a-9a95d5768fd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook aims to simulate a data scientist consuming data from a data pipeline, using PySpark. \n",
    "\n",
    "In this notebook, I will be executing the following steps:\n",
    "  - Conduct EDA on data produced from Medallion Data Pipeline \n",
    "  - Handling Missing Values \n",
    "\n",
    "Thereafter, feature engineering, selection and multicollinearity checks will be handled invidually in their respective notebooks. I have specifically done this, since feature engineering should be targeted for LGD, PD, EAD. Doing feature engineering and producing a master table to use for all 3 models makes our model building tedious, computationally expensive and hard to interpret. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4c9ffd9-2a32-4278-8829-300ccd6b39bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c951cc11-98a2-4550-a49b-e0bd9ef9cf38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/10 16:36:48 WARN Utils: Your hostname, Chengs-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.77 instead (on interface en0)\n",
      "25/08/10 16:36:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Users/lunlun/Downloads/Github/Credit-Risk-Modeling-PySpark/venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/lunlun/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /Users/lunlun/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-59a5969e-74eb-48a0-97af-fd012a3b144b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 77ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-59a5969e-74eb-48a0-97af-fd012a3b144b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n",
      "25/08/10 16:36:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/10 16:36:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/08/10 16:36:49 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/08/10 16:36:49 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/lunlun/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwlunlun1212\u001b[0m (\u001b[33mwlunlun1212-singapore-management-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/lunlun/.netrc\n"
     ]
    }
   ],
   "source": [
    "# == Standard libraries == \n",
    "import os\n",
    "\n",
    "# == Load Environment Variables == \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# == Start Spark == \n",
    "from init_spark import start_spark\n",
    "spark = start_spark()\n",
    "\n",
    "# == Wandb for logging == \n",
    "import wandb\n",
    "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n",
    "\n",
    "# == Pyspark Functions == \n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    when,\n",
    "    count,\n",
    "    desc,\n",
    "    isnan,\n",
    "    isnull,\n",
    "    lit,\n",
    "    length,\n",
    "    trim,\n",
    "    lower,\n",
    "    upper,\n",
    "    to_date,\n",
    "    concat_ws,\n",
    "    regexp_extract,\n",
    "    sum,\n",
    "    unix_timestamp,\n",
    "    from_unixtime,\n",
    ")\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    DateType,\n",
    "    NumericType,\n",
    ")\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from datetime import date\n",
    "\n",
    "# == Pandas, Seaborn, Matplotlib == \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "\n",
    "# == Global Functions == \n",
    "from functions import * # Import functions from the functions module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ Helper Functions ===================\n",
    "\n",
    "# == Miscellaneous == \n",
    "def drop_constant_columns(df):\n",
    "    \"\"\"\n",
    "    Removes all columns in the DataFrame that have only one distinct value.\n",
    "    Returns a new DataFrame with those columns removed.\n",
    "    \"\"\"\n",
    "    cols_to_drop = []\n",
    "\n",
    "    for column_name in df.columns:\n",
    "        if df.select(col(column_name)).distinct().count() <= 1:\n",
    "            cols_to_drop.append(column_name)\n",
    "\n",
    "    print(f\"âš ï¸ Dropping constant columns: {cols_to_drop}\")\n",
    "    return df.drop(*cols_to_drop)\n",
    "\n",
    "# == Outliers Handling ==\n",
    "def compute_outlier_pct(df, col_name, lower_pct=0.25, upper_pct=0.75):\n",
    "    '''Computes pct of outliers per column based on IQR method '''\n",
    "    \n",
    "    # 1. Compute percentile bounds\n",
    "    quantiles = df.approxQuantile(col_name, [lower_pct, upper_pct], 0.01)\n",
    "    q1, q3 = quantiles[0], quantiles[1]\n",
    "    iqr = q3 - q1 \n",
    "\n",
    "    # 2. Obtain lower and upper bound, any data points outside of this are seen as outliers \n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    total_rows = df.count()\n",
    "\n",
    "    return round(df.filter( (col(col_name) < lower_bound) | (col(col_name) > upper_bound) ).count() / total_rows * 100, 2) \n",
    "\n",
    "def display_distributions(df): \n",
    "    \"\"\"Takes in Spark Dataframe. Samples it and display distribution for skewness checking\"\"\"\n",
    "    # 1. Select numerical columns\n",
    "    numeric_cols = [field.name for field in df.schema if isinstance(field.dataType, NumericType)]\n",
    "\n",
    "    # 2. Sample small portion of data (e.g., 5%) and convert to pandas\n",
    "    sample_df = df.select(numeric_cols).sample(fraction=0.1, seed=42)\n",
    "    sample_pdf = sample_df.toPandas()\n",
    "\n",
    "    # 3. Plot histograms as subplots\n",
    "    n_cols = 3  # Number of plots per row\n",
    "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col_name in enumerate(numeric_cols):\n",
    "        axes[i].hist(sample_pdf[col_name].dropna(), bins=50, color='skyblue')\n",
    "        axes[i].set_title(col_name, fontsize=10)\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def inspect_outliers_iqr(df: DataFrame, columns: list, sample_size: int = 5):\n",
    "    for col_name in columns:\n",
    "        try:\n",
    "            print(f\"\\nðŸ“Š Inspecting Outliers for Column: `{col_name}`\")\n",
    "\n",
    "            # Step 1: Calculate Q1, Q3, and IQR\n",
    "            q1, q3 = df.approxQuantile(col_name, [0.25, 0.75], 0.01)\n",
    "            iqr = q3 - q1\n",
    "            lower = q1 - 1.5 * iqr\n",
    "            upper = q3 + 1.5 * iqr\n",
    "\n",
    "            print(f\"Q1 = {q1}, Q3 = {q3}, IQR = {iqr}\")\n",
    "            print(f\"Lower Bound = {lower}, Upper Bound = {upper}\")\n",
    "\n",
    "            # Step 2: Count outliers\n",
    "            outlier_count = df.filter((col(col_name) < lower) | (col(col_name) > upper)).count()\n",
    "            total_count = df.count()\n",
    "            outlier_pct = round(outlier_count / total_count * 100, 2)\n",
    "            print(f\"Outlier Count: {outlier_count} ({outlier_pct}%)\")\n",
    "\n",
    "            # Step 3: Sample outlier values (top and bottom)\n",
    "            print(f\"ðŸ”¼ Top Outliers (>{upper}):\")\n",
    "            df.filter(col(col_name) > upper).select(col_name).orderBy(col(col_name).desc()).show(sample_size)\n",
    "\n",
    "            print(f\"ðŸ”½ Bottom Outliers (<{lower}):\")\n",
    "            df.filter(col(col_name) < lower).select(col_name).orderBy(col(col_name)).show(sample_size)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Could not process column `{col_name}`: {str(e)}\")\n",
    "\n",
    "def winsorise_col(df, col_name,operator: str,  condition_val, final_val):\n",
    "    \"\"\"\n",
    "    Winsorises a column by replacing values above a certain condition with a final value.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        col_name (str): The name of the column to winsorise.\n",
    "        condition_val (float): The value above which to replace with final_val (cut-off)\n",
    "        final_val (float): The value to replace with.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with the winsorised column.\n",
    "    \"\"\"\n",
    "    print(\"âœ… Winsorising column:\", col_name, \"...\")\n",
    "    \n",
    "    if operator == '<':\n",
    "        return df.withColumn(col_name, when(col(col_name) < condition_val, final_val).otherwise(col(col_name)))\n",
    "    \n",
    "    elif operator == '>':\n",
    "        return df.withColumn(col_name, when(col(col_name) > condition_val, final_val).otherwise(col(col_name)))\n",
    "\n",
    "def retain_rows(df: DataFrame, col_name: str, condition_val: float, operator: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Retains rows in the DataFrame where the specified column meets a condition.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with the specified rows dropped.\n",
    "    \"\"\"\n",
    "    \n",
    "    if operator == '<=':\n",
    "        return df.filter(col(col_name) <= condition_val)    \n",
    "    \n",
    "    elif operator == '<':\n",
    "        return df.filter(col(col_name) < condition_val)\n",
    "    \n",
    "    elif operator == '>':\n",
    "        return df.filter(col(col_name) > condition_val)\n",
    "    \n",
    "    elif operator == '>=':\n",
    "        return df.filter(col(col_name) >= condition_val)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Operator must be '>=' or '<='\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Outlier Handling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Highly Skewed Features Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's deal with skewness issues. We will group the following numerical features into the following categories\n",
    "- **Highly Right Skewed (Pct / Probability)**: Logit Transformation\n",
    "- **Highly Right Skewed (Continuous Feature)**: Log Transformation, Else Box-Cox \n",
    "- **Discrete Variables**: WoE binning in Feature Selection \n",
    "\n",
    "\n",
    "âš ï¸ However ... At this checkpoint, I chanced upon how the usage of WoE binning and Feature Selection was robust to outliers and skewness. For example, outliers can be binned into a separate category of itself in WoE binning. Upon this, the WoE encoding of continuous column replaces the original continuous column with each row's bin's WoE value (log-odds scale). Since we are not feeding raw scales of continuous values into our future model, there will not be skewness fixing or transformations needed. \n",
    "\n",
    "Since we still made some minor changes, i.e. removing placeholders and illogical values of some of our features, let's log it to `wandb` to simulate the adherence to strict Basel regulations. "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3. Data Preprocessing (Data Scientist)",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
