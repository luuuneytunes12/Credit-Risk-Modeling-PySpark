{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4aba98c-df7e-4b63-a56a-9a95d5768fd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook aims to simulate a data scientist consuming data from a data pipeline, using PySpark. \n",
    "\n",
    "In this notebook, I will be executing the following steps:\n",
    "  - Conduct EDA on data produced from Medallion Data Pipeline (Missing Values, Identifying Distributions and Relationships etc)\n",
    "  - Deal with Multicollinearity \n",
    "  - Feature Selection & Transformation, Standardisation, Dimensionality Reduction \n",
    "  - Dealing with Dataset Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4c9ffd9-2a32-4278-8829-300ccd6b39bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c951cc11-98a2-4550-a49b-e0bd9ef9cf38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, when, count, desc, isnan, isnull, lit, length, trim, lower, upper, to_date, concat_ws,  regexp_extract, sum \n",
    ")\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, IntegerType, DateType, NumericType\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3e72bbf-1ccd-4a87-8c08-aad374b759d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. EDA: Summary Statistics & Identify Data Issues\n",
    "\n",
    "In this section, I will be acting as a data scientist (credit risk modeling) pulling data from the Gold Delta Layer of the Medallion Structure. I will be mainly observing summary statistics, spotting and solving issues (e.g. missing values), understanding distribution of features etc. [](url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73ca1bfb-bea0-4057-96c1-87527f982bea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table('gold.medallion_cleaned_lc_data')\n",
    "df.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55e3bd83-90c6-4bcc-9f73-b16a3d7bb5ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.summary().display() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37c872d4-1179-4a3f-9cb9-9828beb0e9b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Based on the summary statistics of the Lending Club dataset, several data quality issues become apparent. \n",
    "- Some columns are unusable, due to to having a **large percentage of missing values** (taken reference from `../sandbox/string_issues`)\n",
    "\n",
    "- A number of features exhibit **missing values**, including `emp_length` etc. This requires imputation strategies to be implemented\n",
    "\n",
    "- Some columns are irrelevant to our credit risk modeling project, due to **high cardinality** (large number of unique values), e.g. `emp_title` **(categorical data)**. \n",
    "\n",
    "- There are also **redundant columns**, like `member_id`, which provides no value to our prediction of LGD, EAD and PD.\n",
    "\n",
    "- There are also **post-loan information**. This means that the value of these features are generated after loan origination (attaining application and approval). Hence, such features should be dropped, since they would skew our subsequent machine learning models. Such features include `total_pymnt`, `last_pymnt_d`\n",
    "\n",
    "- There are features like `delinq_2yrs` which have **outliers** (maximum data point way above the 75% quartile)\n",
    "\n",
    "- There are some **invalid data points**, e.g. `dti` being > 100%. Such data points should be removed manually, since they are not handled well enough by the Medallion Architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ab7a5c9-ff60-4788-95e3-7f0480de1f6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. EDA: Feature Handling \n",
    "\n",
    "Based on the above issues identified by me with the Lending Club Dataset, I will now be tackling each of them in order. \n",
    "\n",
    "### 3.1 Find Null Value % Per Column \n",
    "For this credit risk modeling project, I will be dropping columns with &gt; 50% missingness.  Many credit risk modelling projects on Kaggle and Github use 50%-65% missingness as the threshold to drop columns as well. \n",
    "\n",
    "However, let's display the columns which have >=50% missing values first to inspect them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ea8365d-0402-423d-afec-ee48b89850ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get total number of rows\n",
    "total_rows = df.count()\n",
    "\n",
    "# Calculate % of nulls per column and keep only those ≥ 50%\n",
    "missing_val_threshold = 30\n",
    "\n",
    "high_missingness_columns = []\n",
    "\n",
    "for column in df.columns:\n",
    "    null_count = df.select(sum(col(column).isNull().cast(\"int\"))).collect()[0][0]\n",
    "    null_pct = (null_count / total_rows) * 100\n",
    "    if null_pct >= missing_val_threshold:\n",
    "        print(f\"{column}: {null_pct:.2f}% null\")\n",
    "        high_missingness_columns.append(column)\n",
    "\n",
    "# Drop columns with >= 50% missing values (Low predictive power upon inspection)\n",
    "df = df.drop(*high_missingness_columns) \n",
    "print(\"\\n✅ Columns with high pct of missing values dropped ... \\n\")\n",
    "\n",
    "# Inspect Dimensions\n",
    "\n",
    "num_rows = df.count()\n",
    "\n",
    "num_cols = len(df.columns)\n",
    "\n",
    "print(f\"Updated Shape: ({num_rows}, {num_cols})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11ab08d6-63d5-451c-8a3a-f73342625245",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.3 Dropping Irrelevant/Redundant Columns \n",
    "This section implements the removal of **meaningless columns, features which has high cardinality (categorical data), features with little predictive value, e.g. `member_id`, `emp_title` etc, and post-loan features**. Including such features may lead to multicollinearity, and ultimately lead to low predictive power of our credit models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a48b78d5-5d11-40d3-b519-5621821ff271",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reasons why I removed certain columns are as shown: \n",
    "- Columns with `inv`: Largely same as its subset, e.g. `total_pymnt_inv` is largely the same as `total_pymnt`\n",
    "\n",
    "- `last_pymnt_d` and `last_credit_pull_d` (according to Data Dictionary) have little predictive value even after feature engineering. It merely shows the last payment date by borrower and last date where credit report is pulled. This has little value in predicting PD, LGD or EAD. \n",
    "\n",
    "- `sub_grade` is more granular than `grade`. This may lead to a risk of overfitting of our PD, LGD, and EAD models. \n",
    "\n",
    "- High Cardinality Columns may lead to high computational costs in encoding for machine learning models, which makes it undesirable in a big data space such as credit risk. \n",
    "\n",
    "- Hardship & Settlement Features (Borrowers are only eligible for hardship and settlement programmes after loan origination for Lending Club, not when they apply for it). Borrowers will contact lenders of financial hardship, attempting to settle with lenders for interest-fee payments or lower principal sum payments. Such features should not be used to predict PD, LGD, and EAD. My models should not know if a borrower will fall into hardship for this credit risk modeling project \n",
    "\n",
    "- `disbursement_method` indicates how loan funds are delivered to the borrower. This has little relevance in predicting PD, LGD or EAD. \n",
    "\n",
    "- 🚩 Low Variance Features may lead to slower running of PCA (which aims to reduce dimensionality). They also add little value to prediction of PD, EAD and LGD. (Dealt after standardisation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c52e8e1a-c702-49b6-894d-d42ebe068fde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop Derived/Meaningless Features \n",
    "derived_features = [\"funded_amnt_inv\", \"sub_grade\", \"out_prncp_inv\", \"total_pymnt_inv\", \"last_pymnt_d\", \"last_credit_pull_d\"] \n",
    "df = df.drop(*derived_features)\n",
    "print(f\"✅ Derived/Meaningless Features Dropped ...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f38dc896-8e87-4abf-af0a-e50bf306cc5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop High Cardinality Features \n",
    "\n",
    "# 1. Define Threshold \n",
    "high_cardinality_threshold = 50\n",
    "\n",
    "# 2. Find Categorical Features (to identify high cardinality columns)\n",
    "\n",
    "\n",
    "categorical_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, StringType)]\n",
    "print(categorical_cols)\n",
    "\n",
    "# 3. Identify high-cardinality columns\n",
    "high_card_cols = []\n",
    "\n",
    "for col_name in categorical_cols:\n",
    "    unique_count = df.select(col_name).distinct().count()\n",
    "\n",
    "    if unique_count >= high_cardinality_threshold:\n",
    "        print(f\"\\n{col_name} has {unique_count} unique values → dropping ... \")\n",
    "        high_card_cols.append(col_name)\n",
    "\n",
    "# 4. Drop high cardinality columns\n",
    "df = df.drop(*high_card_cols)\n",
    "\n",
    "print(f\"\\n✅ High Cardinality Features Dropped ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f040070-a639-42f3-a9fe-5c7c99b4da5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop hardship related columns & miscelleanous columns \n",
    "hardship_columns = [\"hardship_flag\", \"disbursement_method\", \"debt_settlement_flag\", 'policy_code']\n",
    "\n",
    "df = df.drop(*hardship_columns)\n",
    "print(\"✅ Hardship & miscelleneous columns dropped ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcbd9b74-4ce4-4c3c-8628-e7ff81232749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "🚩 I will need to remove post-loan origination features later on for PD prediction. Post-loan origination features such as `recoveries` are needed for LGD, EAD prediction, but not for PD prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ad97788-e613-40c8-8682-cd47b3903888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.4 Impute Missing Values (Categorical & Numerical)\n",
    "After removing unnecessary columns with little predictive power, we will proceed to impute missing values. We will first identify % missing values per column. \n",
    "\n",
    "For numerical columns, median values shall replace missing values, given how we haven't dealt with outliers yet. For categorical columns, mode categories shall be used to replace missing values. Such an approach is common and simplistic, though there are advanced imputation techniques like clustering. However, we shall not lose focus of learning about the credit risk modeling domain in this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da34daa-76a2-42b2-bab1-6a1bc94545c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_rows = df.count()\n",
    "\n",
    "for column in df.columns: \n",
    "    null_count = df.filter(col(column).isNull()).count()\n",
    "    if null_count > 0:\n",
    "        print(f\"{column}: {null_count} null values, {round(null_count/total_rows * 100,2)}% missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f02ae476-9130-4b72-b769-da22d3567742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loop over each column\n",
    "for feature in df.schema.fields:\n",
    "    col_name = feature.name\n",
    "    dtype = feature.dataType\n",
    "\n",
    "    if isinstance(dtype, StringType):\n",
    "        mode_value = (\n",
    "            df.groupBy( col(f\"{col_name}\")  )\n",
    "            .count()\n",
    "            .orderBy(col(\"count\").desc()) \n",
    "            .first()[0]\n",
    "        )\n",
    "\n",
    "        df = df.fillna({f\"{col_name}\": mode_value})\n",
    "\n",
    "\n",
    "\n",
    "    # Impute Numerical Columns with Median\n",
    "    elif isinstance(dtype, IntegerType) or isinstance(dtype, DoubleType):\n",
    "        if df.filter(   col(col_name).isNull()  ).count() > 0:\n",
    "            median_val = df.approxQuantile(col_name, [0.5], 0.01)[0]\n",
    "            df = df.fillna({col_name: median_val})\n",
    "\n",
    "print('✅ Categorical Column Missing Values Filled!')\n",
    "print('✅ Numerical Column Missing Values Filled!')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d986d939-d943-4d72-9630-58bcdb191aad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Double check if there are any missing values before subsequent steps\n",
    "total_rows = df.count()\n",
    "\n",
    "output_arr = []\n",
    "for column in df.columns: \n",
    "    null_count = df.filter(col(column).isNull()).count()\n",
    "    if null_count > 0:\n",
    "        output_arr.append(f\"{column}: {null_count} null values, {round(null_count/total_rows * 100,2)}% missing values.\")\n",
    "\n",
    "if len(output_arr) == 0: \n",
    "    print('✅ No Missing Values Found!')\n",
    "else:\n",
    "    print(output_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33fbcb79-5009-4419-8d94-2a2576bf1f13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_filled_missing = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0d17791-569e-4f3b-b9e8-cd0c210902b2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.5 Handling Outliers \n",
    "\n",
    "It is important to handle outliers since they create skewed distributions, which can distort credit risk models.\n",
    "\n",
    "They can dominate learning, causing bias or overfitting. \n",
    "\n",
    "But in credit risk, it is important to note that some outliers (e.g., bankrupted borrowers) are important signals. Such outliers should not be blindly trimmed. \n",
    "\n",
    "We will first be using `approxQuantile()` method, which is a relatively more computational effective way to identify outliers for big data. This shall be used in computing the % of outliers per numerical column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c72440fd-90cb-4cb7-b4d0-c2945d1b389f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def percent_outliers(df, col_name, lower_pct=0.25, upper_pct=0.75):\n",
    "    # 1. Compute percentile bounds\n",
    "    quantiles = df.approxQuantile(col_name, [lower_pct, upper_pct], 0.01)\n",
    "    q1, q3 = quantiles[0], quantiles[1]\n",
    "    iqr = q3 - q1 \n",
    "\n",
    "    # 2. Obtain lower and upper bound, any data points outside of this are seen as outliers \n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    total_rows = df.count()\n",
    "\n",
    "    return df.filter( (col(col_name) < lower_bound) | (col(col_name) > upper_bound) ).count() / total_rows * 100\n",
    "\n",
    "# 1. After cleaning missing values, find % outliers per column \n",
    "sample_df = df_filled_missing.sample(fraction=0.05, seed=42) # Efficiency \n",
    "\n",
    "\n",
    "outliers_dict = {}\n",
    "\n",
    "for feature in sample_df.schema: \n",
    "    col_name = feature.name \n",
    "    data_type = feature.dataType\n",
    "\n",
    "    if isinstance(data_type, (DoubleType, IntegerType)): \n",
    "        outlier_pct = percent_outliers(sample_df, col_name) \n",
    "        if outlier_pct > 0: \n",
    "            outliers_dict[col_name] = outlier_pct\n",
    "            print(f\"{col_name}: {round(outlier_pct,2) }% \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34145741-b829-4ed8-bb95-d7a710a0f308",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now, we can define concrete rules on how I should deal with outliers. Rules are as shown below: \n",
    "- **Very few outliers (1 - 2%)**: Trim / Drop rows \n",
    "- **Few Outliers & Outliers are Valid but Extreme Entries**: Winsorise (Cap values)\n",
    "\n",
    "- **High % Outliers**: Likely to be heavily right-skewed (to be transformed later on)\n",
    "\n",
    "The goal of dropping and winsorizing is to ensure % of outliers remain slightly below 5%. To ensure these records do not skew our credit risk models, they will be transformed later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6f2c5b7-5588-4650-a818-e7e1929d3cce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Save df to df1 before dealing with outliers \n",
    "df1 = df_filled_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdd47075-f7fc-49a0-a4fe-a07340924833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 3. Perform trimming & winsorisation \n",
    "def winsorize_column(df, col_name, lower_pct=0.25, upper_pct=0.75):\n",
    "    # Get lower and upper bounds\n",
    "    bounds = df.approxQuantile(col_name, [lower_pct, upper_pct], 0.01)\n",
    "    lower, upper = bounds[0], bounds[1]\n",
    "\n",
    "    # Apply winsorization\n",
    "    return df.withColumn(\n",
    "        col_name,  \n",
    "        when(col(col_name) < lower, lower)\n",
    "        .when(col(col_name) > upper, upper)\n",
    "        .otherwise(col(col_name))\n",
    "    )\n",
    "\n",
    "# Debug: ensure dictionary is not empty\n",
    "print(f\"🧮 Total columns with outliers: {len(outliers_dict)}\\n\")\n",
    "\n",
    "# Iterate over each column in outliers_dict\n",
    "for col_name in outliers_dict.keys(): \n",
    "    pct = outliers_dict[col_name]\n",
    "\n",
    "    if 0 < pct <= 1:\n",
    "        print(f\"✅ Dropping rows with outliers in {col_name} ({round(pct, 2)}%) ...\")\n",
    "        q1, q3 = df1.approxQuantile(col_name, [0.25, 0.75], 0.01)\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - 1.5 * iqr\n",
    "        upper = q3 + 1.5 * iqr\n",
    "        df1 = df1.filter((col(col_name) >= lower) & (col(col_name) <= upper))\n",
    "\n",
    "    elif 1 < pct <= 5:\n",
    "        print(f\"✅ Winsorizing {col_name} ({round(pct, 2)}%) ...\")\n",
    "        df1 = winsorize_column(df1, col_name)\n",
    "\n",
    "    else:\n",
    "        print(f\"🚩 Skipping {col_name} ({round(pct, 2)}%) — too many outliers ...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb089a30-d332-47af-95a5-13b50936ca93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Check for outlier % again \n",
    "sample_df1 = df1.sample(fraction=0.05, seed=42) # Efficiency \n",
    "\n",
    "for feature in df1.schema: \n",
    "    col_name = feature.name \n",
    "    data_type = feature.dataType\n",
    "\n",
    "    if isinstance(data_type, (DoubleType, IntegerType)): \n",
    "        outlier_pct = percent_outliers(df1, col_name) \n",
    "        if outlier_pct > 0: \n",
    "            print(f\"{col_name}: {round(outlier_pct,2) }% \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a1fc814-0c96-49f5-ab75-2a6fe7f0f92b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Check distribution for all numerical columns \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import NumericType\n",
    "\n",
    "# 1. Select numerical columns\n",
    "numeric_cols = [field.name for field in df.schema if isinstance(field.dataType, NumericType)]\n",
    "\n",
    "# 2. Sample small portion of data (e.g., 5%) and convert to pandas\n",
    "sample_df2 = df1.select(numeric_cols).sample(fraction=0.05, seed=42)\n",
    "sample_pdf = sample_df2.toPandas()\n",
    "\n",
    "# 3. Plot histograms as subplots\n",
    "n_cols = 3  # Number of plots per row\n",
    "n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col_name in enumerate(numeric_cols):\n",
    "    axes[i].hist(sample_pdf[col_name].dropna(), bins=50, color='skyblue')\n",
    "    axes[i].set_title(col_name, fontsize=10)\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "211b5680-e4da-43c1-b9f6-c9384eee0425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.6 Peform Feature Transfromation (Skewness)\n",
    "We will group the following numerical features into the following categories: \n",
    "- **Weird Bimodal Distributions**: Avoid over-transforming them since this reflects real-world extreme borrowers' behaviour / demographics\n",
    "- **Highly Right Skewed (Continuous)**: Perform log transformation (if skewness issue is unsolved, utilise Box-Cox)\n",
    "- **Highly Right Skewed (Pct / Probability)**: Logit Transformation\n",
    "- **Left Skewed Variables**: Use Power / Exponential Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0708050c-de41-49fd-bfbb-e7e5123d1bc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. Perform feature transformation techniques to address skewness "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "552e17d8-33ea-46a7-acef-51d287ced1f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. EDA: Examining Distributions and Feature Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5746d7a-710a-4663-933a-043718b305dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Bivariate Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b557b75b-515e-465d-afba-d68e9907bc5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Multicollinearity Handling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec34c83e-1514-4d6e-a494-08995b10c8ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Feature Selection & Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a90baa03-b045-41eb-af1f-436e4fdb8f8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Standardisation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5f10639-26a7-4554-910a-cc25d3b40c07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dimensionality Reduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "111e7ed3-6a54-4c58-ba8a-361a2cf3b40d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Handling Dataset Imbalance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a997a1e5-bcd5-4845-8e28-3c206aac0692",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23e253a2-0fa5-4972-bc4d-5e0ac373afad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3. Data Preprocessing (Data Scientist)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
