{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4aba98c-df7e-4b63-a56a-9a95d5768fd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook aims to simulate a data scientist consuming data from a data pipeline, using PySpark. \n",
    "\n",
    "In this notebook, I will be executing the following steps:\n",
    "  - Conduct EDA on data produced from Medallion Data Pipeline (Missing Values, Identifying Distributions and Relationships etc)\n",
    "  - Deal with Multicollinearity \n",
    "  - Feature Selection & Transformation, Standardisation, Dimensionality Reduction \n",
    "  - Dealing with Dataset Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4c9ffd9-2a32-4278-8829-300ccd6b39bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c951cc11-98a2-4550-a49b-e0bd9ef9cf38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, when, count, desc, isnan, isnull, lit, length, trim, lower, upper, to_date, concat_ws,  regexp_extract, sum \n",
    ")\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, IntegerType, DateType, NumericType\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3e72bbf-1ccd-4a87-8c08-aad374b759d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. EDA: Summary Statistics & Identify Data Issues\n",
    "\n",
    "In this section, I will be acting as a data scientist (credit risk modeling) pulling data from the Gold Delta Layer of the Medallion Structure. I will be mainly observing summary statistics, spotting and solving issues (e.g. missing values), understanding distribution of features etc. [](url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73ca1bfb-bea0-4057-96c1-87527f982bea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table('gold.medallion_cleaned_lc_data')\n",
    "df.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55e3bd83-90c6-4bcc-9f73-b16a3d7bb5ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.summary().display() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37c872d4-1179-4a3f-9cb9-9828beb0e9b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Based on the summary statistics of the Lending Club dataset, several data quality issues become apparent. \n",
    "- Some columns are unusable, due to to having a **large percentage of missing values** (taken reference from `../sandbox/string_issues`)\n",
    "\n",
    "- A number of features exhibit **missing values**, including `emp_length` etc. This requires imputation strategies to be implemented\n",
    "\n",
    "- Some columns are irrelevant to our credit risk modeling project, due to **high cardinality** (large number of unique values), e.g. `emp_title` **(categorical data)**. \n",
    "\n",
    "- There are also **redundant columns**, like `member_id`, which provides no value to our prediction of LGD, EAD and PD.\n",
    "\n",
    "- There are also **post-loan information**. This means that the value of these features are generated after loan origination (attaining application and approval). Hence, such features should be dropped, since they would skew our subsequent machine learning models. Such features include `total_pymnt`, `last_pymnt_d`\n",
    "\n",
    "- There are features like `delinq_2yrs` which have **outliers** (maximum data point way above the 75% quartile)\n",
    "\n",
    "- There are some **invalid data points**, e.g. `dti` being > 100%. Such data points should be removed manually, since they are not handled well enough by the Medallion Architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ab7a5c9-ff60-4788-95e3-7f0480de1f6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. EDA: Feature Handling \n",
    "\n",
    "Based on the above issues identified by me with the Lending Club Dataset, I will now be tackling each of them in order. \n",
    "\n",
    "### 3.1 Find Null Value % Per Column \n",
    "For this credit risk modeling project, I will be dropping columns with &gt; 50% missingness.  Many credit risk modelling projects on Kaggle and Github use 50%-65% missingness as the threshold to drop columns as well. \n",
    "\n",
    "However, let's display the columns which have >=50% missing values first to inspect them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ea8365d-0402-423d-afec-ee48b89850ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get total number of rows\n",
    "total_rows = df.count()\n",
    "\n",
    "# Calculate % of nulls per column and keep only those â‰¥ 50%\n",
    "missing_val_threshold = 30\n",
    "\n",
    "high_missingness_columns = []\n",
    "\n",
    "for column in df.columns:\n",
    "    null_count = df.select(sum(col(column).isNull().cast(\"int\"))).collect()[0][0]\n",
    "    null_pct = (null_count / total_rows) * 100\n",
    "    if null_pct >= missing_val_threshold:\n",
    "        print(f\"{column}: {null_pct:.2f}% null\")\n",
    "        high_missingness_columns.append(column)\n",
    "\n",
    "# Drop columns with >= 50% missing values (Low predictive power upon inspection)\n",
    "df = df.drop(*high_missingness_columns) \n",
    "print(\"\\nâœ… Columns with high pct of missing values dropped ... \\n\")\n",
    "\n",
    "# Inspect Dimensions\n",
    "\n",
    "num_rows = df.count()\n",
    "\n",
    "num_cols = len(df.columns)\n",
    "\n",
    "print(f\"Updated Shape: ({num_rows}, {num_cols})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11ab08d6-63d5-451c-8a3a-f73342625245",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.3 Dropping Irrelevant/Redundant Columns \n",
    "This section implements the removal of **meaningless columns, features which has high cardinality (categorical data), features with little predictive value, e.g. `member_id`, `emp_title` etc, and post-loan features**. Including such features may lead to multicollinearity, and ultimately lead to low predictive power of our credit models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a48b78d5-5d11-40d3-b519-5621821ff271",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reasons why I removed certain columns are as shown: \n",
    "- Columns with `inv`: Largely same as its subset, e.g. `total_pymnt_inv` is largely the same as `total_pymnt`\n",
    "\n",
    "- `last_pymnt_d` and `last_credit_pull_d` (according to Data Dictionary) have little predictive value even after feature engineering. It merely shows the last payment date by borrower and last date where credit report is pulled. This has little value in predicting PD, LGD or EAD. \n",
    "\n",
    "- `sub_grade` is more granular than `grade`. This may lead to a risk of overfitting of our PD, LGD, and EAD models. \n",
    "\n",
    "- High Cardinality Columns may lead to high computational costs in encoding for machine learning models, which makes it undesirable in a big data space such as credit risk. \n",
    "\n",
    "- Hardship & Settlement Features (Borrowers are only eligible for hardship and settlement programmes after loan origination for Lending Club, not when they apply for it). Borrowers will contact lenders of financial hardship, attempting to settle with lenders for interest-fee payments or lower principal sum payments. Such features should not be used to predict PD, LGD, and EAD. My models should not know if a borrower will fall into hardship for this credit risk modeling project \n",
    "\n",
    "- `disbursement_method` indicates how loan funds are delivered to the borrower. This has little relevance in predicting PD, LGD or EAD. \n",
    "\n",
    "- ðŸš© Low Variance Features may lead to slower running of PCA (which aims to reduce dimensionality). They also add little value to prediction of PD, EAD and LGD. (Dealt after standardisation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c52e8e1a-c702-49b6-894d-d42ebe068fde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop Derived/Meaningless Features \n",
    "derived_features = [\"funded_amnt_inv\", \"sub_grade\", \"out_prncp_inv\", \"total_pymnt_inv\", \"last_pymnt_d\", \"last_credit_pull_d\"] \n",
    "df = df.drop(*derived_features)\n",
    "print(f\"âœ… Derived/Meaningless Features Dropped ...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f38dc896-8e87-4abf-af0a-e50bf306cc5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop High Cardinality Features \n",
    "\n",
    "# 1. Define Threshold \n",
    "high_cardinality_threshold = 50\n",
    "\n",
    "# 2. Find Categorical Features (to identify high cardinality columns)\n",
    "\n",
    "\n",
    "categorical_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, StringType)]\n",
    "print(categorical_cols)\n",
    "\n",
    "# 3. Identify high-cardinality columns\n",
    "high_card_cols = []\n",
    "\n",
    "for col_name in categorical_cols:\n",
    "    unique_count = df.select(col_name).distinct().count()\n",
    "\n",
    "    if unique_count >= high_cardinality_threshold:\n",
    "        print(f\"\\n{col_name} has {unique_count} unique values â†’ dropping ... \")\n",
    "        high_card_cols.append(col_name)\n",
    "\n",
    "# 4. Drop high cardinality columns\n",
    "df = df.drop(*high_card_cols)\n",
    "\n",
    "print(f\"\\nâœ… High Cardinality Features Dropped ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f040070-a639-42f3-a9fe-5c7c99b4da5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop hardship related columns & miscelleanous columns \n",
    "hardship_columns = [\"hardship_flag\", \"disbursement_method\", \"debt_settlement_flag\", 'policy_code']\n",
    "\n",
    "df = df.drop(*hardship_columns)\n",
    "print(\"âœ… Hardship & miscelleneous columns dropped ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcbd9b74-4ce4-4c3c-8628-e7ff81232749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸš© I will need to remove post-loan origination features later on for PD prediction. Post-loan origination features such as `recoveries` are needed for LGD, EAD prediction, but not for PD prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ad97788-e613-40c8-8682-cd47b3903888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.4 Impute Missing Values (Categorical & Numerical)\n",
    "After removing unnecessary columns with little predictive power, we will proceed to impute missing values. We will first identify % missing values per column. \n",
    "\n",
    "For numerical columns, median values shall replace missing values, given how we haven't dealt with outliers yet. For categorical columns, mode categories shall be used to replace missing values. Such an approach is common and simplistic, though there are advanced imputation techniques like clustering. However, we shall not lose focus of learning about the credit risk modeling domain in this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da34daa-76a2-42b2-bab1-6a1bc94545c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_rows = df.count()\n",
    "\n",
    "for column in df.columns: \n",
    "    null_count = df.filter(col(column).isNull()).count()\n",
    "    if null_count > 0:\n",
    "        print(f\"{column}: {null_count} null values, {round(null_count/total_rows * 100,2)}% missing values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0d17791-569e-4f3b-b9e8-cd0c210902b2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.5 Handling Outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5034a12-d6d8-4b0c-8dcb-0340aacbeca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Save Cleaned Data for Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "552e17d8-33ea-46a7-acef-51d287ced1f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. EDA: Examining Distributions and Feature Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a451a6c-be9e-4532-8c26-0579adfefb80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Univariate Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5746d7a-710a-4663-933a-043718b305dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Bivariate Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b557b75b-515e-465d-afba-d68e9907bc5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Multicollinearity Handling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec34c83e-1514-4d6e-a494-08995b10c8ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Feature Selection & Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a90baa03-b045-41eb-af1f-436e4fdb8f8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Standardisation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5f10639-26a7-4554-910a-cc25d3b40c07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dimensionality Reduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "111e7ed3-6a54-4c58-ba8a-361a2cf3b40d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Handling Dataset Imbalance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a997a1e5-bcd5-4845-8e28-3c206aac0692",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23e253a2-0fa5-4972-bc4d-5e0ac373afad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3. Data Preprocessing (Data Scientist)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
