{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c3f2a38",
   "metadata": {},
   "source": [
    "This notebook aims to conduct feature engineering and final model building for our PD Model. Order of feature engineering is as followed:\n",
    "\n",
    "1. Creation of Features\n",
    "2. Feature Selection via IV <-> WoE binning selected features\n",
    "\n",
    "This sequence is chosen due to the following reasons:\n",
    "\n",
    "- Reduce chance of discarding features, which may only become useful after feature engineering\n",
    "- Generating interaction features allow me to rank / select variables which are relevant and useful for PD Modeling\n",
    "- Multicollinearity checks can be conducted once, instead of being repeated multiple times\n",
    "\n",
    "# 0. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e6063b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/lunlun/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwlunlun1212\u001b[0m (\u001b[33mwlunlun1212-singapore-management-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/lunlun/.netrc\n",
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/16 16:14:35 WARN Utils: Your hostname, Chengs-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.77 instead (on interface en0)\n",
      "25/08/16 16:14:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Users/lunlun/Downloads/Github/Credit-Risk-Modeling-PySpark/venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/lunlun/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /Users/lunlun/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b1ba6766-ca99-439b-b47a-c9aeae5bc498;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 77ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b1ba6766-ca99-439b-b47a-c9aeae5bc498\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/2ms)\n",
      "25/08/16 16:14:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/16 16:14:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/08/16 16:14:36 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/08/16 16:14:36 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Standard libraries ===\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# === WandB Logging  ===\n",
    "import wandb\n",
    "\n",
    "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n",
    "\n",
    "# == Global Functions ==\n",
    "from functions import *\n",
    "\n",
    "\n",
    "# === Spark Session & Functions ===\n",
    "from init_spark import start_spark\n",
    "\n",
    "spark = start_spark()\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    when,\n",
    "    count,\n",
    "    desc,\n",
    "    isnan,\n",
    "    isnull,\n",
    "    lit,\n",
    "    length,\n",
    "    trim,\n",
    "    lower,\n",
    "    upper,\n",
    "    to_date,\n",
    "    concat_ws,\n",
    "    regexp_extract,\n",
    "    mean,\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    DateType,\n",
    "    NumericType,\n",
    "    FloatType,\n",
    "    LongType,\n",
    ")\n",
    "\n",
    "\n",
    "# === Pandas Dataframe & WoE Binning ===\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from optbinning import OptimalBinning\n",
    "import numpy as np\n",
    "\n",
    "# == Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Machine Learning ===\n",
    "from sklearn.linear_model import LogisticRegression as SkLogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "\n",
    "# == Optbinning ==\n",
    "from optbinning import OptimalPWBinning\n",
    "\n",
    "\n",
    "# === Load Environment Variables ===\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6e56516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ Constants ======================\n",
    "\n",
    "TARGET_COL = \"default_status\"\n",
    "SAMPLE_FRAC = 0.2\n",
    "SEED = 42\n",
    "NOTEBOOK_RUN_NAME = \"Feature Engineering\"\n",
    "\n",
    "IV_THRESHOLDS = {\n",
    "    \"useless (< 0.02)\": 0.02,\n",
    "    \"weak (< 0.1)\": 0.1,\n",
    "    \"medium (< 0.3)\": 0.3,\n",
    "    \"strong (< 0.5)\": 0.5,\n",
    "}\n",
    "\n",
    "# === Feature Categorisation ===\n",
    "iv_categories = {\n",
    "    \"useless (< 0.02)\": [],\n",
    "    \"weak (< 0.1)\": [],\n",
    "    \"medium (< 0.3)\": [],\n",
    "    \"strong (< 0.5)\": [],\n",
    "    \"suspicious\": [],\n",
    "    \"no_variation\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c713f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Reusable Functions ======================\n",
    "\n",
    "\n",
    "# == 0. Outliers ==\n",
    "# == Outliers Handling ==\n",
    "def compute_outlier_pct(df, col_name, lower_pct=0.25, upper_pct=0.75):\n",
    "    \"\"\"Computes pct of outliers per column based on IQR method\"\"\"\n",
    "\n",
    "    # 1. Compute percentile bounds\n",
    "    quantiles = df.approxQuantile(col_name, [lower_pct, upper_pct], 0.01)\n",
    "    q1, q3 = quantiles[0], quantiles[1]\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # 2. Obtain lower and upper bound, any data points outside of this are seen as outliers\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    total_rows = df.count()\n",
    "\n",
    "    return round(\n",
    "        df.filter((col(col_name) < lower_bound) | (col(col_name) > upper_bound)).count()\n",
    "        / total_rows\n",
    "        * 100,\n",
    "        2,\n",
    "    )\n",
    "\n",
    "\n",
    "def display_distributions(df):\n",
    "    \"\"\"Takes in Spark Dataframe. Samples it and display distribution for skewness checking\"\"\"\n",
    "    # 1. Select numerical columns\n",
    "    numeric_cols = [\n",
    "        field.name for field in df.schema if isinstance(field.dataType, NumericType)\n",
    "    ]\n",
    "\n",
    "    # 2. Sample small portion of data (e.g., 5%) and convert to pandas\n",
    "    sample_df = df.select(numeric_cols).sample(fraction=0.1, seed=42)\n",
    "    sample_pdf = sample_df.toPandas()\n",
    "\n",
    "    # 3. Plot histograms as subplots\n",
    "    n_cols = 3  # Number of plots per row\n",
    "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col_name in enumerate(numeric_cols):\n",
    "        axes[i].hist(sample_pdf[col_name].dropna(), bins=50, color=\"skyblue\")\n",
    "        axes[i].set_title(col_name, fontsize=10)\n",
    "        axes[i].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def inspect_outliers_iqr(df: DataFrame, columns: list, sample_size: int = 5):\n",
    "    for col_name in columns:\n",
    "        try:\n",
    "            print(f\"\\nðŸ“Š Inspecting Outliers for Column: `{col_name}`\")\n",
    "\n",
    "            # Step 1: Calculate Q1, Q3, and IQR\n",
    "            q1, q3 = df.approxQuantile(col_name, [0.25, 0.75], 0.01)\n",
    "            iqr = q3 - q1\n",
    "            lower = q1 - 1.5 * iqr\n",
    "            upper = q3 + 1.5 * iqr\n",
    "\n",
    "            print(f\"Q1 = {q1}, Q3 = {q3}, IQR = {iqr}\")\n",
    "            print(f\"Lower Bound = {lower}, Upper Bound = {upper}\")\n",
    "\n",
    "            # Step 2: Count outliers\n",
    "            outlier_count = df.filter(\n",
    "                (col(col_name) < lower) | (col(col_name) > upper)\n",
    "            ).count()\n",
    "            total_count = df.count()\n",
    "            outlier_pct = round(outlier_count / total_count * 100, 2)\n",
    "            print(f\"Outlier Count: {outlier_count} ({outlier_pct}%)\")\n",
    "\n",
    "            # Step 3: Sample outlier values (top and bottom)\n",
    "            print(f\"ðŸ”¼ Top Outliers (>{upper}):\")\n",
    "            df.filter(col(col_name) > upper).select(col_name).orderBy(\n",
    "                col(col_name).desc()\n",
    "            ).show(sample_size)\n",
    "\n",
    "            print(f\"ðŸ”½ Bottom Outliers (<{lower}):\")\n",
    "            df.filter(col(col_name) < lower).select(col_name).orderBy(\n",
    "                col(col_name)\n",
    "            ).show(sample_size)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Could not process column `{col_name}`: {str(e)}\")\n",
    "\n",
    "\n",
    "def winsorise_col(df, col_name, operator: str, condition_val, final_val):\n",
    "    \"\"\"\n",
    "    Winsorises a column by replacing values above a certain condition with a final value.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        col_name (str): The name of the column to winsorise.\n",
    "        condition_val (float): The value above which to replace with final_val (cut-off)\n",
    "        final_val (float): The value to replace with.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with the winsorised column.\n",
    "    \"\"\"\n",
    "    print(\"âœ… Winsorising column:\", col_name, \"...\")\n",
    "\n",
    "    if operator == \"<\":\n",
    "        return df.withColumn(\n",
    "            col_name,\n",
    "            when(col(col_name) < condition_val, final_val).otherwise(col(col_name)),\n",
    "        )\n",
    "\n",
    "    elif operator == \">\":\n",
    "        return df.withColumn(\n",
    "            col_name,\n",
    "            when(col(col_name) > condition_val, final_val).otherwise(col(col_name)),\n",
    "        )\n",
    "\n",
    "\n",
    "def retain_rows(\n",
    "    df: DataFrame, col_name: str, condition_val: float, operator: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Retains rows in the DataFrame where the specified column meets a condition.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with the specified rows dropped.\n",
    "    \"\"\"\n",
    "\n",
    "    if operator == \"<=\":\n",
    "        return df.filter(col(col_name) <= condition_val)\n",
    "\n",
    "    elif operator == \"<\":\n",
    "        return df.filter(col(col_name) < condition_val)\n",
    "\n",
    "    elif operator == \">\":\n",
    "        return df.filter(col(col_name) > condition_val)\n",
    "\n",
    "    elif operator == \">=\":\n",
    "        return df.filter(col(col_name) >= condition_val)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Operator must be '>=' or '<='\")\n",
    "\n",
    "\n",
    "# == 1. Feature Engineering ==\n",
    "def add_ratio_with_flag(df, numerator, denominator, ratio_col, flag_col, fill_value=-1):\n",
    "    \"\"\"\n",
    "    Adds flag column to indicate invalid value upon division. Adds engineered feature column, and fills -1 upon invalid\n",
    "    division operation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: str\n",
    "        Spark Dataframe\n",
    "\n",
    "    numerator: str\n",
    "        Numerator Column Name\n",
    "    denominator: str\n",
    "        Denominator Column Name\n",
    "\n",
    "    ratio_col: str\n",
    "        Engineered Feature Name (Create Yourself)\n",
    "\n",
    "    flag_col: str\n",
    "        For e.g. annual_inc/loan_ratio_invalid\n",
    "\n",
    "    fill_value: int\n",
    "        Default: -1 (Placeholder in flag column, if invalid division operation)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Spark Dataframe with new columns (flag and divided column)\n",
    "    \"\"\"\n",
    "    # ==  invalid denom (null or <= 0) -> 0 or -1 will be in flag columnis ==\n",
    "    df = df.withColumn(\n",
    "        flag_col,\n",
    "        when(\n",
    "            (col(denominator).isNull()) | (col(denominator) <= 0), 1\n",
    "        ).otherwise(  # invalid â†’ 1\n",
    "            0\n",
    "        ),  # valid â†’ 0\n",
    "    )\n",
    "\n",
    "    # == safe ratio: real divide only when valid, else sentinel ==\n",
    "    df = df.withColumn(\n",
    "        ratio_col,\n",
    "        when(col(flag_col) == 0, col(numerator) / col(denominator)).otherwise(\n",
    "            lit(fill_value)\n",
    "        ),\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# == 2. Feature Selection (WoE & IV) ==\n",
    "def get_numerical_cols(df, target_col):\n",
    "    \"\"\"\n",
    "    Get numerical columns of a Spark Dataframe and return list of numerical columns,\n",
    "    excluding 'default_status'\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "\n",
    "    df: Dataframe\n",
    "        Spark Dataframe\n",
    "\n",
    "    target_col: str\n",
    "        default_status\n",
    "\n",
    "    \"\"\"\n",
    "    return [\n",
    "        f.name\n",
    "        for f in df.schema.fields\n",
    "        if isinstance(f.dataType, NumericType) and f.name != target_col\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_categorical_cols(df, target_col):\n",
    "    \"\"\"\n",
    "    Get categorical columns of a Spark Dataframe and return list of\n",
    "    string columns\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "\n",
    "    df: Dataframe\n",
    "        Spark Dataframe\n",
    "\n",
    "    target_col: str\n",
    "        default_status\n",
    "\n",
    "    \"\"\"\n",
    "    return [\n",
    "        f.name\n",
    "        for f in df.schema.fields\n",
    "        if isinstance(f.dataType, StringType) and f.name != target_col\n",
    "    ]\n",
    "\n",
    "\n",
    "def classify_iv(iv_categories, feature, iv_score):\n",
    "    \"\"\"Adds 1 feature into respective IV categories of iv_categories dictionary\"\"\"\n",
    "    for label, threshold in IV_THRESHOLDS.items():\n",
    "        if iv_score < threshold:\n",
    "            iv_categories[label].append((feature, iv_score))\n",
    "            return\n",
    "\n",
    "    iv_categories[\"suspicious\"].append((feature, iv_score))\n",
    "\n",
    "\n",
    "def bin_and_classify_feature(\n",
    "    feature,\n",
    "    x,\n",
    "    y,\n",
    "    dtype=\"numerical\",\n",
    "    monotonic_trend_type=\"auto\",\n",
    "    min_bin_size=0.05,\n",
    "    max_n_bins=5,\n",
    "    iv_categories=iv_categories,\n",
    "):\n",
    "    \"\"\"\n",
    "    Sample first before attempting to inspect binning & classifying into different IV Categories.\n",
    "    Ensure only train_df (Pandas) fed in this function\n",
    "\n",
    "    - Conducts Optimal Binning on a feature.\n",
    "    - Outputs WoE table & Plot (to observe monotonic trend)\n",
    "    - Categorises 1 feature into `iv_categories` dictionary\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - feature: str\n",
    "            Name of Feature\n",
    "    - x: series\n",
    "            e.g. train_df[feature]\n",
    "    - y: series\n",
    "            e.g. train_df['default_status']\n",
    "    - dtype: str\n",
    "            e.g. numerical (continuous /discrete / ordinal) & categorical (nominal) for OptBinning library\n",
    "    - monotonic_trend_type: str\n",
    "            e.g auto (default) / auto_asc_desc\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit binning with automatic solver\n",
    "    optb = OptimalBinning(\n",
    "        name=feature,\n",
    "        dtype=dtype,\n",
    "        monotonic_trend=monotonic_trend_type,\n",
    "        min_bin_size=min_bin_size,  # min bin size is 1% (majority of our features have >= 1% outliers ... )\n",
    "        solver=\"cp\",\n",
    "        max_n_bins=max_n_bins,\n",
    "    )\n",
    "    optb.fit(x, y)\n",
    "\n",
    "    bin_table = optb.binning_table.build()\n",
    "\n",
    "    #  Format binning table to display bins WoE & IV\n",
    "    bin_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Bin\": bin_table[\"Bin\"].astype(str),\n",
    "            \"Count\": round(bin_table[\"Count\"], 4),\n",
    "            \"Default Rate (%)\": round(bin_table[\"Event rate\"], 4),\n",
    "            \"WOE\": bin_table[\"WoE\"],\n",
    "            \"IV\": bin_table[\"IV\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Get total IV for feature\n",
    "    bin_df = bin_df[~bin_df[\"Bin\"].str.contains(\"Total\", na=False)]\n",
    "    total_iv = bin_df.iloc[-1][\"IV\"]\n",
    "    print(f\"âœ… Total IV for {feature}: {total_iv:.4f}\")\n",
    "\n",
    "    # Add feature to iv_categories\n",
    "    classify_iv(iv_categories, feature, total_iv)\n",
    "\n",
    "    # Print binning table\n",
    "    print(tabulate(bin_df, headers=\"keys\", tablefmt=\"fancy_grid\", showindex=False))\n",
    "\n",
    "    # Plot curve\n",
    "    optb.binning_table.plot(metric=\"woe\", figsize=(7, 4))\n",
    "\n",
    "\n",
    "def tx_grade(df):\n",
    "    \"\"\"Takes in Pandas Dataframe and returns correct grade_numeric mapping from 'grade' column.\n",
    "    This ensures ordinal natured of grade to be captured. Returns Pandas Dataframe.\"\"\"\n",
    "    # == Grade = Ordinal -> Dtype = `numerical` ==\n",
    "    grade_map = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7}\n",
    "    df[\"grade_numeric\"] = df[\"grade\"].map(grade_map)\n",
    "\n",
    "    df = df.drop(\"grade\", axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_updated_iv_categories(iv_categories, train_df):\n",
    "\n",
    "    updated_iv_categories = {}\n",
    "    for key, value in iv_categories.items():\n",
    "        new_list = []\n",
    "        if len(value) > 0 and key != \"no_variation\":\n",
    "            for feature, iv_value in value:\n",
    "                if feature in train_df.columns:\n",
    "                    new_list.append((feature, iv_value))\n",
    "            updated_iv_categories[key] = new_list\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return updated_iv_categories\n",
    "\n",
    "\n",
    "def plot_iv_by_category(iv_categories):\n",
    "    # === 1. Flatten into a clean DataFrame ===\n",
    "    rows = []\n",
    "    for category, items in iv_categories.items():\n",
    "        for it in items:\n",
    "            if isinstance(it, (tuple, list)) and len(it) >= 2:\n",
    "                feature, iv = it[0], it[1]\n",
    "                rows.append({\"Feature\": feature, \"IV\": float(iv), \"Category\": category})\n",
    "            elif isinstance(it, str):\n",
    "                rows.append({\"Feature\": it, \"IV\": 0.0, \"Category\": category})\n",
    "\n",
    "    df_iv = pd.DataFrame(rows).sort_values(by=\"Feature\")\n",
    "\n",
    "    # === 2. Create faceted plots, showing only relevant features in each ===\n",
    "    g = sns.catplot(\n",
    "        data=df_iv,\n",
    "        kind=\"bar\",\n",
    "        x=\"IV\",\n",
    "        y=\"Feature\",\n",
    "        col=\"Category\",\n",
    "        col_wrap=2,\n",
    "        height=6,\n",
    "        aspect=1.2,\n",
    "        sharex=False,  # <== allow different x scales\n",
    "        sharey=False,  # <== allow different features per facet\n",
    "        palette=\"Set2\",\n",
    "    )\n",
    "\n",
    "    g.figure.subplots_adjust(top=0.9, wspace=0.9)\n",
    "\n",
    "    # === 3. Annotate bars ===\n",
    "    for ax in g.axes.flat:\n",
    "        for bar in ax.patches:\n",
    "            width = bar.get_width()\n",
    "            if width > 0:\n",
    "                y = bar.get_y() + bar.get_height() / 2\n",
    "                ax.text(width + 0.002, y, f\"{width:.3f}\", va=\"center\", fontsize=8)\n",
    "\n",
    "    # === 4. Final polish ===\n",
    "    g.set_titles(col_template=\"{col_name}\")\n",
    "    g.set_xlabels(\"IV Score\")\n",
    "    g.set_ylabels(\"Feature\")\n",
    "    g.figure.suptitle(\"Information Value (IV) by Feature Category\", fontsize=16)\n",
    "    plt.show()\n",
    "    return g\n",
    "\n",
    "\n",
    "def peek_iv_score(updated_iv_categories, string_name, train_pdf):\n",
    "    res_list = []\n",
    "    for key, iv_list in updated_iv_categories.items():\n",
    "        for feature, iv_score in iv_list:\n",
    "            if (string_name in feature) and (feature in train_pdf.columns):\n",
    "                res_list.append((feature, iv_score))\n",
    "\n",
    "    return res_list\n",
    "\n",
    "\n",
    "# == 3. WoE Transformation ==\n",
    "def woe_bin_transform_train(df, non_mono_cols, target_col=TARGET_COL, monotonic=\"auto\"):\n",
    "    \"\"\"\n",
    "    Takes in train dataframe (Pandas)\n",
    "    For each feature, separate Optbinning model is trained separately.\n",
    "    If there are non-monotonic columns, use `auto_asc_desc` constraint in Optbinning\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    df: Pandas Dataframe\n",
    "            train_pdf\n",
    "\n",
    "    non_mono_cols: list\n",
    "            non-monotonic columns\n",
    "\n",
    "    target_col: str\n",
    "            default_status (exists in train_pdf.columns)\n",
    "\n",
    "    monotonic: str\n",
    "            By default, monotonic='auto'\n",
    "    \"\"\"\n",
    "\n",
    "    excluded_columns = [\"id\", \"issue_d\", \"default_status\", \"earliest_cr_line\"]\n",
    "    loop_cols = [feature for feature in df.columns if feature not in excluded_columns]\n",
    "\n",
    "    optb_dict = {}\n",
    "\n",
    "    for column in loop_cols:\n",
    "        # == Check if we should enforce monotonic trend on current feature ==\n",
    "        if column in non_mono_cols:\n",
    "            monotonic_trend = \"auto_asc_desc\"\n",
    "        else:\n",
    "            monotonic_trend = monotonic\n",
    "\n",
    "        # == Determine dtype of current column ==\n",
    "        if df[column].dtype == \"object\":\n",
    "            dtype = \"categorical\"\n",
    "        elif pd.api.types.is_numeric_dtype(df[column]):\n",
    "            dtype = \"numerical\"\n",
    "        else:\n",
    "            continue  # skip unknown types\n",
    "\n",
    "        optb = OptimalBinning(\n",
    "            name=column,\n",
    "            dtype=dtype,\n",
    "            monotonic_trend=monotonic_trend,\n",
    "            solver=\"cp\",\n",
    "            min_bin_size=0.05,\n",
    "            max_n_bins=5,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            optb.fit(df[column], df[target_col])\n",
    "            df[column + \"_woe\"] = optb.transform(df[column].to_numpy(), metric=\"woe\")\n",
    "            df = df.drop(column, axis=1)\n",
    "            optb_dict[column] = optb\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error fitting {column}: {e}\")\n",
    "\n",
    "    return (df, optb_dict)\n",
    "\n",
    "\n",
    "def apply_woe_transform(df, optb_dict):\n",
    "    \"\"\"\n",
    "    Uses dictionary outputted by woe_bin_transform_train() to fit test dataframe,\n",
    "    variable -> trainned Optbinning Model on train dataframe's feature.\n",
    "\n",
    "    Transform each of test_pdf feature into `feature_woe`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: Dataframe\n",
    "            test_pdf\n",
    "\n",
    "    optb_dict: dict\n",
    "            Maps feature -> Optbinning Object\n",
    "    \"\"\"\n",
    "    for column, optb in optb_dict.items():\n",
    "        df[column + \"_woe\"] = optb.transform(df[column].to_numpy(), metric=\"woe\")\n",
    "        df = df.drop(columns=[column])\n",
    "    return df\n",
    "\n",
    "\n",
    "def pretty_print_iv_dict(iv_dict):\n",
    "    for bucket, feats in iv_dict.items():\n",
    "        print(f\"\\n{bucket}:\")\n",
    "        for feat, iv in feats:\n",
    "            print(f\"  {feat:<40} {float(iv):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c1674ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Remove all existing runs every time I run this notebook ==\n",
    "NOTEBOOK_RUN_NAME = \"PD Model Building II\"\n",
    "api = wandb.Api()\n",
    "for run in api.runs(\n",
    "    f\"wlunlun1212-singapore-management-university/Credit Risk Modeling\"\n",
    "):\n",
    "    if run.group == NOTEBOOK_RUN_NAME:\n",
    "        run.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9078b5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/16 16:14:41 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>...</th>\n",
       "      <th>pct_tl_nvr_dlq</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <th>total_bal_ex_mort</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>total_il_high_credit_limit</th>\n",
       "      <th>credit_history_years</th>\n",
       "      <th>fico_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87704389</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>10.49</td>\n",
       "      <td>32.50</td>\n",
       "      <td>B</td>\n",
       "      <td>2</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>39975.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>146014.0</td>\n",
       "      <td>60931.0</td>\n",
       "      <td>8300.0</td>\n",
       "      <td>70423.0</td>\n",
       "      <td>10</td>\n",
       "      <td>762.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88862675</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>36</td>\n",
       "      <td>18.99</td>\n",
       "      <td>175.93</td>\n",
       "      <td>D</td>\n",
       "      <td>6</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>132000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>78.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>314046.0</td>\n",
       "      <td>53279.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>54636.0</td>\n",
       "      <td>17</td>\n",
       "      <td>662.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85636448</td>\n",
       "      <td>4075.0</td>\n",
       "      <td>4075.0</td>\n",
       "      <td>36</td>\n",
       "      <td>14.49</td>\n",
       "      <td>140.25</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>RENT</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27142.0</td>\n",
       "      <td>18772.0</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>20342.0</td>\n",
       "      <td>4</td>\n",
       "      <td>752.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88012886</td>\n",
       "      <td>23975.0</td>\n",
       "      <td>23975.0</td>\n",
       "      <td>60</td>\n",
       "      <td>15.59</td>\n",
       "      <td>577.82</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>RENT</td>\n",
       "      <td>89999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>95.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48652.0</td>\n",
       "      <td>14397.0</td>\n",
       "      <td>31500.0</td>\n",
       "      <td>12684.0</td>\n",
       "      <td>18</td>\n",
       "      <td>732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87989149</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>12.79</td>\n",
       "      <td>268.75</td>\n",
       "      <td>C</td>\n",
       "      <td>10</td>\n",
       "      <td>OWN</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55262.0</td>\n",
       "      <td>29807.0</td>\n",
       "      <td>11600.0</td>\n",
       "      <td>34562.0</td>\n",
       "      <td>9</td>\n",
       "      <td>707.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>85582885</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>36</td>\n",
       "      <td>12.79</td>\n",
       "      <td>188.13</td>\n",
       "      <td>C</td>\n",
       "      <td>10</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>109000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>81.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>421855.0</td>\n",
       "      <td>50608.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>58902.0</td>\n",
       "      <td>17</td>\n",
       "      <td>702.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>88494037</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>10.49</td>\n",
       "      <td>194.99</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>OWN</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27352.0</td>\n",
       "      <td>7421.0</td>\n",
       "      <td>19500.0</td>\n",
       "      <td>7852.0</td>\n",
       "      <td>15</td>\n",
       "      <td>807.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>88143505</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>60</td>\n",
       "      <td>15.59</td>\n",
       "      <td>578.42</td>\n",
       "      <td>C</td>\n",
       "      <td>10</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>94.7</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>310036.0</td>\n",
       "      <td>38795.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>28679.0</td>\n",
       "      <td>13</td>\n",
       "      <td>682.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>86216472</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>36</td>\n",
       "      <td>9.49</td>\n",
       "      <td>102.50</td>\n",
       "      <td>B</td>\n",
       "      <td>6</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>94.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112899.0</td>\n",
       "      <td>60713.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>57816.0</td>\n",
       "      <td>12</td>\n",
       "      <td>747.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>87624578</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>36</td>\n",
       "      <td>8.59</td>\n",
       "      <td>113.80</td>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>56.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63980.0</td>\n",
       "      <td>40699.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>54780.0</td>\n",
       "      <td>30</td>\n",
       "      <td>692.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  loan_amnt  funded_amnt  term  int_rate  installment grade  \\\n",
       "0  87704389     1000.0       1000.0    36     10.49        32.50     B   \n",
       "1  88862675     4800.0       4800.0    36     18.99       175.93     D   \n",
       "2  85636448     4075.0       4075.0    36     14.49       140.25     C   \n",
       "3  88012886    23975.0      23975.0    60     15.59       577.82     C   \n",
       "4  87989149     8000.0       8000.0    36     12.79       268.75     C   \n",
       "5  85582885     5600.0       5600.0    36     12.79       188.13     C   \n",
       "6  88494037     6000.0       6000.0    36     10.49       194.99     B   \n",
       "7  88143505    24000.0      24000.0    60     15.59       578.42     C   \n",
       "8  86216472     3200.0       3200.0    36      9.49       102.50     B   \n",
       "9  87624578     3600.0       3600.0    36      8.59       113.80     A   \n",
       "\n",
       "   emp_length home_ownership  annual_inc  ... pct_tl_nvr_dlq percent_bc_gt_75  \\\n",
       "0           2       MORTGAGE     39975.0  ...          100.0              0.0   \n",
       "1           6       MORTGAGE    132000.0  ...           78.6              0.0   \n",
       "2           1           RENT     12000.0  ...          100.0              0.0   \n",
       "3           2           RENT     89999.0  ...           95.2              0.0   \n",
       "4          10            OWN     30000.0  ...          100.0             20.0   \n",
       "5          10       MORTGAGE    109000.0  ...           81.2              0.0   \n",
       "6           0            OWN     55000.0  ...          100.0              0.0   \n",
       "7          10       MORTGAGE     65000.0  ...           94.7             50.0   \n",
       "8           6       MORTGAGE     72000.0  ...           94.3              0.0   \n",
       "9           0           RENT     70000.0  ...           56.2            100.0   \n",
       "\n",
       "   pub_rec_bankruptcies  tax_liens  tot_hi_cred_lim  total_bal_ex_mort  \\\n",
       "0                   0.0        0.0         146014.0            60931.0   \n",
       "1                   0.0        0.0         314046.0            53279.0   \n",
       "2                   0.0        0.0          27142.0            18772.0   \n",
       "3                   0.0        0.0          48652.0            14397.0   \n",
       "4                   1.0        0.0          55262.0            29807.0   \n",
       "5                   0.0        0.0         421855.0            50608.0   \n",
       "6                   0.0        0.0          27352.0             7421.0   \n",
       "7                   0.0        0.0         310036.0            38795.0   \n",
       "8                   0.0        0.0         112899.0            60713.0   \n",
       "9                   0.0        0.0          63980.0            40699.0   \n",
       "\n",
       "   total_bc_limit  total_il_high_credit_limit  credit_history_years  \\\n",
       "0          8300.0                     70423.0                    10   \n",
       "1           500.0                     54636.0                    17   \n",
       "2          5200.0                     20342.0                     4   \n",
       "3         31500.0                     12684.0                    18   \n",
       "4         11600.0                     34562.0                     9   \n",
       "5          3000.0                     58902.0                    17   \n",
       "6         19500.0                      7852.0                    15   \n",
       "7         14000.0                     28679.0                    13   \n",
       "8          4000.0                     57816.0                    12   \n",
       "9          7500.0                     54780.0                    30   \n",
       "\n",
       "   fico_score  \n",
       "0       762.0  \n",
       "1       662.0  \n",
       "2       752.0  \n",
       "3       732.0  \n",
       "4       707.0  \n",
       "5       702.0  \n",
       "6       807.0  \n",
       "7       682.0  \n",
       "8       747.0  \n",
       "9       692.0  \n",
       "\n",
       "[10 rows x 75 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"../data/gold/medallion_cleaned_lc_data\")\n",
    "\n",
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7177f82",
   "metadata": {},
   "source": [
    "### 0.0 Base Model Performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf6303e",
   "metadata": {},
   "source": [
    "### 0.2 Skewness & Outlier Treatment\n",
    "\n",
    "Before feature engineering, we will be dealing with outliers and skewed distributions, which can distort credit risk models. They can dominate learning, causing bias or overfitting in our PD Model. Let's first identify features which are highly skewed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46db2a95",
   "metadata": {},
   "source": [
    "To ensure that our WoE & IV Feature Selection produce better binning (more balanced bins) and smoother WoE (if a feature separates good and bad outcomes properly), we should visualise our distributions to understand our dataset better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c27ec1d",
   "metadata": {},
   "source": [
    "From the original distribution of our cleaned dataset, we can observe that we have many skewed features, which is common is credit risk datasets. Before dealing with skewness, it is important to handle outliers first. Skewness measures the symmetry of a feature, and a few outliers can over-inflate the skewness of a feature, rendering the skewness value inaccurate. This can lead to us blindly applying transformations to a feature, when it does harm to our PD Model.\n",
    "\n",
    "As such, based on the nature of an outlier, they will be dealt in different ways.\n",
    "\n",
    "- **Clear Data Error**: Trim\n",
    "- **Real Outlier but Rare (Domain Knowledge Based)**: Winsorise till IQR Bounds\n",
    "- **Outlier is Real & Meaningful** : Let WoE binning handle this outlier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4538434",
   "metadata": {},
   "source": [
    "WoE binning is robust against outliers. There will be no skewness treatment for our Logistic Regression Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76a8bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_outlier_pct(df, col_name, lower_pct=0.25, upper_pct=0.75):\n",
    "    \"\"\"Computes pct of outliers per column based on IQR method\"\"\"\n",
    "\n",
    "    # 1. Compute percentile bounds\n",
    "    quantiles = df.approxQuantile(col_name, [lower_pct, upper_pct], 0.01)\n",
    "    q1, q3 = quantiles[0], quantiles[1]\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # 2. Obtain lower and upper bound, any data points outside of this are seen as outliers\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    total_rows = df.count()\n",
    "\n",
    "    return round(\n",
    "        df.filter((col(col_name) < lower_bound) | (col(col_name) > upper_bound)).count()\n",
    "        / total_rows\n",
    "        * 100,\n",
    "        2,\n",
    "    )\n",
    "\n",
    "\n",
    "def display_distributions(df):\n",
    "    \"\"\"Takes in Spark Dataframe. Samples it and display distribution for skewness checking\"\"\"\n",
    "    # 1. Select numerical columns\n",
    "    numeric_cols = [\n",
    "        field.name for field in df.schema if isinstance(field.dataType, NumericType)\n",
    "    ]\n",
    "\n",
    "    # 2. Sample small portion of data (e.g., 5%) and convert to pandas\n",
    "    sample_df = df.select(numeric_cols).sample(fraction=0.1, seed=42)\n",
    "    sample_pdf = sample_df.toPandas()\n",
    "\n",
    "    # 3. Plot histograms as subplots\n",
    "    n_cols = 3  # Number of plots per row\n",
    "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col_name in enumerate(numeric_cols):\n",
    "        axes[i].hist(sample_pdf[col_name].dropna(), bins=50, color=\"skyblue\")\n",
    "        axes[i].set_title(col_name, fontsize=10)\n",
    "        axes[i].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def inspect_outliers_iqr(df: DataFrame, columns: list, sample_size: int = 5):\n",
    "    for col_name in columns:\n",
    "        try:\n",
    "            print(f\"\\nðŸ“Š Inspecting Outliers for Column: `{col_name}`\")\n",
    "\n",
    "            # Step 1: Calculate Q1, Q3, and IQR\n",
    "            q1, q3 = df.approxQuantile(col_name, [0.25, 0.75], 0.01)\n",
    "            iqr = q3 - q1\n",
    "            lower = q1 - 1.5 * iqr\n",
    "            upper = q3 + 1.5 * iqr\n",
    "\n",
    "            print(f\"Q1 = {q1}, Q3 = {q3}, IQR = {iqr}\")\n",
    "            print(f\"Lower Bound = {lower}, Upper Bound = {upper}\")\n",
    "\n",
    "            # Step 2: Count outliers\n",
    "            outlier_count = df.filter(\n",
    "                (col(col_name) < lower) | (col(col_name) > upper)\n",
    "            ).count()\n",
    "            total_count = df.count()\n",
    "            outlier_pct = round(outlier_count / total_count * 100, 2)\n",
    "            print(f\"Outlier Count: {outlier_count} ({outlier_pct}%)\")\n",
    "\n",
    "            # Step 3: Sample outlier values (top and bottom)\n",
    "            print(f\"ðŸ”¼ Top Outliers (>{upper}):\")\n",
    "            df.filter(col(col_name) > upper).select(col_name).orderBy(\n",
    "                col(col_name).desc()\n",
    "            ).show(sample_size)\n",
    "\n",
    "            print(f\"ðŸ”½ Bottom Outliers (<{lower}):\")\n",
    "            df.filter(col(col_name) < lower).select(col_name).orderBy(\n",
    "                col(col_name)\n",
    "            ).show(sample_size)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Could not process column `{col_name}`: {str(e)}\")\n",
    "\n",
    "\n",
    "def winsorise_col(df, col_name, operator: str, condition_val, final_val):\n",
    "    \"\"\"\n",
    "    Winsorises a column by replacing values above a certain condition with a final value.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        col_name (str): The name of the column to winsorise.\n",
    "        condition_val (float): The value above which to replace with final_val (cut-off)\n",
    "        final_val (float): The value to replace with.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with the winsorised column.\n",
    "    \"\"\"\n",
    "    print(\"âœ… Winsorising column:\", col_name, \"...\")\n",
    "\n",
    "    if operator == \"<\":\n",
    "        return df.withColumn(\n",
    "            col_name,\n",
    "            when(col(col_name) < condition_val, final_val).otherwise(col(col_name)),\n",
    "        )\n",
    "\n",
    "    elif operator == \">\":\n",
    "        return df.withColumn(\n",
    "            col_name,\n",
    "            when(col(col_name) > condition_val, final_val).otherwise(col(col_name)),\n",
    "        )\n",
    "\n",
    "\n",
    "def retain_rows(\n",
    "    df: DataFrame, col_name: str, condition_val: float, operator: str\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Retains rows in the DataFrame where the specified column meets a condition.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with the specified rows dropped.\n",
    "    \"\"\"\n",
    "\n",
    "    if operator == \"<=\":\n",
    "        return df.filter(col(col_name) <= condition_val)\n",
    "\n",
    "    elif operator == \"<\":\n",
    "        return df.filter(col(col_name) < condition_val)\n",
    "\n",
    "    elif operator == \">\":\n",
    "        return df.filter(col(col_name) > condition_val)\n",
    "\n",
    "    elif operator == \">=\":\n",
    "        return df.filter(col(col_name) >= condition_val)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Operator must be '>=' or '<='\")\n",
    "\n",
    "\n",
    "# == PD Outlier Treatment ==\n",
    "\n",
    "# == 1. Find outliers % per column, without sampling (may risk missing absurd placeholders) ==\n",
    "\n",
    "numeric_df5 = df.select(\n",
    "    [feature.name for feature in df.schema if isinstance(feature.dataType, NumericType)]\n",
    ")\n",
    "\n",
    "outliers_dict = {}\n",
    "\n",
    "for feature in numeric_df5.schema.fields:\n",
    "    col_name = feature.name\n",
    "    data_type = feature.dataType\n",
    "\n",
    "    if isinstance(data_type, NumericType):\n",
    "        outlier_pct = compute_outlier_pct(numeric_df5, col_name)\n",
    "        if outlier_pct > 0:\n",
    "            outliers_dict[col_name] = outlier_pct\n",
    "\n",
    "\n",
    "print(\"âŒ Outlier Percentage by Feature (sorted):\")\n",
    "for k, v in sorted(outliers_dict.items(), key=lambda item: item[1], reverse=True):\n",
    "    print(f\"{k}: {round(v, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace0db72",
   "metadata": {},
   "source": [
    "From the original distribution of our cleaned dataset, we can observe that we have many skewed features, which is common is credit risk datasets. Before dealing with skewness, it is important to handle outliers first. Skewness measures the symmetry of a feature, and a few outliers can over-inflate the skewness of a feature, rendering the skewness value inaccurate. This can lead to us blindly applying transformations to a feature, when it does harm to our PD Model.\n",
    "\n",
    "As such, based on the nature of an outlier, they will be dealt in different ways.\n",
    "\n",
    "- **Clear Data Error**: Trim\n",
    "- **Real Outlier but Rare (Domain Knowledge Based)**: Winsorise till IQR Bounds\n",
    "- **Outlier is Real & Meaningful** : Let WoE binning handle this outlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8fa85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == 1. Get Numeric Columns from Spark Dataframe ==\n",
    "numerical_cols1 = get_numerical_cols(df1, TARGET_COL)\n",
    "\n",
    "# == 2. Obtain Sampled Pandas Train, Test Dataframe from Spark Dataframe, orders them ==\n",
    "train_pdf1, test_pdf1 = sample_split_order(\n",
    "    initial_df=df1, sample_frac=0.1, cut_off_date=CUT_DATE, date_col=\"issue_d\"\n",
    ")\n",
    "\n",
    "# == 3. Observe IV of Numerical Variables ==\n",
    "for feature in numerical_cols1:\n",
    "    if feature == \"id\":\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nðŸ” Feature: {feature}\")\n",
    "\n",
    "    if train_pdf1[feature].nunique() < 2:\n",
    "        print(\"âš ï¸ Not enough variation. Skipping.\")\n",
    "        iv_categories[\"no_variation\"].append(feature)\n",
    "        continue\n",
    "\n",
    "    x, y = train_pdf1[feature], train_pdf1[TARGET_COL]\n",
    "\n",
    "    bin_and_classify_feature(feature=feature, x=x, y=y, monotonic_trend_type=\"auto\")\n",
    "\n",
    "# == Include grade_numeric as well (ordinal feature) ==\n",
    "train_pdf1 = tx_grade(train_pdf1)\n",
    "\n",
    "bin_and_classify_feature(\n",
    "    \"grade_numeric\",\n",
    "    train_pdf1[\"grade_numeric\"],\n",
    "    train_pdf1[TARGET_COL],\n",
    "    dtype=\"numerical\",\n",
    "    monotonic_trend_type=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85997caf",
   "metadata": {},
   "source": [
    "### 2.2 Categorical Features IV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb76b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Observe IV of Categorical Variables ==\n",
    "categorical_features = get_categorical_cols(df1, TARGET_COL)\n",
    "\n",
    "for feat in categorical_features:\n",
    "    if feat != \"grade\":\n",
    "        bin_and_classify_feature(\n",
    "            feat,\n",
    "            train_pdf1[feat],\n",
    "            train_pdf1[TARGET_COL],\n",
    "            dtype=\"categorical\",\n",
    "            monotonic_trend_type=\"auto\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cddc02",
   "metadata": {},
   "source": [
    "### 2.3 Feature Importance Rankings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2b9bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Compiled Feature Rankings (Categorical & Numerical) ==\n",
    "plot_iv_by_category(iv_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9dd7d0",
   "metadata": {},
   "source": [
    "### 2.4 Feature Filtering\n",
    "\n",
    "Now that we have identified the more relevant/important features for PD Modeling, lets drop the features and observe feature importance ranking & WoE trends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1be875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Define columns to abandon & keep (Based on train_pdf1) ==\n",
    "no_variation_columns = [\n",
    "    item\n",
    "    for category, feature_list in iv_categories.items()\n",
    "    if category == \"no_variation\"\n",
    "    for item in feature_list\n",
    "]\n",
    "\n",
    "useless_cols = [\n",
    "    feature\n",
    "    for category, feature_list in iv_categories.items()\n",
    "    if category == \"useless (< 0.02)\"\n",
    "    for feature, _ in feature_list\n",
    "]\n",
    "\n",
    "throw_cols = no_variation_columns + useless_cols\n",
    "keep_cols = [c for c in train_pdf1.columns if c not in throw_cols]\n",
    "\n",
    "train_pdf1 = train_pdf1.drop(columns=throw_cols)\n",
    "print(\"âœ… Dropped columns with IV < 0.02 and columns with no variation in TRAIN_DF1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Leftover Columns IV in TRAIN_DF1 ==\n",
    "updated_iv_categories = get_updated_iv_categories(iv_categories, train_pdf1)\n",
    "\n",
    "plot_iv_by_category(updated_iv_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d28089",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pdf1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca7bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Observe WoE & IV of train_pdf1 ==\n",
    "for feature in sorted(train_pdf1.columns):\n",
    "    excluded_columns = [\"id\", \"issue_d\", \"default_status\"]\n",
    "    x, y = train_pdf1[feature], train_pdf1[TARGET_COL]\n",
    "\n",
    "    if feature in excluded_columns:\n",
    "        continue\n",
    "\n",
    "    if train_pdf1[feature].dtype == \"object\":\n",
    "        bin_and_classify_feature(\n",
    "            feature=feature,\n",
    "            x=x,\n",
    "            y=y,\n",
    "            monotonic_trend_type=\"auto\",\n",
    "            dtype=\"categorical\",\n",
    "            max_n_bins=5,\n",
    "        )\n",
    "    else:\n",
    "        bin_and_classify_feature(\n",
    "            feature=feature, x=x, y=y, monotonic_trend_type=\"auto\", max_n_bins=5\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0ffab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Inspect Non Monotonic Features to handle later on ==\n",
    "\n",
    "non_monotonic_features = [\"tot_cur_bal\", \"tot_hi_cred_lim\", \"fico_score/inq_last_6mths\"]\n",
    "\n",
    "for feature in non_monotonic_features:\n",
    "    bin_and_classify_feature(feature, x=train_pdf1[feature], y=train_pdf1[TARGET_COL])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ad527",
   "metadata": {},
   "source": [
    "For these non-monotonic features, we shall attempt to enforce a strict monotonic trend constraint on WoE. If IV drops severely, the feature shall be dropped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb5f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Inspect forced WoE monotonicity constraint ==\n",
    "for non_mono in [\"tot_cur_bal\", \"tot_hi_cred_lim\", \"fico_score/inq_last_6mths\"]:\n",
    "    bin_and_classify_feature(\n",
    "        non_mono,\n",
    "        train_pdf1[non_mono],\n",
    "        train_pdf1[TARGET_COL],\n",
    "        monotonic_trend_type=\"auto_asc_desc\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78833212",
   "metadata": {},
   "source": [
    "As seen, enforcing monotonicity constraint caused `fico_score/inq_last_6mths` resulted in a drop of 0.04 in IV. This variable should be dropped, since monotonicity constraint is important for Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb92975",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pdf1 = train_pdf1.drop(\"fico_score/inq_last_6mths\", axis=1)\n",
    "train_pdf1 = train_pdf1.drop(\"fico_score/inq_last_6mths_invalid\", axis=1)\n",
    "\n",
    "assert (\"fico_score/inq_last_6mths\" not in train_pdf1.columns) and (\n",
    "    \"fico_score/inq_last_6mths_invalid\" not in train_pdf1.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1049b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_iv_categories = get_updated_iv_categories(updated_iv_categories, train_pdf1)\n",
    "plot_iv_by_category(updated_iv_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a6f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Before WoE Binning Transformation ==\n",
    "train_pdf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a6f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Ensure Same Columns between train_df and test_df (Drop `grade`) ==\n",
    "\n",
    "# == 1. Add grade_numeric, drop grade column\n",
    "test_pdf1 = tx_grade(test_pdf1)\n",
    "\n",
    "# = 2. Select only columns train_pdf1 has ==\n",
    "test_pdf1 = test_pdf1[train_pdf1.columns]\n",
    "test_pdf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e6ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Check column discrepancies ==\n",
    "set(test_pdf1.columns) - set(train_pdf1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a90d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == 1. WoE binning Transformation on train_df ==\n",
    "non_monotonic_cols = [\"tot_cur_bal\", \"tot_hi_cred_lim\"]\n",
    "\n",
    "# == 2. Transform Train Dataset ==\n",
    "train_pdf1, optb_dict = woe_bin_transform_train(\n",
    "    train_pdf1, non_monotonic_features, TARGET_COL\n",
    ")\n",
    "\n",
    "train_pdf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163a5dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Transform Test Dataset ==\n",
    "test_pdf1 = apply_woe_transform(test_pdf1, optb_dict)\n",
    "test_pdf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692d236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Evaluate Model Performance upon WoE Transformation ==\n",
    "run_model_checkpoint(\n",
    "    train_pdf1,\n",
    "    test_pdf1,\n",
    "    run_name=\"log_reg_woe_transformation\",\n",
    "    model_type=\"Logistic Regression\",\n",
    "    run_group=NOTEBOOK_RUN_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d63a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pdf2 = train_pdf1\n",
    "test_pdf2 = test_pdf1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b461774",
   "metadata": {},
   "source": [
    "# 3. A/B Testing (Interaction Feature Selection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37309228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# = Find base vs interaction features ==\n",
    "excluded_columns = [\"id\", \"default_status\", \"issue_d\"]\n",
    "base_feats = sorted(\n",
    "    [\n",
    "        col\n",
    "        for col in train_pdf2.columns\n",
    "        if \"_x_\" not in col\n",
    "        and \"/\" not in col\n",
    "        and col != \"avail_ratio_woe\"\n",
    "        and col not in excluded_columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "interaction_features = sorted(\n",
    "    [\n",
    "        feature\n",
    "        for feature in train_pdf2.columns\n",
    "        if feature not in base_feats and feature not in excluded_columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "interaction_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "METRIC = \"gini\"  # \"auc\" or \"gini\"\n",
    "THRESH_GINI = 0.0015  # same but for Gini\n",
    "\n",
    "\n",
    "# == WoE transformed variables are already scaled in log odds ==\n",
    "def score(train_df, test_df, features):\n",
    "    X_tr = train_df[features]\n",
    "    y_tr = train_df[TARGET_COL]\n",
    "    X_te = test_df[features]\n",
    "    y_te = test_df[TARGET_COL]\n",
    "    model = LogisticRegression(solver=\"lbfgs\", C=1.0, max_iter=1000, penalty=\"l2\")\n",
    "    model.fit(X_tr, y_tr)\n",
    "    auc = roc_auc_score(y_te, model.predict_proba(X_te)[:, 1])\n",
    "    return 2 * auc - 1\n",
    "\n",
    "\n",
    "def pick_interactions(train_df, test_df, base_features, interaction_features):\n",
    "    base_score = score(train_df, test_df, base_features)\n",
    "    print(f\"Base {METRIC.upper()}: {base_score:.4f}\")\n",
    "    keep = []\n",
    "    for f in interaction_features:\n",
    "        s = score(train_df, test_df, base_features + [f])\n",
    "        delta = s - base_score\n",
    "        need = THRESH_GINI\n",
    "        print(f\"{f:40s} -> {METRIC.upper()}: {s:.4f}  Î”={delta:+.4f}\")\n",
    "        if delta >= need:\n",
    "            keep.append(f)\n",
    "\n",
    "    return base_features + keep\n",
    "\n",
    "\n",
    "# ==== example usage ====\n",
    "\n",
    "\n",
    "final_feats = pick_interactions(train_pdf2, test_pdf2, base_feats, interaction_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1154ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(final_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7940dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pdf2 = train_pdf2[final_feats + excluded_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b093030",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model_checkpoint(\n",
    "    train_pdf2, test_pdf2, \"log_reg_ab_test\", \"Logistic Regression\", NOTEBOOK_RUN_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01ecd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Obtain IV scores of each feature currently in train_pdf2 ==\n",
    "updated_iv_categories_woe = {\n",
    "    bucket: [\n",
    "        (f\"{feat}_woe\", iv) for feat, iv in feats if f\"{feat}_woe\" in train_pdf2.columns\n",
    "    ]\n",
    "    for bucket, feats in updated_iv_categories.items()\n",
    "}\n",
    "\n",
    "plot_iv_by_category(updated_iv_categories_woe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
