{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "900a3b1b-84a0-4af6-b03b-dc3cf74f7572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Medallion Architecture Data Cleaning Pipeline\n",
    "\n",
    "Delta Live Tables offer a fault-tolerant, optimized approach for building reliable data pipelines, making them ideal for this use case.\n",
    "\n",
    "In the real world, roles & responsibilities of E2E data projects are as shown:\n",
    "\n",
    "- **Data Engineers**: Focus on building pipelines that handle common data issues such as duplicates, formatting of columns, schema definition, and invalid values.\n",
    "\n",
    "- **Data Scientists**: Work on EDA, imputing missing values, handling outliers, and preparing data for modeling (feature engineering / selection / dimensionality reduction etc).\n",
    "\n",
    "In this notebook, I will be implementing a simplified **Medallion Architecture** using **Delta Live Tables (DLT)\n",
    "in Azure Databricks** to simulate real-world data engineering practices.\n",
    "\n",
    "I will be using the following visualisation as a guide to build the data pipeline.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://media.datacamp.com/cms/ad_4nxe4oejrhu9gexxri3ea6vmsu1fgxcxbvlwmbaj4ji5s2u31dg3hbyyg4sxmd7ma8-9zamnbxadzz_h4kllvjylicug3v4-iinvx65erdijn4htymmqvc3mjqblskqzdu5ttmodyua.png\">\n",
    "\n",
    "By the end of this notebook, I should be able to:\n",
    "\n",
    "- Output a **thoroughly cleansed target dataset** ready for data scientists' to conduct EDA, dataset preprocessing and other model building practices.\n",
    "\n",
    "- Define **feature and target variables** from the target table clearly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6befe41e-23c0-430c-8ee8-fa478acdc63a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 0. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16d7e7af-a7c5-4d6e-a563-c17376d47c4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/04 10:45:15 WARN Utils: Your hostname, Chengs-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.77 instead (on interface en0)\n",
      "25/09/04 10:45:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Users/lunlun/Downloads/Github/Credit-Risk-Modeling-PySpark/venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /Users/lunlun/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /Users/lunlun/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4f937c24-62d9-4620-94df-b72e1a67bdc0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 75ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4f937c24-62d9-4620-94df-b72e1a67bdc0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/2ms)\n",
      "25/09/04 10:45:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0.0\n"
     ]
    }
   ],
   "source": [
    "# == Import Libraries ==\n",
    "\n",
    "from init_spark import start_spark\n",
    "\n",
    "spark = start_spark()\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    when,\n",
    "    count,\n",
    "    desc,\n",
    "    isnan,\n",
    "    isnull,\n",
    "    lit,\n",
    "    length,\n",
    "    trim,\n",
    "    lower,\n",
    "    upper,\n",
    "    to_date,\n",
    "    concat_ws,\n",
    "    regexp_extract,\n",
    "    expr,\n",
    ")\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    DateType,\n",
    "    NumericType,\n",
    ")\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Helper Functions (String Cleaning) ==\n",
    "\n",
    "\n",
    "def drop_duplicates(df):\n",
    "    duplicate_rows = df.count() - df.dropDuplicates().count()\n",
    "\n",
    "    return df.dropDuplicates()\n",
    "\n",
    "\n",
    "def handle_string_cols_spaces(df):\n",
    "    string_cols = [\n",
    "        field.name\n",
    "        for field in df.schema.fields\n",
    "        if isinstance(field.dataType, StringType)\n",
    "    ]\n",
    "\n",
    "    # Replaces each existing column with new <string> values which are trimmed\n",
    "    for col_name in string_cols:\n",
    "        df = df.withColumn(col_name, trim(col(col_name)))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_string_cols_formatting(df):\n",
    "    \"\"\"\n",
    "    Uses library of RapidFuzz to provide lightweight similarity calculations, optimised for performance\n",
    "\n",
    "    Takes reference from String issues are in ../sandbox/string_issues.ipynb\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Original Number of Rows: {df.count()}. \")\n",
    "\n",
    "    # # 1. Drops unusable String columns\n",
    "    # unusable_cols = [\"emp_title\",\n",
    "    # \"hardship_type\",\n",
    "    # \"verification_status_joint\",\n",
    "    # \"hardship_status\",\n",
    "    # \"deferral_term\",\n",
    "    # \"hardship_length\",\n",
    "    # \"hardship_loan_status\",\n",
    "    # \"settlement_status\", \"annual_inc_joint\", 'dti_joint']\n",
    "    # df = df.drop(*unusable_cols) # allows dropping of multiple columns\n",
    "\n",
    "    # 2. Fix addr_state (check len() > 2)\n",
    "    df = df.filter(length(col(\"addr_state\")) == 2)\n",
    "\n",
    "    # 3. Fix invalid string column values\n",
    "\n",
    "    # invalid_entries_list = [ \"application_type\",\n",
    "    #     \"policy_code\",\n",
    "    #     \"home_ownership\",\n",
    "    #     \"verification_status\",\n",
    "    #     \"loan_status\",\n",
    "    #     \"pymnt_plan\",\n",
    "    #     \"initial_list_status\",\n",
    "    #     \"hardship_flag\"]\n",
    "\n",
    "    df = df.filter(\n",
    "        col(\"application_type\").isin(\"Individual\", \"Joint App\")\n",
    "        & col(\"policy_code\").isin(\"1.0\", \"0.0\")  # stored as string initially\n",
    "        & col(\"home_ownership\").isin(\"MORTGAGE\", \"RENT\", \"OWN\", \"ANY\", \"OTHER\")\n",
    "        & col(\"verification_status\").isin(\"Source Verified\", \"Not Verified\", \"Verified\")\n",
    "        & col(\"loan_status\").isin(\"Fully Paid\", \"Charged Off\", \"Default\")\n",
    "        & col(\"pymnt_plan\").isin(\"n\", \"y\")\n",
    "        & col(\"initial_list_status\").isin(\"f\", \"w\")\n",
    "        & col(\"hardship_flag\").isin(\"N\", \"Y\")\n",
    "    )\n",
    "    df = df.withColumnRenamed(\"loan_status\", \"default_status\")\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"default_status\",\n",
    "        when(col(\"default_status\") == \"Fully Paid\", 0)\n",
    "        .when(col(\"default_status\") == \"Charged Off\", 1)\n",
    "        .when(col(\"default_status\") == \"Default\", 1),\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"home_ownership\",\n",
    "        when(col(\"home_ownership\") == \"ANY\", \"OTHER\").otherwise(col(\"home_ownership\")),\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"policy_code\",\n",
    "        when(col(\"policy_code\") == \"1.0\", 1).when(col(\"policy_code\") == \"0.0\", 0),\n",
    "    )\n",
    "\n",
    "    # 4. Drop meaningless string columns\n",
    "    meaningless_columns = [\"url\", \"desc\", \"title\", \"zip_code\", \"purpose\"]\n",
    "    df = df.drop(*meaningless_columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def cast_string_to_numeric_cols(df):\n",
    "    numeric_columns = [\n",
    "        \"id\",\n",
    "        \"member_id\",\n",
    "        \"annual_inc\",\n",
    "        \"annual_inc_joint\",\n",
    "        \"dti\",\n",
    "        \"dti_joint\",\n",
    "        \"delinq_2yrs\",\n",
    "        \"fico_range_low\",\n",
    "        \"fico_range_high\",\n",
    "        \"inq_last_6mths\",\n",
    "        \"mths_since_last_delinq\",\n",
    "        \"mths_since_last_record\",\n",
    "        \"open_acc\",\n",
    "        \"pub_rec\",\n",
    "        \"revol_bal\",\n",
    "        \"revol_util\",\n",
    "        \"total_acc\",\n",
    "        \"out_prncp\",\n",
    "        \"out_prncp_inv\",\n",
    "        \"total_pymnt\",\n",
    "        \"total_pymnt_inv\",\n",
    "        \"total_rec_prncp\",\n",
    "        \"total_rec_int\",\n",
    "        \"total_rec_late_fee\",\n",
    "        \"recoveries\",\n",
    "        \"collection_recovery_fee\",\n",
    "        \"last_fico_range_high\",\n",
    "        \"last_fico_range_low\",\n",
    "        \"collections_12_mths_ex_med\",\n",
    "        \"mths_since_last_major_derog\",\n",
    "        \"acc_now_delinq\",\n",
    "        \"tot_coll_amt\",\n",
    "        \"loan_amnt\",\n",
    "        \"funded_amnt\",\n",
    "        \"funded_amnt_inv\",\n",
    "        \"installment\",\n",
    "        \"tot_cur_bal\",\n",
    "        \"total_rev_hi_lim\",\n",
    "        \"inq_fi\",\n",
    "        \"hardship_amount\",\n",
    "        \"hardship_dpd\",\n",
    "        \"orig_projected_additional_accrued_interest\",\n",
    "        \"hardship_payoff_balance_amount\",\n",
    "        \"hardship_last_payment_amount\",\n",
    "        \"settlement_amount\",\n",
    "        \"settlement_percentage\",\n",
    "        \"settlement_term\",\n",
    "        \"avg_cur_bal\",\n",
    "        \"total_bal_il\",\n",
    "        \"bc_util\",\n",
    "        \"il_util\",\n",
    "        \"total_cu_tl\",\n",
    "        \"max_bal_bc\",\n",
    "        \"percent_bc_gt_75\",\n",
    "        \"total_bal_ex_mort\",\n",
    "        \"all_util\",\n",
    "        \"open_acc_6m\",\n",
    "        \"open_act_il\",\n",
    "        \"open_il_12m\",\n",
    "        \"last_pymnt_amnt\",\n",
    "        \"open_il_24m\",\n",
    "        \"mths_since_rcnt_il\",\n",
    "        \"open_rv_12m\",\n",
    "        \"open_rv_24m\",\n",
    "        \"emp_length\",\n",
    "        \"term\",\n",
    "    ]\n",
    "\n",
    "    # Deal with Type Casting to Numeric Data\n",
    "    int_cols = [\n",
    "        \"id\",\n",
    "        \"member_id\",\n",
    "        \"delinq_2yrs\",\n",
    "        \"fico_range_low\",\n",
    "        \"fico_range_high\",\n",
    "        \"inq_last_6mths\",\n",
    "        \"mths_since_last_delinq\",\n",
    "        \"mths_since_last_record\",\n",
    "        \"open_acc\",\n",
    "        \"pub_rec\",\n",
    "        \"total_acc\",\n",
    "        \"last_fico_range_high\",\n",
    "        \"last_fico_range_low\",\n",
    "        \"collections_12_mths_ex_med\",\n",
    "        \"mths_since_last_major_derog\",\n",
    "        \"acc_now_delinq\",\n",
    "        \"inq_fi\",\n",
    "        \"hardship_dpd\",\n",
    "        \"settlement_term\",\n",
    "        \"total_cu_tl\",\n",
    "        \"open_acc_6m\",\n",
    "        \"open_act_il\",\n",
    "        \"open_il_12m\",\n",
    "        \"open_il_24m\",\n",
    "        \"open_rv_12m\",\n",
    "        \"open_rv_24m\",\n",
    "        \"emp_length\",\n",
    "        \"mths_since_rcnt_il\",\n",
    "    ]\n",
    "\n",
    "    for column in numeric_columns:\n",
    "        if column == \"emp_length\":\n",
    "            # == Converts emp_length to integer value ==\n",
    "            df = df.withColumn(\n",
    "                \"emp_length\",\n",
    "                when(col(\"emp_length\").rlike(\"10\\\\+\"), 10)\n",
    "                .when(col(\"emp_length\").rlike(\"< 1\"), 0)\n",
    "                .otherwise(\n",
    "                    regexp_extract(col(\"emp_length\"), r\"(\\d+)\", 1).cast(\n",
    "                        \"int\"\n",
    "                    )  # extracts 1st regex group digit from string\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        elif column in int_cols:\n",
    "            df = df.withColumn(\n",
    "                column, col(column).cast(DoubleType()).cast(IntegerType())\n",
    "            )\n",
    "\n",
    "        elif column == \"term\":\n",
    "            df = df.withColumn(\n",
    "                \"term\",\n",
    "                regexp_extract(col(\"term\"), r\"(\\d+)\", 1)\n",
    "                .cast(DoubleType())\n",
    "                .cast(IntegerType()),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            df = df.withColumn(column, col(column).cast(DoubleType()))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def cast_string_to_date_cols(df):\n",
    "    date_columns = [\n",
    "        \"issue_d\",\n",
    "        \"earliest_cr_line\",\n",
    "        \"last_pymnt_d\",\n",
    "        \"last_credit_pull_d\",\n",
    "        \"next_pymnt_d\",\n",
    "        \"sec_app_earliest_cr_line\",\n",
    "        \"hardship_start_date\",\n",
    "        \"hardship_end_date\",\n",
    "        \"payment_plan_start_date\",\n",
    "        \"debt_settlement_flag_date\",\n",
    "        \"settlement_date\",\n",
    "    ]\n",
    "\n",
    "    # Clean and cast each column\n",
    "    for date_col in date_columns:\n",
    "        # Only parse values that match the pattern like \"Jan-2015\"\n",
    "        df = df.withColumn(\n",
    "            date_col,\n",
    "            when(\n",
    "                col(date_col).rlike(\"^[A-Za-z]{3}-\\\\d{4}$\"),  # only parse valid ones\n",
    "                to_date(concat_ws(\"-\", col(date_col), lit(\"01\")), \"MMM-yyyy-dd\"),\n",
    "            ).otherwise(None),\n",
    "        )  # fallback for malformed ones\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# == Helper Functions (Numerical Cleaning)\n",
    "def clear_invalid_numerical_entries(df):\n",
    "    df = df.filter(~(col(\"dti\") < 0))\n",
    "    df = df.filter(~(col(\"total_rec_late_fee\") < 0))\n",
    "    df = df.filter(col(\"loan_amnt\") > 0)\n",
    "    df = df.filter(col(\"funded_amnt\") > 0)\n",
    "    df = df.filter(\n",
    "        (col(\"int_rate\") >= 0) & (col(\"int_rate\") <= 100)\n",
    "    )  # Assuming interest rate is between 0% and 100%\n",
    "    df = df.filter(col(\"installment\") >= 0)\n",
    "    df = df.filter(col(\"annual_inc\") >= 0)\n",
    "    df = df.filter(\n",
    "        (col(\"revol_util\") >= 0) & (col(\"revol_util\") <= 100)\n",
    "    )  # Else, user maxed out credit accounts' limits\n",
    "    df = df.filter(col(\"total_rec_late_fee\") >= 0)\n",
    "    df = df.filter(col(\"recoveries\") >= 0)\n",
    "    df = df.filter(col(\"collection_recovery_fee\") >= 0)\n",
    "    df = df.filter(col(\"total_rec_prncp\") >= 0)\n",
    "    df = df.filter(col(\"total_rec_int\") >= 0)\n",
    "    df = df.filter(col(\"total_pymnt\") >= 0)\n",
    "    df = df.filter(col(\"out_prncp\") >= 0)\n",
    "\n",
    "    # Fico Range Invalid Entries Removal\n",
    "    df = df.filter((col(\"fico_range_low\") >= 300) & (col(\"fico_range_low\") <= 850))\n",
    "    df = df.filter((col(\"fico_range_high\") >= 300) & (col(\"fico_range_high\") <= 850))\n",
    "    df = df.filter(col(\"fico_range_low\") <= col(\"fico_range_high\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_constant_columns(df):\n",
    "    \"\"\"\n",
    "    Removes all columns in the DataFrame that have only one distinct value.\n",
    "    Returns a new DataFrame with those columns removed.\n",
    "    \"\"\"\n",
    "    cols_to_drop = []\n",
    "\n",
    "    for column_name in df.columns:\n",
    "        if df.select(col(column_name)).distinct().count() <= 1:\n",
    "            cols_to_drop.append(column_name)\n",
    "\n",
    "    print(f\"⚠️ Dropping constant columns: {cols_to_drop}\")\n",
    "    return df.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89517c7b-26cf-4b0f-913d-191bb16b4ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Bronze Delta Table\n",
    "\n",
    "This serves as a 'landing place' for raw data for single-source of truth purposes. In case data processing in subsequent stages go faulty, data specialists can use the **Bronze Delta Table** for reference, ensuring data integrity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a9d406-f5a0-4549-86a1-7a0dcfa14804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/04 10:45:25 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# import dlt (specific to databricks)\n",
    "\n",
    "# This will only be allowed if I can create a DLT pipeline (not allowed due to Azure for Students)\n",
    "\n",
    "# @dlt.table(name=\"bronze_raw_lendingclub_data\", comment=\"Ingest raw loan data from Lending Club csv\")\n",
    "# def bronze_raw_loans():\n",
    "#     return spark.read.csv(\"/FileStore/tables/accepted_2007_to_2018Q4.csv\",\n",
    "#                           header=True,\n",
    "#                           inferSchema=True)\n",
    "\n",
    "# I will need to ensure inferSchema = True, so that all columns dtypes are auto-detected to lessen my workload later\n",
    "\n",
    "# ✅ == The below allows DLT pipeline not to be created ==\n",
    "\n",
    "bronze_df = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"../data/accepted_2007_to_2018Q4.csv\")\n",
    ")\n",
    "\n",
    "\n",
    "# ✅ 2. Save as a Delta table in the `bronze` schema\n",
    "bronze_df.write.format(\"delta\").mode(\"overwrite\").save(\"../data/bronze/lendingclub_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d26b8cd-5002-4eae-adf1-7387365eb7fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Silver Delta Table\n",
    "\n",
    "Next, the pipeline to produce a Silver Delta Table will mainly perform key data cleaning steps.\n",
    "\n",
    "- Deal with Duplicates\n",
    "- Remove String Column Spaces\n",
    "- Handle String Formatting / Spelling Issues\n",
    "- Ensure UTF-8 for String Columns\n",
    "- Schema Definition\n",
    "- Invalid Value Handling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ead7f5e-4328-4e0e-833c-2d1e8236f966",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.1 String Columns Cleaning\n",
    "\n",
    "These cleaning steps will take reference from **sandbox/string_issues** for specific cleaning steps to maintain data integrity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee7024d4-5f90-441d-8a5d-0811e5abf156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows: 2260701\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Duplicates removed...\n",
      "✅ Trailing / Leading Spaces removed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Number of Rows: 2260701. \n",
      "✅ Invalid String Column Formatting Settled & Meaningless Columns Dropped ...\n",
      "✅ String Columns Correctly Type Casted...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New number of rows: 1345075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# == Step 0: Read from Bronze Table ==\n",
    "bronze_df_copy = spark.read.format(\"delta\").load(\"../data/bronze/lendingclub_raw\")\n",
    "\n",
    "\n",
    "# == Step 1: Drop duplicate rows ==\n",
    "print(f\"Original number of rows: {bronze_df_copy.count()}\\n\")\n",
    "\n",
    "df_cleaned = drop_duplicates(bronze_df_copy)\n",
    "print(\"✅ Duplicates removed...\")\n",
    "\n",
    "\n",
    "# == Step 2: Trim spaces in all string columns ==\n",
    "df_cleaned = handle_string_cols_spaces(df_cleaned)\n",
    "print(\"✅ Trailing / Leading Spaces removed...\")\n",
    "\n",
    "# == Step 3: Filter & fix invalid string formatting ==\n",
    "df_cleaned = handle_string_cols_formatting(df_cleaned)\n",
    "print(\"✅ Invalid String Column Formatting Settled & Meaningless Columns Dropped ...\")\n",
    "\n",
    "# == Step 4: Type Casting ==\n",
    "df_cleaned = cast_string_to_numeric_cols(df_cleaned)\n",
    "df_cleaned = cast_string_to_date_cols(df_cleaned)\n",
    "print(\"✅ String Columns Correctly Type Casted...\\n\")\n",
    "\n",
    "print(f\"New number of rows: {df_cleaned.count()}\")\n",
    "\n",
    "# == Step 5: Save as Silver Delta Table 1 (Cleaned Strings Version) ==\n",
    "df_cleaned.write.format(\"delta\").mode(\"overwrite\").save(\n",
    "    \"../data/silver/lendingclub_cleaned_string\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>...</th>\n",
       "      <th>hardship_payoff_balance_amount</th>\n",
       "      <th>hardship_last_payment_amount</th>\n",
       "      <th>disbursement_method</th>\n",
       "      <th>debt_settlement_flag</th>\n",
       "      <th>debt_settlement_flag_date</th>\n",
       "      <th>settlement_status</th>\n",
       "      <th>settlement_date</th>\n",
       "      <th>settlement_amount</th>\n",
       "      <th>settlement_percentage</th>\n",
       "      <th>settlement_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1165552</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>6.62</td>\n",
       "      <td>368.45</td>\n",
       "      <td>A</td>\n",
       "      <td>A2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1375351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>7.62</td>\n",
       "      <td>124.65</td>\n",
       "      <td>A</td>\n",
       "      <td>A3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1342664</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8875.0</td>\n",
       "      <td>8875.0</td>\n",
       "      <td>8875.0</td>\n",
       "      <td>36</td>\n",
       "      <td>10.74</td>\n",
       "      <td>289.47</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1460588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>8.90</td>\n",
       "      <td>381.04</td>\n",
       "      <td>A</td>\n",
       "      <td>A5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1294082</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>5900.0</td>\n",
       "      <td>36</td>\n",
       "      <td>6.03</td>\n",
       "      <td>182.62</td>\n",
       "      <td>A</td>\n",
       "      <td>A1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1209893</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17600.0</td>\n",
       "      <td>17600.0</td>\n",
       "      <td>17350.0</td>\n",
       "      <td>60</td>\n",
       "      <td>12.12</td>\n",
       "      <td>392.58</td>\n",
       "      <td>B</td>\n",
       "      <td>B3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1304249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>13.67</td>\n",
       "      <td>408.22</td>\n",
       "      <td>B</td>\n",
       "      <td>B5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1136970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22650.0</td>\n",
       "      <td>22650.0</td>\n",
       "      <td>22650.0</td>\n",
       "      <td>60</td>\n",
       "      <td>16.29</td>\n",
       "      <td>554.31</td>\n",
       "      <td>C</td>\n",
       "      <td>C5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1187963</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>8.90</td>\n",
       "      <td>127.02</td>\n",
       "      <td>A</td>\n",
       "      <td>A5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1116600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>12.12</td>\n",
       "      <td>365.99</td>\n",
       "      <td>B</td>\n",
       "      <td>B3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  member_id  loan_amnt  funded_amnt  funded_amnt_inv  term  \\\n",
       "0  1165552        NaN    12000.0      12000.0          12000.0    36   \n",
       "1  1375351        NaN     4000.0       4000.0           4000.0    36   \n",
       "2  1342664        NaN     8875.0       8875.0           8875.0    36   \n",
       "3  1460588        NaN    12000.0      12000.0          12000.0    36   \n",
       "4  1294082        NaN     6000.0       6000.0           5900.0    36   \n",
       "5  1209893        NaN    17600.0      17600.0          17350.0    60   \n",
       "6  1304249        NaN    12000.0      12000.0          12000.0    36   \n",
       "7  1136970        NaN    22650.0      22650.0          22650.0    60   \n",
       "8  1187963        NaN     4000.0       4000.0           4000.0    36   \n",
       "9  1116600        NaN    11000.0      11000.0          11000.0    36   \n",
       "\n",
       "   int_rate  installment grade sub_grade  ... hardship_payoff_balance_amount  \\\n",
       "0      6.62       368.45     A        A2  ...                            NaN   \n",
       "1      7.62       124.65     A        A3  ...                            NaN   \n",
       "2     10.74       289.47     B        B2  ...                            NaN   \n",
       "3      8.90       381.04     A        A5  ...                            NaN   \n",
       "4      6.03       182.62     A        A1  ...                            NaN   \n",
       "5     12.12       392.58     B        B3  ...                            NaN   \n",
       "6     13.67       408.22     B        B5  ...                            NaN   \n",
       "7     16.29       554.31     C        C5  ...                            NaN   \n",
       "8      8.90       127.02     A        A5  ...                            NaN   \n",
       "9     12.12       365.99     B        B3  ...                            NaN   \n",
       "\n",
       "   hardship_last_payment_amount disbursement_method  debt_settlement_flag  \\\n",
       "0                           NaN                Cash                     N   \n",
       "1                           NaN                Cash                     N   \n",
       "2                           NaN                Cash                     N   \n",
       "3                           NaN                Cash                     N   \n",
       "4                           NaN                Cash                     N   \n",
       "5                           NaN                Cash                     N   \n",
       "6                           NaN                Cash                     N   \n",
       "7                           NaN                Cash                     N   \n",
       "8                           NaN                Cash                     N   \n",
       "9                           NaN                Cash                     N   \n",
       "\n",
       "  debt_settlement_flag_date settlement_status  settlement_date  \\\n",
       "0                      None              None             None   \n",
       "1                      None              None             None   \n",
       "2                      None              None             None   \n",
       "3                      None              None             None   \n",
       "4                      None              None             None   \n",
       "5                      None              None             None   \n",
       "6                      None              None             None   \n",
       "7                      None              None             None   \n",
       "8                      None              None             None   \n",
       "9                      None              None             None   \n",
       "\n",
       "  settlement_amount settlement_percentage  settlement_term  \n",
       "0               NaN                   NaN              NaN  \n",
       "1               NaN                   NaN              NaN  \n",
       "2               NaN                   NaN              NaN  \n",
       "3               NaN                   NaN              NaN  \n",
       "4               NaN                   NaN              NaN  \n",
       "5               NaN                   NaN              NaN  \n",
       "6               NaN                   NaN              NaN  \n",
       "7               NaN                   NaN              NaN  \n",
       "8               NaN                   NaN              NaN  \n",
       "9               NaN                   NaN              NaN  \n",
       "\n",
       "[10 rows x 146 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf8fc3af-0837-4bea-8562-45059b9de0e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2 Numeric Columns Cleaning\n",
    "\n",
    "These cleaning steps will take reference from **sandbox/numeric_issues** for specific cleaning steps to maintain data integrity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a27dc02-485f-40da-b988-01bb0c84afeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows: 1345075\n",
      "\n",
      "✅ Invalid Numerical Values Settled...\n",
      "Final number of rows: 1339155\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# == Remove invalid numerical values in dataset ==\n",
    "silver_table1 = spark.read.format(\"delta\").load(\n",
    "    \"../data/silver/lendingclub_cleaned_string\"\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Original number of rows: {silver_table1.count()}\\n\")\n",
    "\n",
    "silver_table2 = clear_invalid_numerical_entries(silver_table1)\n",
    "print(\"✅ Invalid Numerical Values Settled...\")\n",
    "\n",
    "print(f\"Final number of rows: {silver_table2.count()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5acd300-046c-47d3-a98b-ee19f2a0e31f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>...</th>\n",
       "      <th>hardship_payoff_balance_amount</th>\n",
       "      <th>hardship_last_payment_amount</th>\n",
       "      <th>disbursement_method</th>\n",
       "      <th>debt_settlement_flag</th>\n",
       "      <th>debt_settlement_flag_date</th>\n",
       "      <th>settlement_status</th>\n",
       "      <th>settlement_date</th>\n",
       "      <th>settlement_amount</th>\n",
       "      <th>settlement_percentage</th>\n",
       "      <th>settlement_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>670653</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>10.74</td>\n",
       "      <td>163.08</td>\n",
       "      <td>B</td>\n",
       "      <td>B4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>843470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>8500.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>7.49</td>\n",
       "      <td>264.37</td>\n",
       "      <td>A</td>\n",
       "      <td>A4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>695876</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>10.37</td>\n",
       "      <td>162.21</td>\n",
       "      <td>B</td>\n",
       "      <td>B3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>766091</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>1875.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>8.49</td>\n",
       "      <td>59.19</td>\n",
       "      <td>A</td>\n",
       "      <td>A5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>505004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>6062.370474</td>\n",
       "      <td>36</td>\n",
       "      <td>7.51</td>\n",
       "      <td>217.77</td>\n",
       "      <td>A</td>\n",
       "      <td>A4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>476497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6250.0</td>\n",
       "      <td>6250.0</td>\n",
       "      <td>6225.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>12.53</td>\n",
       "      <td>209.17</td>\n",
       "      <td>B</td>\n",
       "      <td>B5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1030372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>13.49</td>\n",
       "      <td>271.45</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>543747</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>10.75</td>\n",
       "      <td>163.11</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1377853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>36</td>\n",
       "      <td>13.99</td>\n",
       "      <td>683.46</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>371976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>209.198360</td>\n",
       "      <td>36</td>\n",
       "      <td>14.74</td>\n",
       "      <td>34.54</td>\n",
       "      <td>D</td>\n",
       "      <td>D3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  member_id  loan_amnt  funded_amnt  funded_amnt_inv  term  \\\n",
       "0   670653        NaN     5000.0       5000.0      5000.000000    36   \n",
       "1   843470        NaN     8500.0       8500.0      8500.000000    36   \n",
       "2   695876        NaN     5000.0       5000.0      5000.000000    36   \n",
       "3   766091        NaN     1875.0       1875.0      1875.000000    36   \n",
       "4   505004        NaN     7000.0       7000.0      6062.370474    36   \n",
       "5   476497        NaN     6250.0       6250.0      6225.000000    36   \n",
       "6  1030372        NaN     8000.0       8000.0      8000.000000    36   \n",
       "7   543747        NaN     5000.0       5000.0      5000.000000    36   \n",
       "8  1377853        NaN    20000.0      20000.0     20000.000000    36   \n",
       "9   371976        NaN     1000.0       1000.0       209.198360    36   \n",
       "\n",
       "   int_rate  installment grade sub_grade  ... hardship_payoff_balance_amount  \\\n",
       "0     10.74       163.08     B        B4  ...                            NaN   \n",
       "1      7.49       264.37     A        A4  ...                            NaN   \n",
       "2     10.37       162.21     B        B3  ...                            NaN   \n",
       "3      8.49        59.19     A        A5  ...                            NaN   \n",
       "4      7.51       217.77     A        A4  ...                            NaN   \n",
       "5     12.53       209.17     B        B5  ...                            NaN   \n",
       "6     13.49       271.45     C        C1  ...                            NaN   \n",
       "7     10.75       163.11     B        B2  ...                            NaN   \n",
       "8     13.99       683.46     C        C1  ...                            NaN   \n",
       "9     14.74        34.54     D        D3  ...                            NaN   \n",
       "\n",
       "   hardship_last_payment_amount disbursement_method  debt_settlement_flag  \\\n",
       "0                           NaN                Cash                     N   \n",
       "1                           NaN                Cash                     N   \n",
       "2                           NaN                Cash                     N   \n",
       "3                           NaN                Cash                     N   \n",
       "4                           NaN                Cash                     N   \n",
       "5                           NaN                Cash                     N   \n",
       "6                           NaN                Cash                     N   \n",
       "7                           NaN                Cash                     N   \n",
       "8                           NaN                Cash                     N   \n",
       "9                           NaN                Cash                     N   \n",
       "\n",
       "  debt_settlement_flag_date settlement_status  settlement_date  \\\n",
       "0                      None              None             None   \n",
       "1                      None              None             None   \n",
       "2                      None              None             None   \n",
       "3                      None              None             None   \n",
       "4                      None              None             None   \n",
       "5                      None              None             None   \n",
       "6                      None              None             None   \n",
       "7                      None              None             None   \n",
       "8                      None              None             None   \n",
       "9                      None              None             None   \n",
       "\n",
       "  settlement_amount settlement_percentage  settlement_term  \n",
       "0               NaN                   NaN              NaN  \n",
       "1               NaN                   NaN              NaN  \n",
       "2               NaN                   NaN              NaN  \n",
       "3               NaN                   NaN              NaN  \n",
       "4               NaN                   NaN              NaN  \n",
       "5               NaN                   NaN              NaN  \n",
       "6               NaN                   NaN              NaN  \n",
       "7               NaN                   NaN              NaN  \n",
       "8               NaN                   NaN              NaN  \n",
       "9               NaN                   NaN              NaN  \n",
       "\n",
       "[10 rows x 146 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silver_table2.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Date Column Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Change earliest_cr_line column to credit_history_years (Derived Column) ==\n",
    "from pyspark.sql.functions import months_between, col\n",
    "\n",
    "silver_table2 = silver_table2.withColumn(\n",
    "    \"credit_history_years\",\n",
    "    (months_between(col(\"issue_d\"), col(\"earliest_cr_line\")) / 12).cast(\"int\"),\n",
    ")\n",
    "silver_table2 = silver_table2.drop(col(\"earliest_cr_line\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Handling Missing Values\n",
    "\n",
    "Based on `../sandbox/string_issues.ipynb`, I have identified the following issues with data quality.\n",
    "\n",
    "- Some columns are unusable, due to to having a **large percentage of missing values** (taken reference from `../sandbox/string_issues`)\n",
    "\n",
    "- A number of features exhibit **missing values**, including `emp_length` etc. This requires imputation strategies to be implemented\n",
    "\n",
    "- Some columns are irrelevant to our credit risk modeling project, due to **high cardinality** (large number of unique values), e.g. `emp_title` **(categorical data)**.\n",
    "\n",
    "- There are also **redundant columns**, like `member_id`, which provides no value to our prediction of LGD, EAD and PD.\n",
    "\n",
    "- There are features like `delinq_2yrs` which have **outliers** (maximum data point way above the 75% quartile)\n",
    "\n",
    "#### 2.4.1 Find Null Value % Per Column\n",
    "\n",
    "For this credit risk modeling project, I will be dropping columns with &gt; 50% missingness. Many credit risk modelling projects on Kaggle and Github use 50%-65% missingness as the threshold to drop columns as well.\n",
    "\n",
    "However, let's display the columns which have >=50% missing values first to inspect them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "member_id: 100.00% null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mths_since_last_delinq: 50.43% null\n",
      "mths_since_last_record: 82.96% null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_pymnt_d: 100.00% null\n",
      "mths_since_last_major_derog: 73.68% null\n",
      "annual_inc_joint: 98.11% null\n",
      "dti_joint: 98.11% null\n",
      "verification_status_joint: 98.12% null\n",
      "open_acc_6m: 60.04% null\n",
      "open_act_il: 60.04% null\n",
      "open_il_12m: 60.04% null\n",
      "open_il_24m: 60.04% null\n",
      "mths_since_rcnt_il: 61.09% null\n",
      "total_bal_il: 60.04% null\n",
      "il_util: 65.43% null\n",
      "open_rv_12m: 60.04% null\n",
      "open_rv_24m: 60.04% null\n",
      "max_bal_bc: 60.04% null\n",
      "all_util: 60.04% null\n",
      "inq_fi: 60.04% null\n",
      "total_cu_tl: 60.04% null\n",
      "inq_last_12m: 60.04% null\n",
      "mths_since_recent_bc_dlq: 76.28% null\n",
      "mths_since_recent_revol_delinq: 66.54% null\n",
      "revol_bal_joint: 98.64% null\n",
      "sec_app_fico_range_low: 98.64% null\n",
      "sec_app_fico_range_high: 98.64% null\n",
      "sec_app_earliest_cr_line: 98.64% null\n",
      "sec_app_inq_last_6mths: 98.64% null\n",
      "sec_app_mort_acc: 98.64% null\n",
      "sec_app_open_acc: 98.64% null\n",
      "sec_app_revol_util: 98.66% null\n",
      "sec_app_open_act_il: 98.64% null\n",
      "sec_app_num_rev_accts: 98.64% null\n",
      "sec_app_chargeoff_within_12_mths: 98.64% null\n",
      "sec_app_collections_12_mths_ex_med: 98.64% null\n",
      "sec_app_mths_since_last_major_derog: 99.51% null\n",
      "hardship_type: 99.57% null\n",
      "hardship_reason: 99.57% null\n",
      "hardship_status: 99.57% null\n",
      "deferral_term: 99.57% null\n",
      "hardship_amount: 99.57% null\n",
      "hardship_start_date: 99.57% null\n",
      "hardship_end_date: 99.57% null\n",
      "payment_plan_start_date: 99.57% null\n",
      "hardship_length: 99.57% null\n",
      "hardship_dpd: 99.57% null\n",
      "hardship_loan_status: 99.57% null\n",
      "orig_projected_additional_accrued_interest: 99.72% null\n",
      "hardship_payoff_balance_amount: 99.57% null\n",
      "hardship_last_payment_amount: 99.57% null\n",
      "debt_settlement_flag_date: 97.53% null\n",
      "settlement_status: 97.53% null\n",
      "settlement_date: 97.53% null\n",
      "settlement_amount: 97.53% null\n",
      "settlement_percentage: 97.53% null\n",
      "settlement_term: 97.53% null\n",
      "\n",
      "✅ Columns with high pct of missing values dropped ... \n",
      "\n",
      "Updated Shape: (1339155, 89)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# == Get total number of rows ==\n",
    "total_rows = silver_table2.count()\n",
    "\n",
    "# == Calculate % of nulls per column and keep only those ≥ 50% ==\n",
    "missing_val_threshold = 30\n",
    "high_missingness_columns = []\n",
    "\n",
    "for column in silver_table2.schema.fields:\n",
    "    null_count = silver_table2.select(\n",
    "        sum(col(column.name).isNull().cast(\"int\"))\n",
    "    ).collect()[0][0]\n",
    "    null_pct = (null_count / total_rows) * 100\n",
    "    if null_pct >= missing_val_threshold:\n",
    "        print(f\"{column.name}: {null_pct:.2f}% null\")\n",
    "        high_missingness_columns.append(column.name)\n",
    "\n",
    "# == Drop columns with >= 50% missing values (Low predictive power upon inspection) ==\n",
    "silver_table2 = silver_table2.drop(*high_missingness_columns)\n",
    "print(\"\\n✅ Columns with high pct of missing values dropped ... \\n\")\n",
    "\n",
    "# == Inspect Dimensions ==\n",
    "num_rows = silver_table2.count()\n",
    "num_cols = len(silver_table2.columns)\n",
    "print(f\"Updated Shape: ({num_rows}, {num_cols})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Dropping Irrelevant / Redundant Columns\n",
    "\n",
    "This section implements the removal of **meaningless columns, features which has high cardinality (categorical data), features with little predictive value, e.g. `member_id`, `emp_title` etc, and post-loan features**. Including such features may lead to multicollinearity, and ultimately lead to low predictive power of our credit models.\n",
    "\n",
    "Reasons why I removed certain columns are as shown:\n",
    "\n",
    "- Columns with `inv`: Largely same as its subset, e.g. `total_pymnt_inv` is largely the same as `total_pymnt`\n",
    "\n",
    "- `last_pymnt_d` and `last_credit_pull_d` (according to Data Dictionary) have little predictive value even after feature engineering. It merely shows the last payment date by borrower and last date where credit report is pulled. This has little value in predicting PD, LGD or EAD.\n",
    "\n",
    "- `sub_grade` is more granular than `grade`. This may lead to a risk of overfitting of our PD, LGD, and EAD models.\n",
    "\n",
    "- High Cardinality Columns may lead to high computational costs in encoding for machine learning models, which makes it undesirable in a big data space such as credit risk.\n",
    "\n",
    "- Hardship & Settlement Features (Borrowers are only eligible for hardship and settlement programmes after loan origination for Lending Club, not when they apply for it). Borrowers will contact lenders of financial hardship, attempting to settle with lenders for interest-fee payments or lower principal sum payments. Such features should not be used to predict PD, LGD, and EAD. My models should not know if a borrower will fall into hardship for this credit risk modeling project\n",
    "\n",
    "- `disbursement_method` indicates how loan funds are delivered to the borrower. This has little relevance in predicting PD, LGD or EAD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Derived/Meaningless Features Dropped ...\n"
     ]
    }
   ],
   "source": [
    "# == Drop Derived/Meaningless Features ==\n",
    "derived_features = [\n",
    "    \"funded_amnt_inv\",\n",
    "    \"sub_grade\",\n",
    "    \"out_prncp_inv\",\n",
    "    \"total_pymnt_inv\",\n",
    "    \"last_pymnt_d\",\n",
    "    \"last_credit_pull_d\",\n",
    "]\n",
    "silver_table2 = silver_table2.drop(*derived_features)\n",
    "print(f\"✅ Derived/Meaningless Features Dropped ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['grade', 'emp_title', 'home_ownership', 'verification_status', 'pymnt_plan', 'addr_state', 'initial_list_status', 'application_type', 'hardship_flag', 'disbursement_method', 'debt_settlement_flag']\n",
      "\n",
      "emp_title has 359391 unique values → dropping ... \n",
      "\n",
      "addr_state has 51 unique values → dropping ... \n",
      "\n",
      "✅ High Cardinality Features Dropped ...\n"
     ]
    }
   ],
   "source": [
    "# == Drop High Cardinality Features ==\n",
    "\n",
    "# == 1. Define Threshold ==\n",
    "high_cardinality_threshold = 50\n",
    "\n",
    "# == 2. Find Categorical Features (to identify high cardinality columns) ==\n",
    "categorical_cols = [\n",
    "    field.name\n",
    "    for field in silver_table2.schema.fields\n",
    "    if isinstance(field.dataType, StringType)\n",
    "]\n",
    "print(categorical_cols)\n",
    "\n",
    "# == 3. Identify high-cardinality columns ==\n",
    "high_card_cols = []\n",
    "\n",
    "for col_name in categorical_cols:\n",
    "    unique_count = silver_table2.select(col_name).distinct().count()\n",
    "\n",
    "    if unique_count >= high_cardinality_threshold:\n",
    "        print(f\"\\n{col_name} has {unique_count} unique values → dropping ... \")\n",
    "        high_card_cols.append(col_name)\n",
    "\n",
    "# == 4. Drop high cardinality columns ==\n",
    "silver_table2 = silver_table2.drop(*high_card_cols)\n",
    "\n",
    "print(f\"\\n✅ High Cardinality Features Dropped ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Dropping constant columns: ['pymnt_plan', 'policy_code', 'hardship_flag']\n",
      "✅ Columns with only 1 distinct value dropped...\n"
     ]
    }
   ],
   "source": [
    "# ==  Drop columns with only 1 distinct value (No variation is useless for modeling phase)==\n",
    "silver_table2 = drop_constant_columns(silver_table2)\n",
    "print(\"✅ Columns with only 1 distinct value dropped...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hardship & miscelleneous columns dropped ...\n"
     ]
    }
   ],
   "source": [
    "# == Drop hardship related columns & miscelleanous columns (rare & irrelevant for modeling) ==\n",
    "hardship_columns = [\n",
    "    \"hardship_flag\",\n",
    "    \"disbursement_method\",\n",
    "    \"debt_settlement_flag\",\n",
    "    \"policy_code\",\n",
    "]\n",
    "\n",
    "silver_table2 = silver_table2.drop(*hardship_columns)\n",
    "print(\"✅ Hardship & miscelleneous columns dropped ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.3 Impute Missing Values (Categorical & Numerical)\n",
    "\n",
    "After removing unnecessary columns with little predictive power, we will proceed to impute missing values. We will first identify % missing values per column.\n",
    "\n",
    "For numerical columns, median values shall replace missing values, given how we haven't dealt with outliers yet. For categorical columns, mode categories shall be used to replace missing values. Such an approach is common and simplistic, though there are advanced imputation techniques like clustering. However, we shall not lose focus of learning about the credit risk modeling domain in this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emp_length: 77891 null values, 5.82% missing values.\n",
      "inq_last_6mths: 1 null values, 0.0% missing values.\n",
      "collections_12_mths_ex_med: 54 null values, 0.0% missing values.\n",
      "tot_coll_amt: 67219 null values, 5.02% missing values.\n",
      "tot_cur_bal: 67219 null values, 5.02% missing values.\n",
      "total_rev_hi_lim: 67219 null values, 5.02% missing values.\n",
      "acc_open_past_24mths: 46992 null values, 3.51% missing values.\n",
      "avg_cur_bal: 67219 null values, 5.02% missing values.\n",
      "bc_open_to_buy: 59990 null values, 4.48% missing values.\n",
      "bc_util: 60631 null values, 4.53% missing values.\n",
      "chargeoff_within_12_mths: 54 null values, 0.0% missing values.\n",
      "mo_sin_old_il_acct: 105039 null values, 7.84% missing values.\n",
      "mo_sin_old_rev_tl_op: 67219 null values, 5.02% missing values.\n",
      "mo_sin_rcnt_rev_tl_op: 67219 null values, 5.02% missing values.\n",
      "mo_sin_rcnt_tl: 67219 null values, 5.02% missing values.\n",
      "mort_acc: 46992 null values, 3.51% missing values.\n",
      "mths_since_recent_bc: 59142 null values, 4.42% missing values.\n",
      "mths_since_recent_inq: 172618 null values, 12.89% missing values.\n",
      "num_accts_ever_120_pd: 67219 null values, 5.02% missing values.\n",
      "num_actv_bc_tl: 67219 null values, 5.02% missing values.\n",
      "num_actv_rev_tl: 67219 null values, 5.02% missing values.\n",
      "num_bc_sats: 55542 null values, 4.15% missing values.\n",
      "num_bc_tl: 67219 null values, 5.02% missing values.\n",
      "num_il_tl: 67219 null values, 5.02% missing values.\n",
      "num_op_rev_tl: 67219 null values, 5.02% missing values.\n",
      "num_rev_accts: 67220 null values, 5.02% missing values.\n",
      "num_rev_tl_bal_gt_0: 67219 null values, 5.02% missing values.\n",
      "num_sats: 55542 null values, 4.15% missing values.\n",
      "num_tl_120dpd_2m: 116909 null values, 8.73% missing values.\n",
      "num_tl_30dpd: 67219 null values, 5.02% missing values.\n",
      "num_tl_90g_dpd_24m: 67219 null values, 5.02% missing values.\n",
      "num_tl_op_past_12m: 67219 null values, 5.02% missing values.\n",
      "pct_tl_nvr_dlq: 67373 null values, 5.03% missing values.\n",
      "percent_bc_gt_75: 60401 null values, 4.51% missing values.\n",
      "pub_rec_bankruptcies: 683 null values, 0.05% missing values.\n",
      "tax_liens: 37 null values, 0.0% missing values.\n",
      "tot_hi_cred_lim: 67219 null values, 5.02% missing values.\n",
      "total_bal_ex_mort: 46992 null values, 3.51% missing values.\n",
      "total_bc_limit: 46992 null values, 3.51% missing values.\n",
      "total_il_high_credit_limit: 67219 null values, 5.02% missing values.\n"
     ]
    }
   ],
   "source": [
    "total_rows = silver_table2.count()\n",
    "\n",
    "for column in silver_table2.columns:\n",
    "    null_count = silver_table2.filter(col(column).isNull()).count()\n",
    "    if null_count > 0:\n",
    "        print(\n",
    "            f\"{column}: {null_count} null values, {round(null_count/total_rows * 100,2)}% missing values.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Categorical Column Missing Values Filled!\n",
      "✅ Numerical Column Missing Values Filled!\n"
     ]
    }
   ],
   "source": [
    "# == Loop over each column ==\n",
    "for feature in silver_table2.schema.fields:\n",
    "    col_name = feature.name\n",
    "    dtype = feature.dataType\n",
    "\n",
    "    if isinstance(dtype, StringType):\n",
    "        mode_value = (\n",
    "            silver_table2.groupBy(col(f\"{col_name}\"))\n",
    "            .count()\n",
    "            .orderBy(col(\"count\").desc())\n",
    "            .first()[0]\n",
    "        )\n",
    "\n",
    "        silver_table2 = silver_table2.fillna({f\"{col_name}\": mode_value})\n",
    "\n",
    "    # == Impute Numerical Columns with Median ==\n",
    "    elif isinstance(dtype, NumericType):\n",
    "        if silver_table2.filter(col(col_name).isNull()).count() > 0:\n",
    "            median_val = silver_table2.approxQuantile(col_name, [0.5], 0.01)[0]\n",
    "            silver_table2 = silver_table2.fillna({col_name: median_val})\n",
    "\n",
    "print(\"✅ Categorical Column Missing Values Filled!\")\n",
    "print(\"✅ Numerical Column Missing Values Filled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No Missing Values Found!\n"
     ]
    }
   ],
   "source": [
    "# == Double check if there are any missing values before subsequent steps ==\n",
    "total_rows = silver_table2.count()\n",
    "\n",
    "output_arr = []\n",
    "for column in silver_table2.columns:\n",
    "    null_count = silver_table2.filter(col(column).isNull()).count()\n",
    "    if null_count > 0:\n",
    "        output_arr.append(\n",
    "            f\"{column}: {null_count} null values, {round(null_count/total_rows * 100,2)}% missing values.\"\n",
    "        )\n",
    "\n",
    "if len(output_arr) == 0:\n",
    "    print(\"✅ No Missing Values Found!\")\n",
    "else:\n",
    "    print(output_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.5 Base Feature Creation\n",
    "\n",
    "Let's obtain some base features from our dataset.\n",
    "\n",
    "First, a common derived feature in credit risk is the credit history length of the borrower at the time of loan issuance. Normally, it would be safe to say that loans with borrowers with a longer credit history has a lower probability of default. Hence, this can be taken into account as 1 of the features of the dataset. We can compute this as the difference between `issue_d` and `earliest_cr_line`.\n",
    "\n",
    "Second, it seems that we have 2 features related to FICO Scores: `fico_range_low` and `fico_range_high`. Upon further research, these features do not represent the lowest and highest scores ever recorded for a borrower, but rather the lower and upper bounds of a small range in which the borrower’s actual FICO score falls. Banks record these due to privacy concerns, through a 5 point window, as shown below. As such, it seems that we just need to average these 2 values, to minimise redundancy and multicollinearity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_table2 = silver_table2.withColumn(\n",
    "    \"fico_score\", (col(\"fico_range_low\") + col(\"fico_range_high\")) / 2\n",
    ")\n",
    "silver_table2 = silver_table2.drop(col(\"fico_range_low\"), col(\"fico_range_high\"))\n",
    "\n",
    "# == Assert checks ==\n",
    "expected_columns = set(silver_table2.columns)\n",
    "\n",
    "# == 1. Confirm 'fico_score' exists ==\n",
    "assert \"fico_score\" in expected_columns, \"'fico_score' column is missing\"\n",
    "\n",
    "# == 2. Confirm 'fico_range_low' and 'fico_range_high' are removed ==\n",
    "assert (\n",
    "    \"fico_range_low\" not in expected_columns\n",
    "    and \"fico_range_high\" not in expected_columns\n",
    "), \"Old FICO range columns still present\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>...</th>\n",
       "      <th>pct_tl_nvr_dlq</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <th>total_bal_ex_mort</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>total_il_high_credit_limit</th>\n",
       "      <th>credit_history_years</th>\n",
       "      <th>fico_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>670653</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>10.74</td>\n",
       "      <td>163.08</td>\n",
       "      <td>B</td>\n",
       "      <td>6</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>35184.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>42.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111168.0</td>\n",
       "      <td>37033.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>31463.0</td>\n",
       "      <td>5</td>\n",
       "      <td>687.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>843470</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>36</td>\n",
       "      <td>7.49</td>\n",
       "      <td>264.37</td>\n",
       "      <td>A</td>\n",
       "      <td>6</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>42.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111168.0</td>\n",
       "      <td>37033.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>31463.0</td>\n",
       "      <td>20</td>\n",
       "      <td>742.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>695876</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>10.37</td>\n",
       "      <td>162.21</td>\n",
       "      <td>B</td>\n",
       "      <td>6</td>\n",
       "      <td>RENT</td>\n",
       "      <td>17220.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>42.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111168.0</td>\n",
       "      <td>37033.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>31463.0</td>\n",
       "      <td>12</td>\n",
       "      <td>697.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>766091</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>1875.0</td>\n",
       "      <td>36</td>\n",
       "      <td>8.49</td>\n",
       "      <td>59.19</td>\n",
       "      <td>A</td>\n",
       "      <td>6</td>\n",
       "      <td>RENT</td>\n",
       "      <td>15204.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>42.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111168.0</td>\n",
       "      <td>37033.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>31463.0</td>\n",
       "      <td>11</td>\n",
       "      <td>722.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>505004</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>7.51</td>\n",
       "      <td>217.77</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>RENT</td>\n",
       "      <td>38400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>42.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111168.0</td>\n",
       "      <td>37033.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>31463.0</td>\n",
       "      <td>10</td>\n",
       "      <td>737.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>476497</td>\n",
       "      <td>6250.0</td>\n",
       "      <td>6250.0</td>\n",
       "      <td>36</td>\n",
       "      <td>12.53</td>\n",
       "      <td>209.17</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>42.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111168.0</td>\n",
       "      <td>37033.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>31463.0</td>\n",
       "      <td>5</td>\n",
       "      <td>697.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1030372</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>13.49</td>\n",
       "      <td>271.45</td>\n",
       "      <td>C</td>\n",
       "      <td>10</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>98000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>42.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111168.0</td>\n",
       "      <td>37033.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>31463.0</td>\n",
       "      <td>11</td>\n",
       "      <td>672.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>543747</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>10.75</td>\n",
       "      <td>163.11</td>\n",
       "      <td>B</td>\n",
       "      <td>9</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>46000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>42.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111168.0</td>\n",
       "      <td>37033.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>31463.0</td>\n",
       "      <td>21</td>\n",
       "      <td>712.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1377853</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>13.99</td>\n",
       "      <td>683.46</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>75000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111168.0</td>\n",
       "      <td>17517.0</td>\n",
       "      <td>49400.0</td>\n",
       "      <td>31463.0</td>\n",
       "      <td>15</td>\n",
       "      <td>682.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>371976</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>36</td>\n",
       "      <td>14.74</td>\n",
       "      <td>34.54</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>42.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111168.0</td>\n",
       "      <td>37033.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>31463.0</td>\n",
       "      <td>5</td>\n",
       "      <td>667.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  loan_amnt  funded_amnt  term  int_rate  installment grade  \\\n",
       "0   670653     5000.0       5000.0    36     10.74       163.08     B   \n",
       "1   843470     8500.0       8500.0    36      7.49       264.37     A   \n",
       "2   695876     5000.0       5000.0    36     10.37       162.21     B   \n",
       "3   766091     1875.0       1875.0    36      8.49        59.19     A   \n",
       "4   505004     7000.0       7000.0    36      7.51       217.77     A   \n",
       "5   476497     6250.0       6250.0    36     12.53       209.17     B   \n",
       "6  1030372     8000.0       8000.0    36     13.49       271.45     C   \n",
       "7   543747     5000.0       5000.0    36     10.75       163.11     B   \n",
       "8  1377853    20000.0      20000.0    36     13.99       683.46     C   \n",
       "9   371976     1000.0       1000.0    36     14.74        34.54     D   \n",
       "\n",
       "   emp_length home_ownership  annual_inc  ... pct_tl_nvr_dlq percent_bc_gt_75  \\\n",
       "0           6       MORTGAGE     35184.0  ...           98.0             42.9   \n",
       "1           6       MORTGAGE    230000.0  ...           98.0             42.9   \n",
       "2           6           RENT     17220.0  ...           98.0             42.9   \n",
       "3           6           RENT     15204.0  ...           98.0             42.9   \n",
       "4           2           RENT     38400.0  ...           98.0             42.9   \n",
       "5           0           RENT     24000.0  ...           98.0             42.9   \n",
       "6          10       MORTGAGE     98000.0  ...           98.0             42.9   \n",
       "7           9       MORTGAGE     46000.0  ...           98.0             42.9   \n",
       "8           2       MORTGAGE     75000.0  ...           98.0             37.5   \n",
       "9           0           RENT     10000.0  ...           98.0             42.9   \n",
       "\n",
       "   pub_rec_bankruptcies  tax_liens  tot_hi_cred_lim  total_bal_ex_mort  \\\n",
       "0                   0.0        0.0         111168.0            37033.0   \n",
       "1                   0.0        0.0         111168.0            37033.0   \n",
       "2                   1.0        0.0         111168.0            37033.0   \n",
       "3                   0.0        0.0         111168.0            37033.0   \n",
       "4                   0.0        0.0         111168.0            37033.0   \n",
       "5                   0.0        0.0         111168.0            37033.0   \n",
       "6                   0.0        0.0         111168.0            37033.0   \n",
       "7                   0.0        0.0         111168.0            37033.0   \n",
       "8                   0.0        0.0         111168.0            17517.0   \n",
       "9                   0.0        0.0         111168.0            37033.0   \n",
       "\n",
       "   total_bc_limit  total_il_high_credit_limit  credit_history_years  \\\n",
       "0         15000.0                     31463.0                     5   \n",
       "1         15000.0                     31463.0                    20   \n",
       "2         15000.0                     31463.0                    12   \n",
       "3         15000.0                     31463.0                    11   \n",
       "4         15000.0                     31463.0                    10   \n",
       "5         15000.0                     31463.0                     5   \n",
       "6         15000.0                     31463.0                    11   \n",
       "7         15000.0                     31463.0                    21   \n",
       "8         49400.0                     31463.0                    15   \n",
       "9         15000.0                     31463.0                     5   \n",
       "\n",
       "   fico_score  \n",
       "0       687.0  \n",
       "1       742.0  \n",
       "2       697.0  \n",
       "3       722.0  \n",
       "4       737.0  \n",
       "5       697.0  \n",
       "6       672.0  \n",
       "7       712.0  \n",
       "8       682.0  \n",
       "9       667.0  \n",
       "\n",
       "[10 rows x 75 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# == Save as Silver Delta Table 2 (Cleaned Strings & Numbers Version) ==\n",
    "silver_table2.write.format(\"delta\").mode(\"overwrite\").save(\n",
    "    \"../data/silver/lendingclub_cleaned_numeric\"\n",
    ")\n",
    "\n",
    "silver_table2.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All missing values filled, base features created saved to Gold Delta Table!\n",
      "✅ Ready for PD, LGD, and EAD Modeling!\n"
     ]
    }
   ],
   "source": [
    "silver_table2.limit(10).toPandas()\n",
    "\n",
    "print(\"✅ All missing values filled, base features created saved to Gold Delta Table!\")\n",
    "print(\"✅ Ready for PD, LGD, and EAD Modeling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "442cf3f5-d36f-4f87-b6a6-e66623340857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Gold Delta Table\n",
    "\n",
    "Finally, to produce a Gold Delta Table, I will need to sort the dataset in chronological order.\n",
    "\n",
    "For credit risk modeling, banks use past data loan data to predict future defaults / metrics. As such, we want our dataset to be sorted in **chronological order**, so that built models are trained on older data, and tested on newer data **(out-of-time split)**.\n",
    "\n",
    "There should not be random splitting of data **(out-of-sample split)**, e.g. `train-test-split` from `sklearn` since credit-risk modeling is a **time-series problem**.\n",
    "\n",
    "Hence, I will be sorting the dataset right from the start.\n",
    "\n",
    "By producing the Gold Delta Table, the subsequent jobs would require data scientists to impute missing values, conduct feature engineering and dimensionality reduction for accurate credit risk modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53463362-4d20-4f47-be24-5fc7f18503c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_table2 = spark.read.format(\"delta\").load(\n",
    "    \"../data/silver/lendingclub_cleaned_numeric\"\n",
    ")\n",
    "\n",
    "gold_df = silver_table2.orderBy([\"issue_d\"], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18d12cbe-53a8-4dc3-bc40-7a1c3dd21ae2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# == Save as Silver Delta Table 2 (Cleaned Strings Version) ==\n",
    "gold_df.write.format(\"delta\").mode(\"overwrite\").save(\n",
    "    \"../data/gold/medallion_cleaned_lc_data\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Medallion Architecture Data Cleaning Pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
