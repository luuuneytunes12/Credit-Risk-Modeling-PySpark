{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ed89801-2964-4e65-8da3-7c3a8e454f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# String Formatting EDA (Lending Club)\n",
    "This notebook aims to use **RapidFuzz** to quickly find out issues in string columns in the Bronze Delta Table Lending Club Dataset. After finding out such issues, string formatting issues will then be resolved via the Medallion Architecture Data Cleaning Pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e7118ce-825d-4647-abad-2ba222f567dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d599252d-b6e3-48d6-b65d-58217986af6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import Libraries \n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, count, desc, isnan, isnull, lit, length, trim, lower, upper, to_date, concat_ws\n",
    ")\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DoubleType, IntegerType, DateType\n",
    ")\n",
    "\n",
    "from rapidfuzz import process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5f32172-eb35-4034-a9b5-0799b8f16350",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Read Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2966717e-ae0b-409c-843f-8ca191fff0c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To find which catalog I am currently working in \n",
    "spark.sql(\"SELECT current_catalog(), current_database()\").show()\n",
    "\n",
    "spark.sql(\"SHOW TABLES IN spark_catalog.default\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88e7c3f7-d263-428d-8dc6-9a95ba3629b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read from Bronze Delta Table (Uncleaned & Raw)\n",
    "df = spark.read.table(\"bronze.lendingclub_raw\")\n",
    "\n",
    "df.printSchema() \n",
    "\n",
    "# Adjust the path to wherever your raw Lending Club Delta table sits\n",
    "# raw_df = spark.read.format(\"delta\").load(\"/mnt/bronze/lending_club\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12e0fe83-e896-4712-9a03-04523f57157d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Identify all String Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c303f032-b706-4160-b29f-f1bbc0a040e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_string_cols = [\n",
    "    f.name\n",
    "    for f in df.schema.fields\n",
    "    if isinstance(f.dataType, StringType)\n",
    "]\n",
    "print(\"String columns:\", all_string_cols)\n",
    "\n",
    "\n",
    "print(f\"Number of String Columns: {len(all_string_cols)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "879ce62e-138f-4e4b-8f96-e624ffedcd0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Identify Number of Distinct Values Per String Column\n",
    "The following code prints how many distinct values each string column has, so we can decide which columns we can tackle with fuzzy grouping with Rapid Fuzz (more computationally expensive) and which allows manual cleaning (lower effort and cost). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56f6f2ab-271c-4876-bef8-3768811f10cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for col_num, col_name in enumerate(all_string_cols, 1): # starts from col_num = 1 \n",
    "    distinct_count = df.select(col_name).distinct().count() \n",
    "    print(f\"{col_num:02d}. {col_name} -> {distinct_count:6d} distinct values\" ) #:6d is adding space and right-aligning it or sth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1aef5aa-9169-4971-9034-61a2592f0b23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "From the results above, we shall define a list of columns whereby they should be skipped for fuzzy string matching. This is because they are numeric or date columns or meaningless columns inherently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9055edd-2ce2-4cd1-8b15-617a61e93b3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numeric_columns = [\n",
    "    \"id\", \"member_id\", \"emp_length\", \"annual_inc\", \"annual_inc_joint\",\n",
    "    \"dti\", \"dti_joint\", \"delinq_2yrs\", \"fico_range_low\", \"fico_range_high\",\n",
    "    \"inq_last_6mths\", \"mths_since_last_delinq\", \"mths_since_last_record\",\n",
    "    \"open_acc\", \"pub_rec\", \"revol_bal\", \"revol_util\", \"total_acc\", \n",
    "    \"out_prncp\", \"out_prncp_inv\", \"total_pymnt\", \"total_pymnt_inv\", \n",
    "    \"total_rec_prncp\", \"total_rec_int\", \"total_rec_late_fee\", \"recoveries\", \n",
    "    \"collection_recovery_fee\", \"last_fico_range_high\", \"last_fico_range_low\", \n",
    "    \"collections_12_mths_ex_med\", \"mths_since_last_major_derog\",\n",
    "    \"acc_now_delinq\", \"tot_coll_amt\", \"loan_amnt\", \"funded_amnt\", \n",
    "    \"funded_amnt_inv\", \"installment\", \"tot_cur_bal\", \"total_rev_hi_lim\", \n",
    "    \"inq_fi\", \"hardship_amount\", \"hardship_dpd\", \"orig_projected_additional_accrued_interest\",\n",
    "    \"hardship_payoff_balance_amount\", \"hardship_last_payment_amount\", \n",
    "    \"settlement_amount\", \"settlement_percentage\", \"settlement_term\", \n",
    "    \"avg_cur_bal\", \"total_bal_il\", \"bc_util\", \"il_util\", \"total_cu_tl\",\n",
    "    \"max_bal_bc\", \"percent_bc_gt_75\", \"total_bal_ex_mort\", \"all_util\", \n",
    "    \"open_acc_6m\", \"open_act_il\", \"open_il_12m\", \"last_pymnt_amnt\", \"open_il_24m\", \"mths_since_rcnt_il\", \n",
    "    \"open_rv_12m\", \"open_rv_24m\", 'emp_length', 'term'\n",
    "]\n",
    "\n",
    "\n",
    "date_columns = [\n",
    "    \"issue_d\", \"earliest_cr_line\", \"last_pymnt_d\", \"last_credit_pull_d\", \"next_pymnt_d\",\n",
    "    \"sec_app_earliest_cr_line\", \"hardship_start_date\", \"hardship_end_date\", \n",
    "    \"payment_plan_start_date\", \"debt_settlement_flag_date\", \"settlement_date\"\n",
    "]\n",
    "\n",
    "meaningless_columns = [\n",
    "    \"url\", \"desc\", \"title\", \"zip_code\", \"purpose\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e480d9e3-8e9b-41bf-9e03-f9406762bc4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Show Top Frequent N Values Per String Column \n",
    "Before doing any fuzzy grouping, we can see the top frequent values per column in strings. An example would be `home_ownership` showing 'rent', 'Rent' and 'RENT '. This will affect subsequent analysis and skew machine learning models. \n",
    "\n",
    "As such, the following approach allows me to check if a column has obvious variants, guiding our decisions on which columns to cluster with RapidFuzz to reduce computational expenses. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eacf298c-8272-41dd-a277-97bda7f6e4d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def show_raw_top_values(df, column, top_n=30):\n",
    "    print(f\"\\nTop {top_n} raw values for column '{column}':\")\n",
    "    (\n",
    "        df\n",
    "        .groupBy(col(column))\n",
    "        .count()\n",
    "        .orderBy(desc(\"count\"))\n",
    "        .limit(top_n)\n",
    "        .show(n=top_n, truncate=False) # by default, shows 20 entries only\n",
    "    )\n",
    "\n",
    "\n",
    "# Combine all columns to skip\n",
    "skip_cols = set(numeric_columns + date_columns + meaningless_columns)\n",
    "\n",
    "# Loop through only valid string columns not in skip list\n",
    "for col_name in all_string_cols:\n",
    "    if col_name not in skip_cols:\n",
    "        show_raw_top_values(df, col_name, top_n=30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef769759-e92b-415e-acd3-95d86a509241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "From the results above, we have grouped faulty entries which require cleaning. This is to streamline data cleaning later on. \n",
    "\n",
    "**Invalid Entries**\n",
    "- **application_type**: Drop entries that are not Individual or Joint App.\n",
    "- **home_ownership**: Drop anything that is not MORTGAGE, RENT, OWN, ANY, OTHER \n",
    "- **verification_status** : Drop anything that is not Source Verified, Not Verified, Verified \n",
    "- **loan_status**: Has legacy system entries \n",
    "    - **'Does not meet credit policy'**: Shows that loan was not approved. There shouldn't be records for this, or rather, the credit risk modeling project aims to predict PD, LGD, EAD based on approved loans. \n",
    "    - Given the data points for these are extremely small, they shall be **dropped** \n",
    "    - Valid Entres: Fully Paid, Current, Charged Off, Late (31-120 days), In Grace Period, Later (16-30 days), 'Default' \n",
    "- **pymnt_plan**: Drop all entries that is not 'n' and 'y'\n",
    "- **initial_list_status**: Drop all entries that are not 'w' and 'f'\n",
    "- **policy_code**: Drop all entries that are not 1.0 and 0.0, and change to 1 and 0 \n",
    "\n",
    "- **hardship_flag**: Drop all entries that are not N or Y\n",
    "\n",
    "\n",
    "**Rapid Fuzz: String Columns**\n",
    "- **emp_title**\n",
    "- **sub_grade**\n",
    "- **addr_state**\n",
    "\n",
    "**Unusable Columns**: \n",
    "- **hardship_type**: Many null values (2 million+)\n",
    "- **verification_status_joint**: Drop all entries that are not Verified, Individual, Not Verified, or Source Verified (Many Null Values - 2 million records)\n",
    "- **hardship_status**: 2 million records Null \n",
    "- **deferral_term**\n",
    "- **hardship_length**\n",
    "- **hardship_loan_status**\n",
    "- **settlement_status**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b74028b1-65cd-442e-863b-1b827e3a7b67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. RapidFuzz String Similarity Matching Function\n",
    "Now that we have drilled down to the columns that have inconsistent spelling and formats, we would want to group similar strings together and map similar variants to 1 single string value. \n",
    "\n",
    "RapidFuzz clusters items that exceed the similarity threshold. It is a library that is better than FuzzyWuzzy for string matching, which allows better performance in big data environments. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57f44cb7-9f13-470b-9373-3b78a698dce1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_fuzzy_groups(distinct_values, threshold):\n",
    "    \"\"\"\n",
    "    Given a Python list of distinct strings from a column, return a dict mapping each string\n",
    "    to a chosen 'canonical' string (first occurence). Two strings with similarity ≥ threshold\n",
    "    collapse into the same 'canonical string'.\n",
    "\n",
    "    Output:\n",
    "    {\n",
    "        \"Rent\": \"Rent\",\n",
    "        \"RENT\": \"Rent\",\n",
    "        \"rent\": \"Rent\",\n",
    "        \"Own\": \"Own\",\n",
    "        \"own\": \"Own\",\n",
    "        \"mortgag\": \"mortgag\",\n",
    "        \"Mortgage\": \"mortgag\"\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    mapping_dict = {}\n",
    "    canonical_list = []\n",
    "\n",
    "    for val in distinct_values:\n",
    "        if not val or val.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        result = process.extractOne(val, canonical_list, score_cutoff=threshold)\n",
    "        if result:\n",
    "            match, score, _ = result\n",
    "            mapping_dict[val] = match\n",
    "        else:\n",
    "            canonical_list.append(val)\n",
    "            mapping_dict[val] = val\n",
    "\n",
    "    return mapping_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02b501ed-6cde-497b-8ad0-8f361e428354",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6.1 RapidFuzz String Matching Test (1 column)\n",
    "\n",
    "The following code demonstrates how fuzzy matching works, for learning purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7c0aeaaf-bfc9-4802-a5a6-00b7e0f8f0ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Choose one column you want to test fuzzy grouping on\n",
    "example_column = \"emp_title\"\n",
    "\n",
    "# Step 1: All Row Objects in this column \n",
    "distinct_vals = df.select(example_column).where(col(example_column).isNotNull()).distinct().collect()\n",
    "\n",
    "# Step 0: Keep only top 5,000 most common entries\n",
    "top_vals = (\n",
    "    df.groupBy(example_column)\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "    .limit(5000)  # try 1000 or 5000 for speed\n",
    "    .where(col(example_column).isNotNull())\n",
    "    .rdd.flatMap(lambda x: [x[0]])\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "# Run fuzzy grouping on top frequent raw values\n",
    "threshold = 98\n",
    "mapping_dict = get_fuzzy_groups(top_vals, threshold=threshold)\n",
    "\n",
    "\n",
    "fuzzy_groups = defaultdict(list)\n",
    "for original, canonical in mapping_dict.items():\n",
    "    fuzzy_groups[canonical].append(original)\n",
    "\n",
    "print(f\"\\nFuzzy groups for column: '{example_column}' (threshold = {threshold})\")\n",
    "for canonical, members in fuzzy_groups.items():\n",
    "    if len(members) > 1:\n",
    "        print(f\"  Canonical: '{canonical}' ← {members}\\n\")\n",
    "    else:\n",
    "        print(f\"  (Solo) '{canonical}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c42d40bd-086d-47e5-bf5a-e73044662e62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6.2 RapidFuzz Fuzzy Grouping (String Columns) \n",
    "We will be checking similar strings in each of the necessary columns as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31fdd271-7607-44f5-8de7-9d229d25f5de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for column in ['emp_title', 'sub_grade', \"addr_state\"]: \n",
    "    print('Current Col: ' + column + '\\n')\n",
    "    # Choose one column you want to test fuzzy grouping on\n",
    "    example_column = column\n",
    "\n",
    "    # Step 1: All Row Objects in this column \n",
    "    distinct_vals = df.select(example_column).where(col(example_column).isNotNull()).distinct().collect()\n",
    "\n",
    "    # Step 0: Keep only top 5,000 most common entries\n",
    "    top_vals = (\n",
    "        df.groupBy(example_column)\n",
    "        .count()\n",
    "        .orderBy(\"count\", ascending=False)\n",
    "        .limit(5000)  # try 1000 or 5000 for speed\n",
    "        .where(col(example_column).isNotNull())\n",
    "        .rdd.flatMap(lambda x: [x[0]])\n",
    "        .collect()\n",
    "    )\n",
    "    \n",
    "    top_vals_cleaned = [val.lower().strip() for val in top_vals if isinstance(val, str)]\n",
    "\n",
    "    # Run fuzzy grouping on top frequent raw values\n",
    "    threshold = 80\n",
    "    mapping_dict = get_fuzzy_groups(top_vals_cleaned, threshold=threshold)\n",
    "\n",
    "\n",
    "    fuzzy_groups = defaultdict(list)\n",
    "    for original, canonical in mapping_dict.items():\n",
    "        fuzzy_groups[canonical].append(original)\n",
    "\n",
    "    print(f\"\\nFuzzy groups for column: '{example_column}' (threshold = {threshold})\")\n",
    "    for canonical, members in fuzzy_groups.items():\n",
    "        if len(members) > 1:\n",
    "            print(f\"  Canonical: '{canonical}' ← {members}\\n\")\n",
    "        else:\n",
    "            print(f\"  (Solo) '{canonical}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d49cfbd-8311-438c-9786-d879c94c22b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "`emp_title` has some string formatting issues. But since the column has high cardinality (too many distinct values), and are not categorisable, they serve little purpose in credit risk modeling. \n",
    "\n",
    "`subgrade` is fully clean.\n",
    "\n",
    "As seen, `addr_state` has few entries that are too long, when it should only consist of 2 string characters. To assess the severity of this, we shall calculate the % of entries that have > 2 string characters. From the results, it is safe to remove all rows that have > 2 characters and they will have little impact on subsequent machine learning model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03213d7f-d6c1-4f93-bbd8-5da94b938f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Code to calculate the percentage of entries with more than 2 characters\n",
    "total_entries = df.count() \n",
    "long_entries_addr_state = df.filter(length(df.addr_state) > 2).count()\n",
    "percentage_long_entries = (long_entries_addr_state / total_entries) * 100\n",
    "print(f\"Percentage of entries with more than 2 characters: {percentage_long_entries:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28fb949d-18b0-4dc2-b01d-ba8b15af575c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Inspect Numeric Columns (Currently String)\n",
    "To ensure that type casting these columns to numeric columns will not lead to loss of data integrity, I will inspect top 30 distinct values of such columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79911993-d74f-4fe5-acda-5b53c9f0e9be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for col_name in numeric_columns:\n",
    "    show_raw_top_values(df, col_name, top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef6c0eac-e5a9-4405-b9c8-d1bc73bef40c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "From the table, issues are as shown \n",
    "\n",
    "**Unusable Columns**\n",
    "- **annual_inc_joint** \n",
    "- **dti_joint**\n",
    "\n",
    "**Numeric Columns**\n",
    "- **emp_length**\n",
    "- **term**\n",
    "\n",
    "I kept certain columns, despite having high percentage of null values. For example, `mths_since_last_rec` is a strong signal to the good credit health of an individual. \n",
    "\n",
    "Columns like `collection_12_mths_ex_med` have string values. However, there are low percentages of such values, and type casting such columns to numeric columns will make little impact on data integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9eb91104-8f37-40a9-b875-7c9f0bc99c5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Inpsecting Date Columns (Currently String)\n",
    "\n",
    "Given how dates are important in credit risk modeling, due to out-of-time splitting, it is important to conduct type casting in the data cleaning stage. As such, let's inspect unique values in the date columns (currently string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "634f0181-f435-4d9e-aef2-935953b6c7dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for col_name in date_columns:\n",
    "    show_raw_top_values(df, col_name, top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7689699-d9cb-4be1-bb7d-255686954347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As seen, there aren't major issues with current date columns. However, to ensure that time-based analysis. Such columns should be type casted to date-time data type instead. It should be noted that they are of `MM-YYYY` format, and should be type casted to `DD-YYYY` format, to allow out-of-time split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b33ecad-d7f8-457a-8c06-62cfe59cf107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9. Conclusion \n",
    "\n",
    "Concluding our findings for string columns, columns needed for subsequent data cleaning is as shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7bf79a5-70d0-4fef-b104-dc8acbf42b9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To be continued in finding issues with numeric data ... \n",
    "invalid_entries_list = [ \"application_type\",\n",
    "    \"policy_code\",\n",
    "    \"home_ownership\",\n",
    "    \"verification_status\",\n",
    "    \"loan_status\",\n",
    "    \"pymnt_plan\",\n",
    "    \"initial_list_status\",\n",
    "    \"hardship_flag\", 'term', 'emp_length']\n",
    "    \n",
    "string_cols_formatting_fix = ['addr_state']\n",
    "\n",
    "\n",
    "numeric_columns = [\n",
    "    \"id\", \"member_id\", \"annual_inc\", \"annual_inc_joint\",\n",
    "    \"dti\", \"dti_joint\", \"delinq_2yrs\", \"fico_range_low\", \"fico_range_high\",\n",
    "    \"inq_last_6mths\", \"mths_since_last_delinq\", \"mths_since_last_record\",\n",
    "    \"open_acc\", \"pub_rec\", \"revol_bal\", \"revol_util\", \"total_acc\", \n",
    "    \"out_prncp\", \"out_prncp_inv\", \"total_pymnt\", \"total_pymnt_inv\", \n",
    "    \"total_rec_prncp\", \"total_rec_int\", \"total_rec_late_fee\", \"recoveries\", \n",
    "    \"collection_recovery_fee\", \"last_fico_range_high\", \"last_fico_range_low\", \n",
    "    \"collections_12_mths_ex_med\", \"mths_since_last_major_derog\",\n",
    "    \"acc_now_delinq\", \"tot_coll_amt\", \"loan_amnt\", \"funded_amnt\", \n",
    "    \"funded_amnt_inv\", \"installment\", \"tot_cur_bal\", \"total_rev_hi_lim\", \n",
    "    \"inq_fi\", \"hardship_amount\", \"hardship_dpd\", \"orig_projected_additional_accrued_interest\",\n",
    "    \"hardship_payoff_balance_amount\", \"hardship_last_payment_amount\", \n",
    "    \"settlement_amount\", \"settlement_percentage\", \"settlement_term\", \n",
    "    \"avg_cur_bal\", \"total_bal_il\", \"bc_util\", \"il_util\", \"total_cu_tl\",\n",
    "    \"max_bal_bc\", \"percent_bc_gt_75\", \"total_bal_ex_mort\", \"all_util\", \n",
    "    \"open_acc_6m\", \"open_act_il\", \"open_il_12m\", \"last_pymnt_amnt\", \"open_il_24m\", \"mths_since_rcnt_il\", \n",
    "    \"open_rv_12m\", \"open_rv_24m\"\n",
    "]\n",
    "\n",
    "\n",
    "date_columns = [\n",
    "    \"issue_d\", \"earliest_cr_line\", \"last_pymnt_d\", \"last_credit_pull_d\", \"next_pymnt_d\",\n",
    "    \"sec_app_earliest_cr_line\", \"hardship_start_date\", \"hardship_end_date\", \n",
    "    \"payment_plan_start_date\", \"debt_settlement_flag_date\", \"settlement_date\"\n",
    "]\n",
    "\n",
    "meaningless_columns = [\n",
    "    \"url\", \"desc\", \"title\", \"zip_code\", \"purpose\"\n",
    "]\n",
    "\n",
    "unusable_cols = [\"emp_title\",\n",
    "    \"hardship_type\",\n",
    "    \"verification_status_joint\",\n",
    "    \"hardship_status\",\n",
    "    \"deferral_term\",\n",
    "    \"hardship_length\",\n",
    "    \"hardship_loan_status\",\n",
    "    \"settlement_status\", \"annual_inc_joint\", 'dti_joint']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "093af72a-b0b4-4ced-a375-4c51208ba8fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Unusable columns refer to records that have a large percentage of missing records. Missing values will be dealt by data scientists later on, since it is under feature selection, and would not make sense to be in data engineering pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ddb97f2-9b60-40e3-bf9f-c21dd69c8da1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "string_issues",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
